Using torch device NVIDIA TITAN Xp
net: causal
cc_weight: 1.0
abstract_pen: 0.0
params: {'epochs': 100, 'lr': 0.0008, 'n': 5000}
Net has 207442 parameters
Round 0
Generated trajectories in 102.7 seconds
epoch: 0	train loss: 145540.375	(34.1s)
epoch: 1	train loss: 136729.390625	(30.4s)
epoch: 2	train loss: 116914.59375	(30.6s)
epoch: 3	train loss: 101540.0078125	(30.5s)
epoch: 4	train loss: 94397.2890625	(30.6s)
epoch: 5	train loss: 88951.625	(30.6s)
epoch: 6	train loss: 82265.8984375	(30.7s)
epoch: 7	train loss: 73454.8828125	(30.5s)
epoch: 8	train loss: 67761.3203125	(30.4s)
epoch: 9	train loss: 64240.41796875	(30.4s)
epoch: 10	train loss: 61094.5078125	(30.5s)
epoch: 11	train loss: 57570.22265625	(30.5s)
epoch: 12	train loss: 54151.703125	(30.4s)
epoch: 13	train loss: 52062.0	(30.5s)
epoch: 14	train loss: 50005.2265625	(30.5s)
epoch: 15	train loss: 47020.265625	(30.5s)
epoch: 16	train loss: 46193.92578125	(30.4s)
epoch: 17	train loss: 44719.45703125	(30.4s)
epoch: 18	train loss: 43669.2265625	(30.5s)
epoch: 19	train loss: 42640.23046875	(30.6s)
epoch: 20	train loss: 42650.0390625	(30.4s)
epoch: 21	train loss: 41848.63671875	(30.7s)
epoch: 22	train loss: 40731.98046875	(31.0s)
epoch: 23	train loss: 40128.32421875	(30.6s)
epoch: 24	train loss: 39075.50390625	(30.6s)
epoch: 25	train loss: 38251.5234375	(30.6s)
epoch: 26	train loss: 38499.5703125	(30.7s)
epoch: 27	train loss: 37724.55078125	(30.7s)
epoch: 28	train loss: 37370.12109375	(30.5s)
epoch: 29	train loss: 36578.01171875	(30.4s)
epoch: 30	train loss: 36357.72265625	(30.6s)
epoch: 31	train loss: 36081.94140625	(30.4s)
epoch: 32	train loss: 35511.26171875	(30.4s)
epoch: 33	train loss: 35731.046875	(30.5s)
epoch: 34	train loss: 35000.80859375	(30.5s)
epoch: 35	train loss: 34570.5546875	(30.5s)
epoch: 36	train loss: 35354.7265625	(30.4s)
epoch: 37	train loss: 33224.51953125	(30.4s)
epoch: 38	train loss: 33222.4375	(30.4s)
epoch: 39	train loss: 32703.669921875	(30.5s)
epoch: 40	train loss: 32604.748046875	(30.4s)
epoch: 41	train loss: 32548.783203125	(30.5s)
epoch: 42	train loss: 32704.73046875	(30.6s)
epoch: 43	train loss: 32382.189453125	(30.5s)
epoch: 44	train loss: 32594.013671875	(30.4s)
epoch: 45	train loss: 31285.953125	(30.4s)
epoch: 46	train loss: 31074.947265625	(30.5s)
epoch: 47	train loss: 32769.14453125	(30.6s)
epoch: 48	train loss: 32738.302734375	(30.7s)
epoch: 49	train loss: 30695.900390625	(30.4s)
epoch: 50	train loss: 29573.048828125	(30.5s)
epoch: 51	train loss: 29902.2421875	(30.8s)
epoch: 52	train loss: 29534.517578125	(31.0s)
epoch: 53	train loss: 29302.154296875	(30.5s)
epoch: 54	train loss: 28910.658203125	(30.5s)
epoch: 55	train loss: 30065.755859375	(30.6s)
epoch: 56	train loss: 30148.65625	(30.6s)
epoch: 57	train loss: 29094.4609375	(30.4s)
epoch: 58	train loss: 29892.220703125	(30.5s)
epoch: 59	train loss: 30278.265625	(30.5s)
epoch: 60	train loss: 30241.1875	(30.5s)
epoch: 61	train loss: 31885.896484375	(30.4s)
epoch: 62	train loss: 30551.23046875	(30.4s)
epoch: 63	train loss: 29046.046875	(30.6s)
epoch: 64	train loss: 28868.16796875	(30.6s)
epoch: 65	train loss: 27962.482421875	(30.5s)
epoch: 66	train loss: 29239.19921875	(30.6s)
epoch: 67	train loss: 28342.26953125	(30.7s)
epoch: 68	train loss: 28362.447265625	(30.6s)
epoch: 69	train loss: 27633.005859375	(30.5s)
epoch: 70	train loss: 26779.94921875	(30.5s)
epoch: 71	train loss: 27719.82421875	(30.5s)
epoch: 72	train loss: 29685.15625	(30.4s)
epoch: 73	train loss: 28966.75	(30.4s)
epoch: 74	train loss: 27155.845703125	(30.5s)
epoch: 75	train loss: 28725.228515625	(30.5s)
epoch: 76	train loss: 28099.986328125	(30.5s)
epoch: 77	train loss: 29364.798828125	(30.4s)
epoch: 78	train loss: 27229.126953125	(30.4s)
epoch: 79	train loss: 28368.212890625	(30.5s)
epoch: 80	train loss: 26949.583984375	(30.4s)
epoch: 81	train loss: 26838.455078125	(30.5s)
epoch: 82	train loss: 26721.8203125	(30.4s)
epoch: 83	train loss: 25196.32421875	(30.5s)
epoch: 84	train loss: 26340.353515625	(30.6s)
epoch: 85	train loss: 27352.65625	(30.4s)
epoch: 86	train loss: 29346.677734375	(30.5s)
epoch: 87	train loss: 27537.69921875	(30.6s)
epoch: 88	train loss: 26274.314453125	(30.9s)
epoch: 89	train loss: 25131.71875	(30.7s)
epoch: 90	train loss: 24626.0390625	(30.7s)
epoch: 91	train loss: 25559.2265625	(30.4s)
epoch: 92	train loss: 25305.4375	(30.4s)
epoch: 93	train loss: 25231.92578125	(30.3s)
epoch: 94	train loss: 24527.93359375	(30.6s)
epoch: 95	train loss: 24442.19921875	(30.4s)
epoch: 96	train loss: 25551.658203125	(30.4s)
epoch: 97	train loss: 27754.005859375	(30.4s)
epoch: 98	train loss: 29124.12109375	(30.5s)
epoch: 99	train loss: 25811.330078125	(30.5s)
Evaluating model on 200 episodes
0.0047416692759725265
0.003115757464651584
0.0033508289416204207
0.0021348639208802274
0.004541768750641495
0.001377251377562061
0.0017070149158826097
0.000931063448661007
0.0007601685465488117
0.0035832262074109167
0.0028623344987863675
0.0012281139691670735
0.0028245279182946043
0.0032537190773938266
0.0039616639855508265
0.003272510884146738
0.0010329142642149236
0.0027125147568188945
0.0014487107458990068
0.0012395802158506638
0.008404160287682316
0.0016364301797390606
0.02010698313533794
0.0040520458796891035
0.003689067987059908
0.004081451289948745
0.0010822920491645003
0.004781507886946201
0.0029682867565757726
0.004968734057102766
0.00045079421397531405
0.005415795524539944
0.0017277335053742199
0.003405240386200603
0.002930998313240707
0.0040110359359459835
Solved 36/200 episodes
0.0006189638027723883
Evaluated model in 26.8 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-0
Round 1
Generated trajectories in 102.9 seconds
epoch: 0	train loss: 34959.578125	(30.3s)
epoch: 1	train loss: 32265.953125	(30.2s)
epoch: 2	train loss: 32526.099609375	(30.5s)
epoch: 3	train loss: 31031.43359375	(30.6s)
epoch: 4	train loss: 30149.166015625	(30.5s)
epoch: 5	train loss: 30043.865234375	(30.6s)
epoch: 6	train loss: 30096.576171875	(30.5s)
epoch: 7	train loss: 29029.2578125	(30.5s)
epoch: 8	train loss: 29344.88671875	(30.4s)
epoch: 9	train loss: 28417.912109375	(30.3s)
epoch: 10	train loss: 27034.220703125	(30.5s)
epoch: 11	train loss: 27454.806640625	(30.4s)
epoch: 12	train loss: 27270.994140625	(30.4s)
epoch: 13	train loss: 28174.080078125	(30.4s)
epoch: 14	train loss: 26520.662109375	(30.5s)
epoch: 15	train loss: 26041.958984375	(30.5s)
epoch: 16	train loss: 26329.4296875	(30.3s)
epoch: 17	train loss: 25650.294921875	(30.4s)
epoch: 18	train loss: 25559.546875	(30.4s)
epoch: 19	train loss: 26153.2890625	(30.4s)
epoch: 20	train loss: 24843.46875	(30.3s)
epoch: 21	train loss: 24374.837890625	(30.3s)
epoch: 22	train loss: 25594.267578125	(30.5s)
epoch: 23	train loss: 26754.453125	(30.4s)
epoch: 24	train loss: 26309.880859375	(30.5s)
epoch: 25	train loss: 27441.044921875	(30.3s)
epoch: 26	train loss: 25311.33203125	(30.4s)
epoch: 27	train loss: 24717.564453125	(30.5s)
epoch: 28	train loss: 23548.03125	(30.4s)
epoch: 29	train loss: 25861.96875	(30.4s)
epoch: 30	train loss: 26073.4921875	(30.5s)
epoch: 31	train loss: 24604.96484375	(30.3s)
epoch: 32	train loss: 24682.619140625	(30.4s)
epoch: 33	train loss: 24095.439453125	(30.3s)
epoch: 34	train loss: 23389.591796875	(30.5s)
epoch: 35	train loss: 24271.23046875	(30.3s)
epoch: 36	train loss: 23479.5546875	(30.5s)
epoch: 37	train loss: 22234.546875	(30.3s)
epoch: 38	train loss: 22330.546875	(30.6s)
epoch: 39	train loss: 23591.76953125	(30.4s)
epoch: 40	train loss: 23920.6171875	(30.4s)
epoch: 41	train loss: 23772.3359375	(30.5s)
epoch: 42	train loss: 22640.56640625	(30.4s)
epoch: 43	train loss: 21915.5	(30.3s)
epoch: 44	train loss: 23194.462890625	(30.5s)
epoch: 45	train loss: 23657.35546875	(30.4s)
epoch: 46	train loss: 24274.189453125	(30.6s)
epoch: 47	train loss: 22614.015625	(30.5s)
epoch: 48	train loss: 21385.1796875	(30.6s)
epoch: 49	train loss: 21187.044921875	(30.4s)
epoch: 50	train loss: 21005.205078125	(30.4s)
epoch: 51	train loss: 20933.07421875	(30.4s)
epoch: 52	train loss: 21036.390625	(30.4s)
epoch: 53	train loss: 24046.896484375	(30.5s)
epoch: 54	train loss: 22472.607421875	(30.4s)
epoch: 55	train loss: 22499.853515625	(30.4s)
epoch: 56	train loss: 22788.392578125	(30.4s)
epoch: 57	train loss: 21727.845703125	(30.4s)
epoch: 58	train loss: 21324.556640625	(30.5s)
epoch: 59	train loss: 21643.384765625	(30.3s)
epoch: 60	train loss: 20631.654296875	(30.5s)
epoch: 61	train loss: 20281.501953125	(30.3s)
epoch: 62	train loss: 19882.919921875	(30.4s)
epoch: 63	train loss: 19956.474609375	(30.7s)
epoch: 64	train loss: 19136.716796875	(30.4s)
epoch: 65	train loss: 18768.271484375	(30.5s)
epoch: 66	train loss: 18588.796875	(30.4s)
epoch: 67	train loss: 18503.8203125	(30.4s)
epoch: 68	train loss: 18390.841796875	(30.3s)
epoch: 69	train loss: 18176.31640625	(30.4s)
epoch: 70	train loss: 18173.998046875	(30.8s)
epoch: 71	train loss: 18427.16015625	(30.4s)
epoch: 72	train loss: 18242.326171875	(30.4s)
epoch: 73	train loss: 18280.064453125	(30.3s)
epoch: 74	train loss: 18262.248046875	(30.5s)
epoch: 75	train loss: 18946.814453125	(30.5s)
epoch: 76	train loss: 19195.8125	(30.6s)
epoch: 77	train loss: 20083.484375	(30.6s)
epoch: 78	train loss: 19152.8984375	(30.4s)
epoch: 79	train loss: 18787.0546875	(30.4s)
epoch: 80	train loss: 18719.498046875	(30.3s)
epoch: 81	train loss: 18315.220703125	(30.4s)
epoch: 82	train loss: 17941.02734375	(30.5s)
epoch: 83	train loss: 17604.80078125	(30.5s)
epoch: 84	train loss: 17453.75	(30.4s)
epoch: 85	train loss: 17384.224609375	(30.6s)
epoch: 86	train loss: 17341.181640625	(30.4s)
epoch: 87	train loss: 17383.708984375	(30.3s)
epoch: 88	train loss: 17516.49609375	(30.3s)
epoch: 89	train loss: 17084.255859375	(30.8s)
epoch: 90	train loss: 16790.58984375	(30.7s)
epoch: 91	train loss: 17359.83984375	(30.3s)
epoch: 92	train loss: 16916.12890625	(30.3s)
epoch: 93	train loss: 16792.1640625	(30.4s)
epoch: 94	train loss: 17142.1796875	(30.6s)
epoch: 95	train loss: 17014.412109375	(30.4s)
epoch: 96	train loss: 17070.693359375	(30.5s)
epoch: 97	train loss: 17290.3046875	(30.4s)
epoch: 98	train loss: 17070.541015625	(30.5s)
epoch: 99	train loss: 19520.484375	(30.3s)
Evaluating model on 200 episodes
0.0012247269239903996
0.0023663341999053955
0.0016326339488538604
0.000988338209156479
0.002584411663216694
0.0012994232727743854
0.0005986272947418249
0.0016200319314521039
0.00027824246644740923
0.0015656100493453612
0.0011922891664488072
0.0015319911368957644
0.0015736252283274482
0.0016732282221710193
0.002066338792019451
0.004723063185034941
0.0009500559725616451
0.0009835741333922164
0.003403003851417452
0.0035603392686122484
0.00413426682644058
0.000530100070384585
0.001831698594077562
0.004045464420212536
0.0020000224394607358
0.001148208326776512
0.0010644491475042222
0.0014155429850619
0.001394165982674167
0.0006450671356684655
0.002921593110335784
0.002935581231213291
0.00392932229260623
0.0008812612421544534
0.0008214753936044872
0.0044236868681244755
0.0015653690235922113
0.001474080367188435
Solved 38/200 episodes
0.00036488622186922766
Evaluated model in 25.4 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-1
Round 2
Generated trajectories in 102.9 seconds
epoch: 0	train loss: 38617.2734375	(30.4s)
epoch: 1	train loss: 33205.90625	(30.3s)
epoch: 2	train loss: 30573.859375	(30.5s)
epoch: 3	train loss: 29106.998046875	(30.3s)
epoch: 4	train loss: 27817.60546875	(30.5s)
epoch: 5	train loss: 27001.53125	(30.5s)
epoch: 6	train loss: 26158.2578125	(30.7s)
epoch: 7	train loss: 25624.556640625	(30.5s)
epoch: 8	train loss: 25259.8359375	(30.4s)
epoch: 9	train loss: 24502.107421875	(30.5s)
epoch: 10	train loss: 24092.345703125	(30.6s)
epoch: 11	train loss: 23887.30078125	(30.3s)
epoch: 12	train loss: 23328.5	(30.4s)
epoch: 13	train loss: 23029.1640625	(30.4s)
epoch: 14	train loss: 22927.900390625	(30.6s)
epoch: 15	train loss: 22284.794921875	(30.4s)
epoch: 16	train loss: 22218.400390625	(30.4s)
epoch: 17	train loss: 21796.509765625	(30.6s)
epoch: 18	train loss: 21805.548828125	(30.6s)
epoch: 19	train loss: 21740.771484375	(30.6s)
epoch: 20	train loss: 21220.287109375	(30.5s)
epoch: 21	train loss: 20887.46484375	(31.7s)
epoch: 22	train loss: 20835.6875	(30.5s)
epoch: 23	train loss: 21918.119140625	(30.4s)
epoch: 24	train loss: 25714.294921875	(30.4s)
epoch: 25	train loss: 22085.25	(30.4s)
epoch: 26	train loss: 20923.111328125	(30.5s)
epoch: 27	train loss: 20525.357421875	(30.5s)
epoch: 28	train loss: 20201.978515625	(30.4s)
epoch: 29	train loss: 19851.19140625	(30.4s)
epoch: 30	train loss: 19445.697265625	(30.4s)
epoch: 31	train loss: 19101.013671875	(30.4s)
epoch: 32	train loss: 18990.33203125	(30.4s)
epoch: 33	train loss: 18513.705078125	(30.4s)
epoch: 34	train loss: 18159.734375	(30.5s)
epoch: 35	train loss: 18307.583984375	(30.4s)
epoch: 36	train loss: 18408.28125	(30.4s)
epoch: 37	train loss: 18661.71484375	(30.4s)
epoch: 38	train loss: 18167.30078125	(30.5s)
epoch: 39	train loss: 17866.076171875	(30.5s)
epoch: 40	train loss: 18005.712890625	(30.4s)
epoch: 41	train loss: 17509.7578125	(30.4s)
epoch: 42	train loss: 17273.7890625	(30.4s)
epoch: 43	train loss: 17201.287109375	(30.4s)
epoch: 44	train loss: 17317.298828125	(30.3s)
epoch: 45	train loss: 17677.365234375	(30.3s)
epoch: 46	train loss: 17444.779296875	(30.3s)
epoch: 47	train loss: 17155.947265625	(30.3s)
epoch: 48	train loss: 17800.130859375	(30.4s)
epoch: 49	train loss: 19078.9765625	(30.5s)
epoch: 50	train loss: 17440.998046875	(30.4s)
epoch: 51	train loss: 17190.587890625	(30.5s)
epoch: 52	train loss: 16902.19921875	(30.4s)
epoch: 53	train loss: 16757.498046875	(30.4s)
epoch: 54	train loss: 17094.97265625	(30.4s)
epoch: 55	train loss: 16634.484375	(30.4s)
epoch: 56	train loss: 16714.849609375	(30.4s)
epoch: 57	train loss: 16268.9462890625	(30.4s)
epoch: 58	train loss: 16234.5009765625	(30.4s)
epoch: 59	train loss: 16309.3525390625	(30.5s)
epoch: 60	train loss: 15884.9638671875	(30.4s)
epoch: 61	train loss: 15925.3291015625	(30.5s)
epoch: 62	train loss: 16630.052734375	(30.4s)
epoch: 63	train loss: 15783.1845703125	(30.5s)
epoch: 64	train loss: 15765.3525390625	(30.4s)
epoch: 65	train loss: 15255.9833984375	(30.4s)
epoch: 66	train loss: 15141.603515625	(30.4s)
epoch: 67	train loss: 15178.19921875	(30.3s)
epoch: 68	train loss: 14762.62109375	(30.3s)
epoch: 69	train loss: 14795.064453125	(30.3s)
epoch: 70	train loss: 14824.689453125	(30.3s)
epoch: 71	train loss: 14903.109375	(30.4s)
epoch: 72	train loss: 14828.42578125	(30.3s)
epoch: 73	train loss: 15006.328125	(30.3s)
epoch: 74	train loss: 14473.318359375	(30.3s)
epoch: 75	train loss: 14704.6787109375	(30.3s)
epoch: 76	train loss: 15038.93359375	(30.4s)
epoch: 77	train loss: 14711.2421875	(30.3s)
epoch: 78	train loss: 14504.21484375	(30.3s)
epoch: 79	train loss: 14326.5205078125	(30.4s)
epoch: 80	train loss: 14355.373046875	(30.5s)
epoch: 81	train loss: 15402.412109375	(30.4s)
epoch: 82	train loss: 14677.1171875	(30.3s)
epoch: 83	train loss: 14338.94140625	(30.3s)
epoch: 84	train loss: 14427.0830078125	(30.3s)
epoch: 85	train loss: 14363.470703125	(30.3s)
epoch: 86	train loss: 14040.20703125	(30.4s)
epoch: 87	train loss: 14271.2919921875	(30.3s)
epoch: 88	train loss: 15423.51953125	(30.3s)
epoch: 89	train loss: 14343.6806640625	(30.2s)
epoch: 90	train loss: 13933.3466796875	(30.4s)
epoch: 91	train loss: 13736.0703125	(31.6s)
epoch: 92	train loss: 13583.119140625	(30.3s)
epoch: 93	train loss: 13614.5185546875	(30.3s)
epoch: 94	train loss: 13467.9482421875	(30.2s)
epoch: 95	train loss: 14027.2861328125	(30.2s)
epoch: 96	train loss: 13480.6181640625	(30.3s)
epoch: 97	train loss: 13304.8330078125	(30.3s)
epoch: 98	train loss: 13026.4755859375	(30.3s)
epoch: 99	train loss: 12862.869140625	(30.2s)
Evaluating model on 200 episodes
0.0025484685632233906
0.0023081425460986793
0.0008091693365713581
0.0021552113044890574
0.0036071395879844204
0.0030897468594568117
0.0008517335409123916
0.0035751143645029516
0.0011796512961154803
0.0023029033053556785
0.0016138561404659413
0.0020771095994859936
0.002060013048928037
0.0012155922420788556
0.0021778487178380603
0.00313511812237266
0.002762060207411802
0.0020703585032606497
0.0023711544240035843
0.004425855571753345
0.001711856961871187
0.0036744379128019014
0.0011725944103091023
0.0034073154986669707
0.00228698834325769
0.0014728155030752533
0.0020206036424497142
0.0032052985068944774
0.0028328341296097884
0.0028747633380893
0.004218766208326157
0.0028950272958768023
0.0017388700377653419
0.0029169408763454366
0.0015671126093366183
0.0013498778051663457
0.002635129527854068
0.0020467779715545474
0.0016574500592209265
0.002773692292976193
Solved 40/200 episodes
0.0004739770010687847
Evaluated model in 28.4 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-2
Round 3
Generated trajectories in 101.5 seconds
epoch: 0	train loss: 43480.66015625	(30.1s)
epoch: 1	train loss: 34861.546875	(30.2s)
epoch: 2	train loss: 31725.837890625	(30.4s)
epoch: 3	train loss: 29693.0078125	(30.3s)
epoch: 4	train loss: 28300.31640625	(30.4s)
epoch: 5	train loss: 27018.80078125	(30.4s)
epoch: 6	train loss: 26009.265625	(30.5s)
epoch: 7	train loss: 25285.357421875	(30.5s)
epoch: 8	train loss: 24674.771484375	(30.5s)
epoch: 9	train loss: 24228.095703125	(30.5s)
epoch: 10	train loss: 23689.845703125	(30.5s)
epoch: 11	train loss: 23441.6328125	(30.5s)
epoch: 12	train loss: 23039.357421875	(30.5s)
epoch: 13	train loss: 23053.373046875	(30.5s)
epoch: 14	train loss: 22397.044921875	(30.6s)
epoch: 15	train loss: 23147.943359375	(30.5s)
epoch: 16	train loss: 22033.498046875	(30.6s)
epoch: 17	train loss: 21444.021484375	(30.6s)
epoch: 18	train loss: 20851.865234375	(30.5s)
epoch: 19	train loss: 20614.73046875	(30.4s)
epoch: 20	train loss: 20251.619140625	(30.5s)
epoch: 21	train loss: 19993.943359375	(30.5s)
epoch: 22	train loss: 19700.517578125	(30.6s)
epoch: 23	train loss: 19669.912109375	(30.4s)
epoch: 24	train loss: 19411.47265625	(30.5s)
epoch: 25	train loss: 19278.9765625	(30.5s)
epoch: 26	train loss: 19741.234375	(30.4s)
epoch: 27	train loss: 18902.4453125	(30.6s)
epoch: 28	train loss: 18521.35546875	(30.5s)
epoch: 29	train loss: 20315.39453125	(30.5s)
epoch: 30	train loss: 18919.5234375	(30.5s)
epoch: 31	train loss: 18554.587890625	(30.5s)
epoch: 32	train loss: 17844.984375	(30.6s)
epoch: 33	train loss: 17444.7109375	(30.5s)
epoch: 34	train loss: 17684.951171875	(30.6s)
epoch: 35	train loss: 17198.466796875	(30.5s)
epoch: 36	train loss: 17426.677734375	(30.4s)
epoch: 37	train loss: 16687.154296875	(30.6s)
epoch: 38	train loss: 16696.771484375	(30.5s)
epoch: 39	train loss: 16609.83984375	(30.5s)
epoch: 40	train loss: 16861.939453125	(30.5s)
epoch: 41	train loss: 16758.458984375	(30.5s)
epoch: 42	train loss: 16774.212890625	(30.6s)
epoch: 43	train loss: 16231.3837890625	(30.5s)
epoch: 44	train loss: 16122.359375	(30.6s)
epoch: 45	train loss: 16110.12890625	(30.5s)
epoch: 46	train loss: 16345.5712890625	(30.5s)
epoch: 47	train loss: 16698.296875	(30.6s)
epoch: 48	train loss: 16514.697265625	(30.5s)
epoch: 49	train loss: 16174.3154296875	(30.5s)
epoch: 50	train loss: 15463.2294921875	(30.5s)
epoch: 51	train loss: 15174.4375	(30.5s)
epoch: 52	train loss: 15817.40234375	(30.6s)
epoch: 53	train loss: 15207.2421875	(30.5s)
epoch: 54	train loss: 15235.392578125	(30.6s)
epoch: 55	train loss: 14897.67578125	(30.5s)
epoch: 56	train loss: 15218.9248046875	(30.5s)
epoch: 57	train loss: 15543.3486328125	(30.5s)
epoch: 58	train loss: 14922.625	(30.5s)
epoch: 59	train loss: 14984.1708984375	(30.6s)
epoch: 60	train loss: 14741.3310546875	(30.4s)
epoch: 61	train loss: 14717.173828125	(30.5s)
epoch: 62	train loss: 14487.927734375	(30.6s)
epoch: 63	train loss: 15303.4716796875	(30.5s)
epoch: 64	train loss: 14509.4033203125	(30.5s)
epoch: 65	train loss: 14038.689453125	(30.5s)
epoch: 66	train loss: 13961.494140625	(30.4s)
epoch: 67	train loss: 13636.935546875	(30.6s)
epoch: 68	train loss: 13637.880859375	(30.5s)
epoch: 69	train loss: 15207.0576171875	(30.6s)
epoch: 70	train loss: 14586.322265625	(30.5s)
epoch: 71	train loss: 13844.0068359375	(30.6s)
epoch: 72	train loss: 13599.2734375	(30.6s)
epoch: 73	train loss: 13464.134765625	(30.5s)
epoch: 74	train loss: 13487.07421875	(30.5s)
epoch: 75	train loss: 13647.1884765625	(30.5s)
epoch: 76	train loss: 13682.400390625	(30.5s)
epoch: 77	train loss: 13753.7705078125	(30.6s)
epoch: 78	train loss: 14053.5302734375	(30.5s)
epoch: 79	train loss: 14315.099609375	(30.5s)
epoch: 80	train loss: 13616.3330078125	(30.5s)
epoch: 81	train loss: 13371.6923828125	(30.5s)
epoch: 82	train loss: 13227.822265625	(30.6s)
epoch: 83	train loss: 13306.0185546875	(30.5s)
epoch: 84	train loss: 13066.689453125	(30.6s)
epoch: 85	train loss: 13548.943359375	(30.5s)
epoch: 86	train loss: 13367.232421875	(30.5s)
epoch: 87	train loss: 13688.537109375	(30.6s)
epoch: 88	train loss: 13091.892578125	(30.5s)
epoch: 89	train loss: 13933.7451171875	(30.5s)
epoch: 90	train loss: 13125.7470703125	(30.5s)
epoch: 91	train loss: 12872.6884765625	(30.5s)
epoch: 92	train loss: 12739.2119140625	(30.6s)
epoch: 93	train loss: 12736.185546875	(30.5s)
epoch: 94	train loss: 12417.3427734375	(30.6s)
epoch: 95	train loss: 12489.298828125	(30.5s)
epoch: 96	train loss: 12495.83984375	(30.5s)
epoch: 97	train loss: 12352.841796875	(30.5s)
epoch: 98	train loss: 12345.5400390625	(30.5s)
epoch: 99	train loss: 12456.5556640625	(30.6s)
Evaluating model on 200 episodes
0.0021656680895830505
0.0021265634237682784
0.0018998790874320548
0.001839738854161826
0.0010758925054688007
0.0010841166974084142
0.0025694846803422217
0.0013069244620661873
0.0013627435716140706
0.001310086217100351
0.0021594713398371823
0.0010860575535584108
0.002521093598261359
0.0009723357864762269
0.0004950041372715284
0.0009519191085322139
0.00181204994393435
0.001376832226841626
0.001581915997828926
0.001932747651153477
0.0026049579187794893
0.001121069895556762
0.0023896443474638674
0.001586842375607895
0.0009628923968800033
0.0018917279521701857
0.0010233387456537457
0.0014400258252862841
0.0005182493511132303
0.0019737676848308183
0.0010135952529708447
0.004785920444555813
0.0013224372778495308
0.0008837457389745396
0.0005774868007971567
0.0017789855355658801
0.0022485445321459943
0.0008241044239471911
Solved 38/200 episodes
0.0003028893071639489
Evaluated model in 25.9 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-3
Round 4
Generated trajectories in 100.9 seconds
epoch: 0	train loss: 42527.0859375	(30.1s)
epoch: 1	train loss: 34098.46484375	(30.1s)
epoch: 2	train loss: 30571.310546875	(30.0s)
epoch: 3	train loss: 28257.560546875	(30.1s)
epoch: 4	train loss: 26494.65625	(30.1s)
epoch: 5	train loss: 25207.478515625	(30.1s)
epoch: 6	train loss: 24328.888671875	(30.3s)
epoch: 7	train loss: 23221.27734375	(30.2s)
epoch: 8	train loss: 22835.76953125	(30.2s)
epoch: 9	train loss: 21986.919921875	(30.2s)
epoch: 10	train loss: 21230.0546875	(30.2s)
epoch: 11	train loss: 21151.517578125	(30.3s)
epoch: 12	train loss: 20353.67578125	(30.3s)
epoch: 13	train loss: 19973.443359375	(30.3s)
epoch: 14	train loss: 19646.150390625	(30.2s)
epoch: 15	train loss: 18780.333984375	(30.2s)
epoch: 16	train loss: 18575.419921875	(30.3s)
epoch: 17	train loss: 18084.53515625	(30.2s)
epoch: 18	train loss: 17775.037109375	(30.2s)
epoch: 19	train loss: 17666.580078125	(30.2s)
epoch: 20	train loss: 17578.552734375	(30.2s)
epoch: 21	train loss: 17058.23828125	(30.3s)
epoch: 22	train loss: 17465.7890625	(30.2s)
epoch: 23	train loss: 17059.015625	(30.2s)
epoch: 24	train loss: 16303.7314453125	(30.2s)
epoch: 25	train loss: 15915.7958984375	(30.3s)
epoch: 26	train loss: 16030.9453125	(30.3s)
epoch: 27	train loss: 15777.693359375	(30.2s)
epoch: 28	train loss: 15543.0908203125	(30.2s)
epoch: 29	train loss: 14993.4716796875	(30.2s)
epoch: 30	train loss: 16436.2578125	(30.2s)
epoch: 31	train loss: 15644.0205078125	(30.3s)
epoch: 32	train loss: 14735.48046875	(30.2s)
epoch: 33	train loss: 14187.1865234375	(30.3s)
epoch: 34	train loss: 14111.8525390625	(30.2s)
epoch: 35	train loss: 13818.0947265625	(30.2s)
epoch: 36	train loss: 13928.83203125	(30.3s)
epoch: 37	train loss: 14135.341796875	(30.2s)
epoch: 38	train loss: 13737.6083984375	(30.2s)
epoch: 39	train loss: 13423.728515625	(30.2s)
epoch: 40	train loss: 13525.0927734375	(30.2s)
epoch: 41	train loss: 13882.6201171875	(30.3s)
epoch: 42	train loss: 13679.0458984375	(30.2s)
epoch: 43	train loss: 13298.578125	(30.2s)
epoch: 44	train loss: 13610.1513671875	(30.2s)
epoch: 45	train loss: 13298.5986328125	(30.2s)
epoch: 46	train loss: 12912.9208984375	(30.3s)
epoch: 47	train loss: 12852.173828125	(30.2s)
epoch: 48	train loss: 13590.314453125	(30.2s)
epoch: 49	train loss: 12883.0615234375	(30.2s)
epoch: 50	train loss: 12833.30859375	(30.3s)
epoch: 51	train loss: 12611.6572265625	(30.3s)
epoch: 52	train loss: 12215.015625	(30.2s)
epoch: 53	train loss: 11943.5029296875	(30.2s)
epoch: 54	train loss: 11498.6240234375	(30.2s)
epoch: 55	train loss: 11521.8583984375	(30.2s)
epoch: 56	train loss: 12162.1435546875	(30.3s)
epoch: 57	train loss: 11759.3583984375	(30.2s)
epoch: 58	train loss: 12256.140625	(30.2s)
epoch: 59	train loss: 11317.2255859375	(30.2s)
epoch: 60	train loss: 11216.783203125	(30.2s)
epoch: 61	train loss: 10886.126953125	(30.3s)
epoch: 62	train loss: 11022.6416015625	(30.2s)
epoch: 63	train loss: 10881.390625	(30.2s)
epoch: 64	train loss: 10712.7939453125	(30.2s)
epoch: 65	train loss: 10832.615234375	(30.3s)
epoch: 66	train loss: 11058.828125	(30.3s)
epoch: 67	train loss: 10774.8134765625	(30.2s)
epoch: 68	train loss: 10694.3828125	(30.2s)
epoch: 69	train loss: 10600.3251953125	(30.2s)
epoch: 70	train loss: 11395.970703125	(30.3s)
epoch: 71	train loss: 10548.4033203125	(30.3s)
epoch: 72	train loss: 10139.830078125	(30.2s)
epoch: 73	train loss: 10008.71875	(30.2s)
epoch: 74	train loss: 9992.06640625	(30.3s)
epoch: 75	train loss: 10244.513671875	(30.3s)
epoch: 76	train loss: 10016.8017578125	(30.3s)
epoch: 77	train loss: 10405.5205078125	(30.2s)
epoch: 78	train loss: 10573.859375	(30.2s)
epoch: 79	train loss: 10316.625	(30.3s)
epoch: 80	train loss: 9711.806640625	(30.2s)
epoch: 81	train loss: 10585.38671875	(30.3s)
epoch: 82	train loss: 10187.1484375	(30.2s)
epoch: 83	train loss: 9703.044921875	(30.2s)
epoch: 84	train loss: 9814.2666015625	(30.3s)
epoch: 85	train loss: 9259.1982421875	(30.3s)
epoch: 86	train loss: 9322.240234375	(30.3s)
epoch: 87	train loss: 9037.3212890625	(30.2s)
epoch: 88	train loss: 9114.6435546875	(30.3s)
epoch: 89	train loss: 9184.0048828125	(30.3s)
epoch: 90	train loss: 9066.1748046875	(30.3s)
epoch: 91	train loss: 9109.943359375	(30.3s)
epoch: 92	train loss: 8673.0537109375	(30.2s)
epoch: 93	train loss: 8532.962890625	(30.3s)
epoch: 94	train loss: 8691.4423828125	(30.3s)
epoch: 95	train loss: 8578.6328125	(30.3s)
epoch: 96	train loss: 8832.955078125	(30.3s)
epoch: 97	train loss: 8798.6669921875	(30.2s)
epoch: 98	train loss: 9597.58203125	(30.3s)
epoch: 99	train loss: 9426.466796875	(30.4s)
Evaluating model on 200 episodes
0.0013560088717734769
0.0014564933923846932
0.002213019175667733
0.0011569025480543803
0.0014063495909795165
0.001203981612282708
0.0016177304810712071
0.0010508772338653216
0.0010608224339583622
0.0024725848720663635
0.0006415677958607881
0.0033171196300827432
0.0005273652215691982
0.0011485641405574586
0.0014056897023692727
0.0029543353458783323
0.0013433532021736028
0.0027015386046567824
0.0019148282347152034
0.0018999610154423862
0.0010814166180352913
0.0012311620339460205
0.001383790753179175
0.0020751279761316256
0.0015987579985945063
0.0016530078748473898
0.0009347369843453635
0.0019114644082340723
0.001462579256137057
0.001580720648538166
0.002277801191667095
0.004052542652061675
0.0022346943063894288
0.0029826055106241256
0.0010223588415101403
0.0020644161086238455
0.001402675052425669
0.0008159644535226107
0.0030359188224085504
0.0014858902262252134
0.0018549890474577507
0.0015657549478679097
0.0010807812175698927
0.0016245909816158626
0.0020707579647673163
0.0017087626416468992
0.0008545764859181046
0.0009520429778300846
0.0011691012950905133
0.0021105104078742443
0.0015670745497118332
0.002025457059601279
0.0017427535494789481
0.001367155803354063
0.0020448220234624494
0.000776935635561434
0.0009375480254271275
Solved 57/200 episodes
0.0004729516971854613
Evaluated model in 29.8 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-4
Round 5
Generated trajectories in 100.8 seconds
epoch: 0	train loss: 32153.81640625	(30.1s)
epoch: 1	train loss: 24811.806640625	(30.0s)
epoch: 2	train loss: 21199.8671875	(30.2s)
epoch: 3	train loss: 19081.380859375	(30.2s)
epoch: 4	train loss: 17784.43359375	(30.1s)
epoch: 5	train loss: 16449.431640625	(30.2s)
epoch: 6	train loss: 15715.0234375	(30.1s)
epoch: 7	train loss: 14779.2431640625	(30.3s)
epoch: 8	train loss: 14074.205078125	(30.4s)
epoch: 9	train loss: 13087.134765625	(30.3s)
epoch: 10	train loss: 12652.6728515625	(30.3s)
epoch: 11	train loss: 12426.671875	(30.2s)
epoch: 12	train loss: 11967.0234375	(30.3s)
epoch: 13	train loss: 11496.677734375	(30.4s)
epoch: 14	train loss: 11172.455078125	(30.3s)
epoch: 15	train loss: 11358.0712890625	(30.3s)
epoch: 16	train loss: 10759.892578125	(30.2s)
epoch: 17	train loss: 10340.56640625	(31.2s)
epoch: 18	train loss: 10076.4072265625	(30.4s)
epoch: 19	train loss: 10016.408203125	(30.3s)
epoch: 20	train loss: 9728.048828125	(30.3s)
epoch: 21	train loss: 10448.189453125	(30.2s)
epoch: 22	train loss: 9659.611328125	(30.3s)
epoch: 23	train loss: 9329.966796875	(30.4s)
epoch: 24	train loss: 8943.3203125	(30.3s)
epoch: 25	train loss: 8962.314453125	(30.3s)
epoch: 26	train loss: 8587.4228515625	(30.2s)
epoch: 27	train loss: 8750.02734375	(30.3s)
epoch: 28	train loss: 8484.17578125	(30.3s)
epoch: 29	train loss: 8881.40625	(30.3s)
epoch: 30	train loss: 8503.2744140625	(30.5s)
epoch: 31	train loss: 8493.671875	(30.4s)
epoch: 32	train loss: 7967.3125	(30.3s)
epoch: 33	train loss: 7959.43896484375	(30.4s)
epoch: 34	train loss: 7549.75439453125	(30.3s)
epoch: 35	train loss: 7145.12060546875	(30.3s)
epoch: 36	train loss: 7346.50390625	(30.2s)
epoch: 37	train loss: 7754.28955078125	(30.3s)
epoch: 38	train loss: 6839.18408203125	(30.3s)
epoch: 39	train loss: 6790.20751953125	(30.3s)
epoch: 40	train loss: 6480.5888671875	(30.3s)
epoch: 41	train loss: 6532.64306640625	(30.2s)
epoch: 42	train loss: 6293.80322265625	(30.3s)
epoch: 43	train loss: 6660.9658203125	(30.3s)
epoch: 44	train loss: 6672.61181640625	(30.3s)
epoch: 45	train loss: 6457.177734375	(30.3s)
epoch: 46	train loss: 5942.58154296875	(30.2s)
epoch: 47	train loss: 5851.13525390625	(30.3s)
epoch: 48	train loss: 6254.298828125	(30.3s)
epoch: 49	train loss: 6104.74755859375	(30.2s)
epoch: 50	train loss: 5669.30419921875	(30.3s)
epoch: 51	train loss: 6061.3251953125	(30.2s)
epoch: 52	train loss: 6045.06884765625	(30.3s)
epoch: 53	train loss: 5981.85205078125	(30.3s)
epoch: 54	train loss: 6054.5439453125	(30.3s)
epoch: 55	train loss: 6288.6435546875	(30.3s)
epoch: 56	train loss: 5693.3544921875	(30.2s)
epoch: 57	train loss: 5466.1767578125	(30.3s)
epoch: 58	train loss: 5451.201171875	(30.3s)
epoch: 59	train loss: 5498.0068359375	(30.2s)
epoch: 60	train loss: 5331.6611328125	(30.3s)
epoch: 61	train loss: 5413.58154296875	(30.2s)
epoch: 62	train loss: 6200.7744140625	(30.3s)
epoch: 63	train loss: 5834.470703125	(30.3s)
epoch: 64	train loss: 6482.826171875	(30.3s)
epoch: 65	train loss: 5924.666015625	(30.3s)
epoch: 66	train loss: 5625.650390625	(30.3s)
epoch: 67	train loss: 5089.28271484375	(30.3s)
epoch: 68	train loss: 5488.982421875	(30.3s)
epoch: 69	train loss: 5044.4990234375	(30.2s)
epoch: 70	train loss: 5380.2041015625	(30.3s)
epoch: 71	train loss: 5235.4482421875	(30.2s)
epoch: 72	train loss: 5175.28662109375	(30.3s)
epoch: 73	train loss: 5108.6650390625	(30.3s)
epoch: 74	train loss: 4850.35693359375	(30.2s)
epoch: 75	train loss: 4750.775390625	(30.2s)
epoch: 76	train loss: 5099.18310546875	(30.3s)
epoch: 77	train loss: 4452.19677734375	(30.3s)
epoch: 78	train loss: 4811.744140625	(30.4s)
epoch: 79	train loss: 4272.73974609375	(30.3s)
epoch: 80	train loss: 4521.07373046875	(30.2s)
epoch: 81	train loss: 4339.67138671875	(30.3s)
epoch: 82	train loss: 4145.916015625	(30.3s)
epoch: 83	train loss: 4550.6435546875	(30.3s)
epoch: 84	train loss: 4423.328125	(30.2s)
epoch: 85	train loss: 4478.59814453125	(30.3s)
epoch: 86	train loss: 4672.1865234375	(30.3s)
epoch: 87	train loss: 4988.5087890625	(30.3s)
epoch: 88	train loss: 4270.99072265625	(30.4s)
epoch: 89	train loss: 4012.73486328125	(30.2s)
epoch: 90	train loss: 4337.62109375	(30.3s)
epoch: 91	train loss: 4136.9541015625	(30.3s)
epoch: 92	train loss: 4125.22509765625	(30.3s)
epoch: 93	train loss: 3820.887939453125	(30.3s)
epoch: 94	train loss: 4681.01904296875	(30.3s)
epoch: 95	train loss: 3990.389404296875	(30.3s)
epoch: 96	train loss: 4113.04638671875	(30.3s)
epoch: 97	train loss: 3857.973876953125	(30.3s)
epoch: 98	train loss: 4028.721923828125	(30.3s)
epoch: 99	train loss: 3638.337646484375	(30.2s)
Evaluating model on 200 episodes
0.0007413538266215861
0.0012965444693691097
0.0010234837053841065
0.0018663198901446197
0.0009560300030508577
0.0010864507484225684
0.001402380268614035
0.001160124670215217
0.001243098658354332
0.0010207945830361929
0.001227367300959051
0.0007887689275578495
0.0017830525936336596
0.0005367065439349972
0.0015955621397138616
0.002800831397897647
0.002288339711495527
0.000656753159135059
0.0009667144982622153
0.0009262897917020988
0.0018433784862281755
0.0006930524848160401
0.0010477940959390253
0.0006201383423883401
0.001096902060549167
0.0007720042090763205
0.0010637273853717488
0.0016910606696001196
0.0005213559604325222
0.00019796915491015706
0.0005066793451796433
0.0007059417839627713
0.0009862145961960778
0.0016909063087950926
0.0009105502322199754
0.0013028673836908472
0.0007160560575464522
0.0008555516673368402
0.000716258971078787
0.00095550419864594
0.0009231200907496743
0.0019514361328792464
0.0017082465942208232
0.0004793155324781158
0.0011444614893452337
0.0008351802190935069
0.0010162017000160024
0.001345343807770405
0.0018940002461352958
0.0013006442032907645
0.0007020750012429744
0.0010592479004571942
0.0004623320846197506
0.0005842840382683789
0.0024736814256440002
0.0006023734383822611
0.0010860431444102273
0.0018435300675371442
0.0007705068661986539
0.0018641119781282864
0.001622130835585267
0.0009512648016425373
0.001982245660959355
0.0011915508964049973
0.0018550501461924375
0.0011576045382071243
0.0006012384777325982
0.0018167583071368022
0.001595172849575695
0.0010033201226633537
0.0008823834548820741
0.0004964509801842117
0.0014199094257492106
0.00028268135388821573
0.001091525793526671
0.00048182106768633403
0.0008834058382944932
0.0010288851144650835
0.0017212548647710885
0.0020512306280124903
0.0016732739258581968
0.0028778396277630235
0.0009679604011277358
0.0013018993347941432
0.00048475144171789
0.0019520114198842617
0.0011963788080417241
0.0012295989743304541
0.0019803449166912205
0.0013864865567383175
0.0007128315576870415
0.0010216735390713438
0.002584762326538718
0.0009483466629507558
0.002879145599581534
0.00045146488737373146
0.0014600370842572374
0.0029997657298736157
Solved 98/200 episodes
0.0006026573709708877
Evaluated model in 36.5 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-5
Round 6
Generated trajectories in 101.7 seconds
epoch: 0	train loss: 20867.103515625	(30.1s)
epoch: 1	train loss: 15056.7060546875	(30.1s)
epoch: 2	train loss: 12543.890625	(30.3s)
epoch: 3	train loss: 11190.3515625	(30.3s)
epoch: 4	train loss: 10434.60546875	(30.2s)
epoch: 5	train loss: 9416.0732421875	(30.1s)
epoch: 6	train loss: 9414.2333984375	(30.1s)
epoch: 7	train loss: 8251.7734375	(30.2s)
epoch: 8	train loss: 7947.359375	(30.2s)
epoch: 9	train loss: 7522.8349609375	(30.3s)
epoch: 10	train loss: 7726.32275390625	(30.2s)
epoch: 11	train loss: 7291.95068359375	(30.3s)
epoch: 12	train loss: 7263.080078125	(30.4s)
epoch: 13	train loss: 6819.017578125	(30.3s)
epoch: 14	train loss: 6946.328125	(30.3s)
epoch: 15	train loss: 7061.04541015625	(30.2s)
epoch: 16	train loss: 6237.20556640625	(30.3s)
epoch: 17	train loss: 6469.53173828125	(30.3s)
epoch: 18	train loss: 5958.1005859375	(30.3s)
epoch: 19	train loss: 5672.361328125	(30.4s)
epoch: 20	train loss: 5712.39306640625	(30.3s)
epoch: 21	train loss: 5904.56787109375	(30.3s)
epoch: 22	train loss: 6117.45751953125	(30.3s)
epoch: 23	train loss: 5235.1025390625	(30.3s)
epoch: 24	train loss: 5356.95068359375	(30.4s)
epoch: 25	train loss: 5374.24609375	(30.3s)
epoch: 26	train loss: 5140.58203125	(30.3s)
epoch: 27	train loss: 4790.373046875	(30.3s)
epoch: 28	train loss: 5026.22021484375	(30.3s)
epoch: 29	train loss: 4930.970703125	(30.4s)
epoch: 30	train loss: 4439.6904296875	(30.3s)
epoch: 31	train loss: 4479.09033203125	(30.3s)
epoch: 32	train loss: 4650.0478515625	(30.2s)
epoch: 33	train loss: 4984.9345703125	(30.3s)
epoch: 34	train loss: 4697.04345703125	(30.4s)
epoch: 35	train loss: 4362.0810546875	(30.3s)
epoch: 36	train loss: 4416.345703125	(30.3s)
epoch: 37	train loss: 4380.80712890625	(30.3s)
epoch: 38	train loss: 4396.51416015625	(30.3s)
epoch: 39	train loss: 4161.3828125	(30.4s)
epoch: 40	train loss: 4274.48828125	(30.3s)
epoch: 41	train loss: 4342.93896484375	(30.3s)
epoch: 42	train loss: 4034.035888671875	(30.2s)
epoch: 43	train loss: 3806.07177734375	(30.3s)
epoch: 44	train loss: 3617.64990234375	(30.4s)
epoch: 45	train loss: 4311.21533203125	(30.3s)
epoch: 46	train loss: 4194.7431640625	(30.4s)
epoch: 47	train loss: 3969.859619140625	(30.3s)
epoch: 48	train loss: 3289.77001953125	(30.3s)
epoch: 49	train loss: 3042.923828125	(30.4s)
epoch: 50	train loss: 3107.152099609375	(30.3s)
epoch: 51	train loss: 3201.828125	(30.3s)
epoch: 52	train loss: 3618.958984375	(30.2s)
epoch: 53	train loss: 4047.6201171875	(30.3s)
epoch: 54	train loss: 3434.45458984375	(30.3s)
epoch: 55	train loss: 3237.65771484375	(30.3s)
epoch: 56	train loss: 2895.023193359375	(30.4s)
epoch: 57	train loss: 3133.70751953125	(30.3s)
epoch: 58	train loss: 3014.640625	(30.3s)
epoch: 59	train loss: 3138.78173828125	(30.3s)
epoch: 60	train loss: 2849.59521484375	(30.4s)
epoch: 61	train loss: 3290.2177734375	(30.4s)
epoch: 62	train loss: 3740.998291015625	(30.3s)
epoch: 63	train loss: 3511.472412109375	(30.3s)
epoch: 64	train loss: 3632.898193359375	(30.2s)
epoch: 65	train loss: 2884.68701171875	(30.3s)
epoch: 66	train loss: 2715.7333984375	(30.4s)
epoch: 67	train loss: 2758.0888671875	(30.3s)
epoch: 68	train loss: 2845.5380859375	(30.3s)
epoch: 69	train loss: 2540.931396484375	(30.2s)
epoch: 70	train loss: 2818.30322265625	(30.3s)
epoch: 71	train loss: 4291.869140625	(30.4s)
epoch: 72	train loss: 2788.92578125	(30.3s)
epoch: 73	train loss: 2437.58935546875	(30.3s)
epoch: 74	train loss: 2263.3193359375	(30.2s)
epoch: 75	train loss: 3011.900146484375	(30.3s)
epoch: 76	train loss: 3058.651123046875	(30.4s)
epoch: 77	train loss: 2423.564208984375	(30.3s)
epoch: 78	train loss: 2287.728271484375	(30.3s)
epoch: 79	train loss: 2131.988525390625	(30.3s)
epoch: 80	train loss: 2614.80126953125	(30.3s)
epoch: 81	train loss: 2283.47314453125	(30.4s)
epoch: 82	train loss: 3333.645263671875	(30.3s)
epoch: 83	train loss: 2352.0771484375	(30.4s)
epoch: 84	train loss: 3692.016845703125	(30.2s)
epoch: 85	train loss: 2187.979736328125	(30.3s)
epoch: 86	train loss: 2681.12890625	(30.4s)
epoch: 87	train loss: 2122.451171875	(30.3s)
epoch: 88	train loss: 2207.4013671875	(30.4s)
epoch: 89	train loss: 1949.6527099609375	(30.3s)
epoch: 90	train loss: 2037.018310546875	(30.3s)
epoch: 91	train loss: 3824.66650390625	(30.4s)
epoch: 92	train loss: 2211.53466796875	(30.3s)
epoch: 93	train loss: 1773.5712890625	(30.4s)
epoch: 94	train loss: 3045.654052734375	(30.3s)
epoch: 95	train loss: 2773.08935546875	(30.3s)
epoch: 96	train loss: 2087.97265625	(30.3s)
epoch: 97	train loss: 1824.16455078125	(30.3s)
epoch: 98	train loss: 2123.7685546875	(30.4s)
epoch: 99	train loss: 2310.92822265625	(30.3s)
Evaluating model on 200 episodes
0.0009808344917213877
0.0011983408515404265
0.0007012885014887615
0.0005893932509273079
0.0011571503727054733
0.001184438419884481
0.0005396266982455112
0.0009506557626082213
0.000799361567680115
0.001289041778287962
0.00045524267673802875
0.0008749536921095569
0.0016579849208937958
0.0005426410356449196
0.000933304646120329
0.0012186025915316596
0.0006210804053158167
0.0013116849101303767
0.0009919885416916497
0.000329107727090807
0.0010702837262215326
0.0009517025569797019
0.0009341980259907561
0.001071728711394826
0.00033804224199229793
0.0011339307020534761
0.0016319988412760722
0.00046499580812884406
0.0006440832715573682
0.00239171480027905
0.0016371608257435986
0.0011023395850497763
0.0013598898522468516
0.0007548433225019835
0.0007082416830598958
0.0004585841204971075
0.0007384726699456223
0.0007409282490016267
0.0009181515216672172
0.0006588490522027804
0.00030332040987559596
0.00048588502441917727
0.00042257790573785314
0.0012680225932450008
0.0013322284173935056
0.0006058581308864702
0.0005977752145712577
0.0008892120423843153
0.0008293541891775517
0.001224443077675956
0.0007098804019699821
0.00035066676788292605
0.0008555485495283696
0.001057364821146779
0.0013154548318457923
0.0008526161403400087
0.0004301855788071407
0.0006562860778558388
0.00115537400051835
0.0013159834571442016
0.0019642515103709336
0.001007311401917832
0.001089734240582295
0.0007760579291789327
0.0013121898466678153
0.0006473532303061802
0.0007183874614485023
0.0008076807052103421
0.0015764094977759454
0.0006983897767081783
0.0005779548928330769
0.0010499094923464949
0.0004450013027183429
0.0008639379405146676
0.0009444937832085998
0.0013691726853721775
0.0009013590155518614
0.000977068976984204
0.000950980501168969
0.001427910266708447
0.0010661281739885453
0.0005579921476468867
0.0009917960241150937
0.0010405806157602153
0.0009042486939279115
0.0017517650431338033
0.0007021285719019943
0.0009105719800572842
0.0020128682981043027
0.001483068011501538
0.0006252775325265248
0.0004522643678986545
0.000569471739936489
0.0005773153539242533
0.000761259943828918
0.0013135598775180471
0.0018916157637755128
0.0008316053072121576
0.0009257252653803923
0.0008882174479367677
0.0008610500658375843
0.0005675982134892944
0.001361819050169502
0.0010507643651530647
0.00121187739013193
0.0007779416101501821
0.0006738129281745243
0.0006584981199045649
0.0009332384261479429
0.00028811243191739777
0.002114110186388037
0.0007248275454643321
0.0007242549140755727
0.0009758934106244969
0.0005405796073318925
0.0012928772782658969
0.0008876967719212795
0.0011273782533680787
0.0013912722482928076
0.00041547229781057325
0.001210641720717831
0.0009112105823893718
0.0007354187375974354
0.0007964207565237302
0.0009305284387664869
0.0011146733328738871
0.001313384113018401
0.001880078559606186
0.0006787364455468142
0.0003552664744678292
0.001006295882689301
0.0006828161337081485
0.0009143821766234136
0.00034057909042097716
0.0011461519293684082
0.0010504509302249755
0.002303961429528759
0.0008031007123968771
0.0010673287244918313
0.0007566476725675303
0.0015913809957882304
0.0008165669976531515
0.0008720164205442416
0.0016540847669198734
0.0007198961752042773
0.0008584644656366436
0.000677479801500643
0.0009238855288588509
0.000965970632684978
0.0008694579886176381
0.0016676840041327523
0.0007283893642392393
0.000635572601822787
Solved 153/200 episodes
0.000733079566809763
Evaluated model in 39.3 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-6
Round 7
Generated trajectories in 102.4 seconds
epoch: 0	train loss: 12021.318359375	(30.5s)
epoch: 1	train loss: 8336.0224609375	(30.5s)
epoch: 2	train loss: 7258.5185546875	(30.5s)
epoch: 3	train loss: 6081.55712890625	(30.7s)
epoch: 4	train loss: 5583.904296875	(30.7s)
epoch: 5	train loss: 5088.7216796875	(30.5s)
epoch: 6	train loss: 4985.9140625	(30.5s)
epoch: 7	train loss: 4804.61083984375	(30.4s)
epoch: 8	train loss: 3859.2734375	(30.5s)
epoch: 9	train loss: 3923.32861328125	(30.7s)
epoch: 10	train loss: 4158.1015625	(30.7s)
epoch: 11	train loss: 5027.162109375	(30.7s)
epoch: 12	train loss: 3564.9873046875	(30.6s)
epoch: 13	train loss: 3757.419921875	(30.6s)
epoch: 14	train loss: 3597.900146484375	(30.7s)
epoch: 15	train loss: 3397.935546875	(30.7s)
epoch: 16	train loss: 3484.475830078125	(30.7s)
epoch: 17	train loss: 3063.837646484375	(30.6s)
epoch: 18	train loss: 2976.19580078125	(30.7s)
epoch: 19	train loss: 3235.119384765625	(30.7s)
epoch: 20	train loss: 3420.118408203125	(30.6s)
epoch: 21	train loss: 5100.5478515625	(30.7s)
epoch: 22	train loss: 4131.85791015625	(30.6s)
epoch: 23	train loss: 3086.291259765625	(30.7s)
epoch: 24	train loss: 3129.343505859375	(30.7s)
epoch: 25	train loss: 2683.430908203125	(30.6s)
epoch: 26	train loss: 3590.568359375	(30.7s)
epoch: 27	train loss: 3036.92822265625	(30.6s)
epoch: 28	train loss: 2320.191162109375	(30.7s)
epoch: 29	train loss: 3240.62841796875	(30.7s)
epoch: 30	train loss: 2742.007568359375	(30.7s)
epoch: 31	train loss: 2374.16357421875	(30.7s)
epoch: 32	train loss: 2621.0615234375	(30.6s)
epoch: 33	train loss: 3689.774169921875	(30.7s)
epoch: 34	train loss: 3119.5712890625	(30.7s)
epoch: 35	train loss: 3281.60888671875	(30.7s)
epoch: 36	train loss: 2661.362548828125	(30.7s)
epoch: 37	train loss: 2122.567626953125	(30.6s)
epoch: 38	train loss: 2464.471923828125	(30.7s)
epoch: 39	train loss: 2644.927490234375	(30.7s)
epoch: 40	train loss: 2645.263671875	(30.7s)
epoch: 41	train loss: 2405.9677734375	(30.7s)
epoch: 42	train loss: 2344.195556640625	(30.6s)
epoch: 43	train loss: 2342.8896484375	(30.6s)
epoch: 44	train loss: 2931.482666015625	(30.7s)
epoch: 45	train loss: 2823.054931640625	(30.7s)
epoch: 46	train loss: 2273.111328125	(30.7s)
epoch: 47	train loss: 2627.8720703125	(30.6s)
epoch: 48	train loss: 2103.189453125	(30.6s)
epoch: 49	train loss: 2651.99072265625	(30.6s)
epoch: 50	train loss: 3604.672119140625	(30.7s)
epoch: 51	train loss: 2093.6728515625	(30.8s)
epoch: 52	train loss: 2067.24072265625	(30.6s)
epoch: 53	train loss: 3362.408447265625	(30.6s)
epoch: 54	train loss: 2297.980712890625	(30.6s)
epoch: 55	train loss: 1806.03759765625	(30.7s)
epoch: 56	train loss: 2923.1181640625	(30.7s)
epoch: 57	train loss: 2299.230224609375	(30.6s)
epoch: 58	train loss: 1877.3167724609375	(30.7s)
epoch: 59	train loss: 2616.007080078125	(30.6s)
epoch: 60	train loss: 1726.1900634765625	(30.7s)
epoch: 61	train loss: 1935.911865234375	(30.7s)
epoch: 62	train loss: 2671.667724609375	(30.5s)
epoch: 63	train loss: 1604.72998046875	(30.7s)
epoch: 64	train loss: 1598.394775390625	(30.6s)
epoch: 65	train loss: 1663.471923828125	(30.7s)
epoch: 66	train loss: 2332.531494140625	(30.7s)
epoch: 67	train loss: 3571.93603515625	(30.6s)
epoch: 68	train loss: 1875.958984375	(30.7s)
epoch: 69	train loss: 2192.251708984375	(30.6s)
epoch: 70	train loss: 1778.4993896484375	(30.7s)
epoch: 71	train loss: 3488.540771484375	(30.7s)
epoch: 72	train loss: 1606.9432373046875	(30.7s)
epoch: 73	train loss: 1338.4444580078125	(30.7s)
epoch: 74	train loss: 1561.8614501953125	(30.6s)
epoch: 75	train loss: 1607.2874755859375	(30.7s)
epoch: 76	train loss: 3295.441650390625	(30.7s)
epoch: 77	train loss: 2221.129638671875	(30.6s)
epoch: 78	train loss: 2517.930419921875	(30.7s)
epoch: 79	train loss: 2963.51806640625	(30.6s)
epoch: 80	train loss: 1564.677734375	(30.7s)
epoch: 81	train loss: 1514.6033935546875	(30.7s)
epoch: 82	train loss: 1563.7415771484375	(30.6s)
epoch: 83	train loss: 1544.379638671875	(30.7s)
epoch: 84	train loss: 1787.0367431640625	(30.6s)
epoch: 85	train loss: 2342.01904296875	(30.7s)
epoch: 86	train loss: 2755.7578125	(30.7s)
epoch: 87	train loss: 2010.294921875	(30.6s)
epoch: 88	train loss: 1636.7530517578125	(30.7s)
epoch: 89	train loss: 2752.345458984375	(30.6s)
epoch: 90	train loss: 2760.714111328125	(30.7s)
epoch: 91	train loss: 1735.2991943359375	(30.7s)
epoch: 92	train loss: 2448.180419921875	(30.7s)
epoch: 93	train loss: 1611.20947265625	(30.7s)
epoch: 94	train loss: 1215.1165771484375	(30.6s)
epoch: 95	train loss: 1642.1923828125	(30.6s)
epoch: 96	train loss: 1584.879150390625	(30.7s)
epoch: 97	train loss: 2324.716552734375	(30.7s)
epoch: 98	train loss: 1957.6495361328125	(30.7s)
epoch: 99	train loss: 1955.927978515625	(30.6s)
Evaluating model on 200 episodes
0.0006110930146612438
0.0009481045902551463
0.001199963445327265
0.0007976548374174924
0.0005465718090168334
0.00045495759322396846
0.0007343105422162056
0.0006073991407902213
0.0006155264118206105
0.000923264221081113
0.0004846304940516487
0.0009378097356602666
0.0005496464225209572
0.0006285983332645628
0.0006781624615541659
0.0009150895930361003
0.001010817414175512
0.00022739844484931382
0.0012955673062771728
0.0010225518478294432
0.0007361569488156503
0.00033117826179867346
0.0008999879225029872
0.0005345046467317098
0.001142767098144759
0.0019339037971803918
0.0007579742976724027
0.0003327436581457732
0.0001685435851565368
0.0007988625899694549
0.0004724104003579324
0.000531438160413183
0.0006475928459015752
0.00036121971697866684
0.0011213158226194631
0.0005117588041850303
0.0013014541177870428
0.0007559615275424317
0.0005787747868453152
0.0013667817303725956
0.0008397056387222715
0.0006829425407387692
0.0014328789583487379
0.0005123819956528072
0.0005867307107461872
0.0009407541517378247
0.001201704269866847
0.0011702882771455084
0.0009433945360169987
0.0008010859048226849
0.0005310432456206079
0.0006789839466768575
0.0009795860344678183
0.00032059318790047923
0.00047463461893270363
0.0006020926904663307
0.0003480610955323625
0.0011685305012179015
0.0006795974273140578
0.0006748407679631121
0.0010843287367606536
0.0005321704929277378
0.0007995763503109069
0.000575804058306468
0.000668852193030034
0.0008310249351779931
0.0007328175005178699
0.0009732265357992478
0.0004851189077507351
0.001530559634375095
0.0008861346478725965
0.0007683252881120538
0.000641157795999949
0.0005578262121895434
0.0005644769965632198
0.0006970746336960207
0.0009627243598515633
0.0006455539434682578
0.0011924652231391519
0.0019637428621687527
0.0009554136592599277
0.00047567115309546983
0.0010312782268599783
0.0006777468089421745
0.000720093503332464
0.0006258182553497854
0.0005910434463823384
0.0007837866392755563
0.0006338773913739715
0.0022701932534151376
0.0011308371263462154
0.0011252194752159994
0.00047429767619178165
0.00035593684219747956
0.00037866139318794013
0.0005996972680376904
0.0006546149673410512
0.00032667092552098135
0.0009873324838736153
0.00022373964784492273
0.0008279860395022893
0.00037007852458503164
0.0008656784535300435
0.0009950693800055887
0.0009167507948652863
0.000760398438989582
0.0008693625014631026
0.00037883945390747515
0.0010649831259528313
0.0009724625532678482
0.000837111270729193
0.00047605254763362207
0.0003136226242449166
0.0008315721509126505
0.0011969926202324971
0.0006780092437346015
0.0007630046508590264
0.00027870477742908406
0.0008693987776842689
0.0009554365607982618
0.0006554421699547675
0.0006235623604879947
0.0004757797754522202
0.0003703825847333064
0.00046351080993645155
0.0012180461127220617
0.0010627744217904715
0.0009972388722752313
0.0006938257053358635
0.0027015903920982963
0.001193043430976104
0.0008472437625944925
0.0005236632964624732
0.0005987311434572413
0.0006728566717356443
0.0012452428318283637
0.0005787031859654235
0.0005909308486783024
0.0011927129493241886
0.0007900575213170668
0.00046293408134564135
0.0005096004128669544
0.0006302664705799543
0.0009527795847524041
0.00034656175103009446
0.0007762978517954858
0.000526689799058928
0.0007055063442749088
0.0010458968559396453
0.000332761612298782
0.0004819785275945573
0.0005987276830247927
0.0011287295475246057
0.00047879327402093304
0.0003536404623850507
0.0007173793530776439
0.0008185922166982588
0.0005969674530206248
0.0005985318553872208
0.0008620694992714561
0.0005655550443120056
0.0006296566257093984
0.0006461034002010516
0.0007918765786598669
0.0007814116995034151
0.0008627388837560305
0.0008814404400514572
0.0011741644578121071
0.0009727250642026775
Solved 169/200 episodes
0.0006555320024835654
Evaluated model in 41.1 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-7
Round 8
Generated trajectories in 102.1 seconds
epoch: 0	train loss: 7391.6748046875	(30.6s)
epoch: 1	train loss: 4725.69677734375	(30.4s)
epoch: 2	train loss: 4479.8916015625	(30.5s)
epoch: 3	train loss: 3910.018798828125	(30.4s)
epoch: 4	train loss: 3558.732421875	(30.5s)
epoch: 5	train loss: 3010.9033203125	(30.4s)
epoch: 6	train loss: 3075.30712890625	(30.4s)
epoch: 7	train loss: 2892.688720703125	(30.4s)
epoch: 8	train loss: 3674.4755859375	(30.3s)
epoch: 9	train loss: 3025.09326171875	(30.4s)
epoch: 10	train loss: 2565.226318359375	(30.3s)
epoch: 11	train loss: 2706.204833984375	(30.5s)
epoch: 12	train loss: 2940.8486328125	(30.5s)
epoch: 13	train loss: 3333.662841796875	(30.5s)
epoch: 14	train loss: 2852.943603515625	(30.5s)
epoch: 15	train loss: 2471.014404296875	(30.4s)
epoch: 16	train loss: 4356.93310546875	(30.4s)
epoch: 17	train loss: 2632.3447265625	(30.5s)
epoch: 18	train loss: 2196.60986328125	(30.5s)
epoch: 19	train loss: 1923.9888916015625	(30.5s)
epoch: 20	train loss: 1808.6285400390625	(30.4s)
epoch: 21	train loss: 1783.234375	(30.4s)
epoch: 22	train loss: 3580.36474609375	(30.4s)
epoch: 23	train loss: 2207.472412109375	(30.4s)
epoch: 24	train loss: 2744.390380859375	(30.5s)
epoch: 25	train loss: 2252.37890625	(30.4s)
epoch: 26	train loss: 1986.795654296875	(30.4s)
epoch: 27	train loss: 1594.4920654296875	(30.4s)
epoch: 28	train loss: 1580.177734375	(30.4s)
epoch: 29	train loss: 1678.15234375	(30.5s)
epoch: 30	train loss: 2036.8648681640625	(30.4s)
epoch: 31	train loss: 2599.2236328125	(30.6s)
epoch: 32	train loss: 2737.630859375	(30.6s)
epoch: 33	train loss: 1768.72900390625	(30.5s)
epoch: 34	train loss: 1637.0911865234375	(30.5s)
epoch: 35	train loss: 1684.7899169921875	(30.4s)
epoch: 36	train loss: 3139.562255859375	(30.6s)
epoch: 37	train loss: 1998.063720703125	(30.4s)
epoch: 38	train loss: 1345.215576171875	(30.5s)
epoch: 39	train loss: 1300.3192138671875	(30.6s)
epoch: 40	train loss: 2011.8016357421875	(30.4s)
epoch: 41	train loss: 3433.0771484375	(30.4s)
epoch: 42	train loss: 1619.7930908203125	(30.4s)
epoch: 43	train loss: 1555.071533203125	(30.4s)
epoch: 44	train loss: 1545.82958984375	(30.5s)
epoch: 45	train loss: 1226.07373046875	(30.4s)
epoch: 46	train loss: 3047.433349609375	(30.5s)
epoch: 47	train loss: 1777.5086669921875	(30.4s)
epoch: 48	train loss: 1569.556396484375	(30.6s)
epoch: 49	train loss: 1644.8951416015625	(30.5s)
epoch: 50	train loss: 1313.182861328125	(30.4s)
epoch: 51	train loss: 1318.1593017578125	(30.4s)
epoch: 52	train loss: 1677.503173828125	(30.5s)
epoch: 53	train loss: 2039.344970703125	(30.6s)
epoch: 54	train loss: 2167.740966796875	(30.5s)
epoch: 55	train loss: 1431.920166015625	(30.4s)
epoch: 56	train loss: 1316.388427734375	(30.4s)
epoch: 57	train loss: 1481.3720703125	(30.4s)
epoch: 58	train loss: 1252.8408203125	(30.5s)
epoch: 59	train loss: 1647.18408203125	(30.5s)
epoch: 60	train loss: 1793.6507568359375	(30.5s)
epoch: 61	train loss: 4165.59814453125	(30.4s)
epoch: 62	train loss: 2149.103515625	(30.4s)
epoch: 63	train loss: 1084.4927978515625	(30.4s)
epoch: 64	train loss: 1025.4105224609375	(30.5s)
epoch: 65	train loss: 972.5531616210938	(30.4s)
epoch: 66	train loss: 1870.3294677734375	(30.5s)
epoch: 67	train loss: 2168.3515625	(30.4s)
epoch: 68	train loss: 1775.9677734375	(30.5s)
epoch: 69	train loss: 2136.792236328125	(30.5s)
epoch: 70	train loss: 1340.5457763671875	(30.4s)
epoch: 71	train loss: 1083.5570068359375	(30.5s)
epoch: 72	train loss: 922.1552734375	(30.4s)
epoch: 73	train loss: 1205.7265625	(30.5s)
epoch: 74	train loss: 1342.3634033203125	(30.5s)
epoch: 75	train loss: 2420.378173828125	(30.4s)
epoch: 76	train loss: 2040.40673828125	(30.4s)
epoch: 77	train loss: 1287.477294921875	(30.4s)
epoch: 78	train loss: 867.5189819335938	(30.4s)
epoch: 79	train loss: 2487.315185546875	(30.5s)
epoch: 80	train loss: 1324.254150390625	(30.4s)
epoch: 81	train loss: 1080.9556884765625	(30.5s)
epoch: 82	train loss: 1629.6829833984375	(30.4s)
epoch: 83	train loss: 3214.214111328125	(30.5s)
epoch: 84	train loss: 1403.9742431640625	(30.5s)
epoch: 85	train loss: 1154.750732421875	(30.4s)
epoch: 86	train loss: 961.9466552734375	(30.4s)
epoch: 87	train loss: 1109.124755859375	(30.4s)
epoch: 88	train loss: 1292.6485595703125	(30.4s)
epoch: 89	train loss: 1366.9000244140625	(30.5s)
epoch: 90	train loss: 1070.432373046875	(30.4s)
epoch: 91	train loss: 1169.1632080078125	(30.6s)
epoch: 92	train loss: 1875.9735107421875	(30.5s)
epoch: 93	train loss: 1646.146728515625	(30.5s)
epoch: 94	train loss: 1096.1968994140625	(30.5s)
epoch: 95	train loss: 1668.76416015625	(30.4s)
epoch: 96	train loss: 1257.6439208984375	(30.4s)
epoch: 97	train loss: 3400.049072265625	(30.4s)
epoch: 98	train loss: 1239.8250732421875	(30.6s)
epoch: 99	train loss: 2068.4921875	(30.5s)
Evaluating model on 200 episodes
0.0005372441945736581
0.0006875553207907778
0.0006548851224215469
0.0005306512193278144
0.0008444610238029782
0.0008465924884512787
0.0010751946071610766
0.00027240096278546844
0.0006596629216425033
0.0008521644689799837
0.0010483457227757863
0.0009802074913750403
0.0004524848646884921
0.0006451564724037391
0.0011518391196580562
0.0007458521076478064
0.00020862109935974743
0.0003563968026962054
0.0006031180620429322
0.0007201889013577741
0.0006748013230309072
0.0008199574312411955
0.000826811314774594
0.0005119623563266085
0.0011119816899372382
0.00040977529198770704
0.0006220725485532471
0.0008169143055359503
0.0005466068868675696
0.0007800157712335932
0.0007528721976086672
0.0006761572217833924
0.0005404018156696111
0.0010659257770731486
0.0009851131105986293
0.0006780448118796037
0.0009413568726673615
0.0006218592064984618
0.0008852108772101163
0.0008365295039270263
0.0008076313373142815
0.001178017290264203
0.00030989548566354
0.0007536876040830975
0.0008461562219963525
0.0012354101580478107
0.0007826389348338126
0.001273150356428232
0.001070260490905639
0.00042442643475974014
0.00028781687360606155
0.0002578069529374174
0.0004762167622845338
0.0007349858916803406
0.0009360755168679382
0.0003099634871735664
0.0010236703725742449
0.0015018359161466058
0.000805652068915411
0.0011116098052298185
0.0009257028414140223
0.0005408007127024812
0.000910248194807483
0.0007644024634222054
0.0003347621222928865
0.00030490903906562987
0.0010343966986928954
0.0007392724050574047
0.0010704130608019113
0.0003704163354996126
0.00045108905875454903
0.0013965272474275767
0.0017645761158333203
0.0007812138716568976
0.0008197832935949032
0.0011400670828152215
0.0006445204132129826
0.0005075262173856269
0.0007339070513738149
0.000868974159829756
0.00043108805372564046
0.0008491752654056958
0.000498503497730216
0.000518939356879855
0.0005910588692672349
0.0015002940126578324
0.0005161073924000448
0.001095534551881649
0.0007389793380651907
0.0003106709552412212
0.0007719285377243068
0.0008126440415026082
0.0006741853903046527
0.0008737208339880453
0.0014223876916124331
0.0006526326583298214
0.0007236964918774901
0.0008759563689636707
0.0002500991733237849
0.0008886938695258533
0.0005515488949680256
0.0015900751740705996
0.0003243790754193189
0.0011158452434756327
0.0007768134924653599
0.00047378166992631223
0.0005798403296801936
0.0004722792468939964
0.0010524390554228038
0.0009302686714363518
0.0008160992546519
0.0009246042658820183
0.0007494865960527224
0.0008928548820785224
0.002017971273744479
0.0004066697411358093
0.001115366461124809
0.0019555918455201513
0.0008414327832204955
0.0006083683205765232
0.0002908675105572911
0.0007439027345893779
0.0003022504604034891
0.0005718589270733954
0.0008887828050449441
0.0007884854302084512
0.0007782879508795304
0.0006091217205721478
0.00038663675900352246
0.0010927913692946524
0.0008439760864803247
0.0009017977220537432
0.0006059004455164541
0.0012822237840737216
0.0007192610140312657
0.00038443541621824767
0.00047581208612731035
0.00046209270124475757
0.0008993705234085306
0.0005762714132136656
0.0004797409766559632
0.0006238969034794453
0.0007686129478922036
0.00043853158713318406
0.0005212399494666897
0.000537262541608167
0.00108836609251739
0.000725186071446972
0.0008757426472477484
0.0008064638346722324
0.0005939864091757314
0.0006031575072224119
0.0009664134588872132
0.00021969966292090248
0.0012075178160052423
0.0010401207263467686
0.000651839827241929
0.0007551602610484613
0.001258244353514535
0.0005938086396054132
0.0008935294389865939
0.0008451293362503003
0.0005080157790399864
0.00042339481005910785
0.0007096912540873745
0.0010790875014208723
0.0007804204284911975
0.0005611132346530212
0.0009332597121381416
0.0006311801909994815
0.0006084060208715007
0.0004631963119027205
0.0006780764194575438
0.0015029823499779532
0.0011936444999654278
0.0002696942229660982
0.0004897720606934924
Solved 177/200 episodes
0.0006766476745494931
Evaluated model in 43.0 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-8
Round 9
Generated trajectories in 102.0 seconds
epoch: 0	train loss: 5153.07568359375	(30.5s)
epoch: 1	train loss: 3453.58984375	(30.4s)
epoch: 2	train loss: 2855.68017578125	(30.6s)
epoch: 3	train loss: 2660.259033203125	(30.5s)
epoch: 4	train loss: 3884.653564453125	(30.6s)
epoch: 5	train loss: 2680.5791015625	(30.7s)
epoch: 6	train loss: 2459.78564453125	(30.6s)
epoch: 7	train loss: 1991.901611328125	(30.5s)
epoch: 8	train loss: 2047.2908935546875	(30.4s)
epoch: 9	train loss: 2325.0439453125	(30.5s)
epoch: 10	train loss: 2237.818115234375	(30.6s)
epoch: 11	train loss: 2363.217041015625	(30.6s)
epoch: 12	train loss: 2108.572998046875	(30.7s)
epoch: 13	train loss: 2454.599853515625	(30.4s)
epoch: 14	train loss: 2243.902587890625	(30.5s)
epoch: 15	train loss: 2780.14306640625	(30.7s)
epoch: 16	train loss: 1666.658203125	(30.6s)
epoch: 17	train loss: 1860.3544921875	(30.6s)
epoch: 18	train loss: 1821.7586669921875	(30.5s)
epoch: 19	train loss: 2060.965576171875	(30.6s)
epoch: 20	train loss: 2262.985595703125	(30.6s)
epoch: 21	train loss: 1645.2086181640625	(30.6s)
epoch: 22	train loss: 1648.284912109375	(30.7s)
epoch: 23	train loss: 1461.2020263671875	(30.6s)
epoch: 24	train loss: 1238.6993408203125	(30.6s)
epoch: 25	train loss: 1276.766357421875	(30.5s)
epoch: 26	train loss: 1910.9244384765625	(30.7s)
epoch: 27	train loss: 2191.567138671875	(30.7s)
epoch: 28	train loss: 2487.496337890625	(30.6s)
epoch: 29	train loss: 1847.8267822265625	(30.5s)
epoch: 30	train loss: 1986.8482666015625	(30.6s)
epoch: 31	train loss: 1608.313720703125	(30.6s)
epoch: 32	train loss: 1871.4207763671875	(30.7s)
epoch: 33	train loss: 1929.6947021484375	(30.6s)
epoch: 34	train loss: 1899.3753662109375	(30.6s)
epoch: 35	train loss: 3631.368896484375	(30.5s)
epoch: 36	train loss: 1659.0037841796875	(30.6s)
epoch: 37	train loss: 1435.0028076171875	(30.7s)
epoch: 38	train loss: 1085.049560546875	(30.6s)
epoch: 39	train loss: 981.4362182617188	(30.6s)
epoch: 40	train loss: 1535.0723876953125	(30.6s)
epoch: 41	train loss: 1758.4053955078125	(30.6s)
epoch: 42	train loss: 1430.212890625	(30.7s)
epoch: 43	train loss: 1872.4625244140625	(30.6s)
epoch: 44	train loss: 1768.0421142578125	(30.6s)
epoch: 45	train loss: 1261.216064453125	(30.5s)
epoch: 46	train loss: 960.3978881835938	(30.7s)
epoch: 47	train loss: 982.6707153320312	(30.7s)
epoch: 48	train loss: 1004.4697875976562	(30.6s)
epoch: 49	train loss: 1031.271484375	(30.6s)
epoch: 50	train loss: 1173.7099609375	(30.8s)
epoch: 51	train loss: 1010.9169921875	(31.1s)
epoch: 52	train loss: 4056.974853515625	(31.6s)
epoch: 53	train loss: 1671.6318359375	(31.0s)
epoch: 54	train loss: 1083.224609375	(31.1s)
epoch: 55	train loss: 921.250732421875	(31.5s)
epoch: 56	train loss: 1128.052734375	(31.6s)
epoch: 57	train loss: 1181.8519287109375	(31.7s)
epoch: 58	train loss: 1346.9700927734375	(31.7s)
epoch: 59	train loss: 1526.5537109375	(31.7s)
epoch: 60	train loss: 1048.6578369140625	(31.7s)
epoch: 61	train loss: 1006.1892700195312	(31.7s)
epoch: 62	train loss: 1289.6162109375	(31.8s)
epoch: 63	train loss: 1792.9810791015625	(31.6s)
epoch: 64	train loss: 1355.896484375	(31.9s)
epoch: 65	train loss: 1570.237060546875	(31.6s)
epoch: 66	train loss: 906.8473510742188	(31.8s)
epoch: 67	train loss: 1120.686279296875	(31.7s)
epoch: 68	train loss: 2002.31884765625	(31.8s)
epoch: 69	train loss: 1457.037109375	(31.3s)
epoch: 70	train loss: 893.049072265625	(31.1s)
epoch: 71	train loss: 761.4154052734375	(31.7s)
epoch: 72	train loss: 1416.2698974609375	(31.6s)
epoch: 73	train loss: 1361.6373291015625	(31.7s)
epoch: 74	train loss: 963.7986450195312	(31.8s)
epoch: 75	train loss: 719.3265380859375	(31.8s)
epoch: 76	train loss: 1432.53857421875	(31.6s)
epoch: 77	train loss: 1089.1522216796875	(31.6s)
epoch: 78	train loss: 2392.261962890625	(31.8s)
epoch: 79	train loss: 1287.8975830078125	(31.7s)
epoch: 80	train loss: 2603.052490234375	(31.8s)
epoch: 81	train loss: 877.3106079101562	(31.7s)
epoch: 82	train loss: 778.0880126953125	(31.6s)
epoch: 83	train loss: 787.2041625976562	(31.8s)
epoch: 84	train loss: 749.2936401367188	(31.8s)
epoch: 85	train loss: 801.225830078125	(31.7s)
epoch: 86	train loss: 805.023193359375	(31.6s)
epoch: 87	train loss: 756.9484252929688	(31.6s)
epoch: 88	train loss: 1846.385009765625	(31.6s)
epoch: 89	train loss: 776.2974853515625	(31.7s)
epoch: 90	train loss: 591.07958984375	(31.7s)
epoch: 91	train loss: 1148.40625	(31.8s)
epoch: 92	train loss: 1065.9727783203125	(31.6s)
epoch: 93	train loss: 903.8555908203125	(30.9s)
epoch: 94	train loss: 2128.141357421875	(30.7s)
epoch: 95	train loss: 1187.069091796875	(31.6s)
epoch: 96	train loss: 821.005615234375	(31.9s)
epoch: 97	train loss: 603.6056518554688	(31.6s)
epoch: 98	train loss: 798.1109008789062	(31.7s)
epoch: 99	train loss: 663.1436767578125	(31.6s)
Evaluating model on 200 episodes
0.0008695933803127529
0.0004010024666740719
0.00035080293337859283
0.00018488493454545582
0.0011077823714320243
0.0006862560601348378
0.0009838756799706908
0.00024040934610335777
0.0010030313377635473
0.00035587548700277694
0.0008801223866612418
0.0011743059553737112
0.0005792104930719688
0.0015847799187251137
0.0005453129884057596
0.0010105988679677703
0.0004454973312493638
0.001258502549376317
0.000572967843254446
0.0012233507742556477
0.0005236986728829964
0.0004770744967572682
0.0002607318123409641
0.0003337608212811909
0.00038187636998069746
0.00015046256203277153
0.0008858345318003558
0.0006692933355390613
0.000696685974854399
0.0003526448208504007
0.0008917702257616946
0.0004049046984214834
0.0008798772381851449
0.00046550283890004114
0.0003063574309635442
0.0009767730538652148
0.0007892878747668419
0.000630654197888553
0.0007150364629394192
0.0010066893534193305
0.0003937896946808905
0.0006959769337339347
0.0007441327512564245
0.0005325405869219443
0.0007950505084227188
0.0011961298867390723
0.0007990891421690725
0.000974523696640972
0.0010294553661272705
0.0007881511812325028
0.0007956974441185594
0.0007747705020240877
0.0009127732341767114
0.0005494773625357014
0.0009221369243732624
0.0005467999666367318
0.00035417529748623826
0.0008377338025032562
0.0009392822110385168
0.0007385451459413162
0.0007029125571794534
0.0009504642265320788
0.00045761994457088325
0.0006395579478186463
0.000567470482482432
0.0011308562104539689
0.0005165567516895761
0.0003466768746355748
0.00045740258533791907
0.0006776544327098039
0.000508449452657563
0.0005101323106608671
0.0009422106987862524
0.0005520771703327467
0.0011625063353676523
0.0006853196225620195
0.0005100298280233277
0.0004477908969420241
0.00040747883896542726
0.0002124969451566964
0.0009721992949744163
0.0004716225343549417
0.00038862119566582135
0.00048767231055535376
0.0005570854998820453
0.0005226513265673324
0.0006331812898565659
0.000790269440650653
0.00025830003930119955
0.0008229648153300171
0.0005397281000477961
0.000332819477459149
0.0003575844611112608
0.00019356493885425152
0.0007395584291641301
0.0008351263293748378
0.000586750016282167
0.0010487553736058747
0.0003127013653379675
0.0009228211077147295
0.000979597168495161
0.0011884509257167033
0.0005444725044244912
0.0006485821548949389
0.0006087113849162051
0.0004608472309115541
0.00036278260500921533
0.0005658778169769599
0.0005153773240782621
0.0007667653055989376
0.0002806155068052855
0.0009684175414887167
0.0006705153031968318
0.0009382333761701981
0.0008311400258987435
0.0006129955979001048
0.0007675858690138714
0.00037311503274395363
0.000432869541760911
0.0006374820555663518
0.00030850838425067195
0.0008211213996742806
0.0007674895843173055
0.0009759406312486345
0.0002537801972266607
0.0005708380150463199
0.0011995990328159678
0.0011868449821901908
0.000359195573037141
0.000456725191660163
0.00042152778989172174
0.0006802507642229709
0.0004658554492958832
0.0011910045563296122
0.0007432838672987904
0.0004895971624894931
0.00016127186124019013
0.00032034042378654705
0.0009647309443129128
0.00041338197883078465
0.0007995148411737319
0.0011929430347663583
0.00045757231098125177
0.0007996907304058207
0.0002643039556027188
0.0009470346000928961
0.0008570642388329599
0.0010874488515355551
0.0003663424158730777
0.0007445710964682141
0.00046973525049907037
0.0004250774170486693
0.00026386018701239004
0.0005522496420174624
0.0003227864007645361
0.0005085108608419922
0.0008826724514879711
0.0010726755682459472
0.0009552452225119958
0.0003972868128130358
0.0009158108430924852
0.00024039169996588802
0.0005223169801231988
0.000934314757159882
0.0002890170565348983
0.00031536979856485104
0.00020513273789219967
0.0006840525654958831
0.0012718236477837763
0.0006078803360521436
0.0008242845685231259
0.0007369381266991176
0.0004935786809558824
0.0005851067892381839
0.0008196130776923383
0.0007427902982913913
0.0003686954148309139
0.00042262043613542727
0.00034197227978438605
0.0006192254395617242
0.0007950060110791431
0.0009711336741020204
Solved 182/200 episodes
0.000598581680730368
Evaluated model in 49.4 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-9
Round 10
Generated trajectories in 109.1 seconds
epoch: 0	train loss: 4259.73291015625	(31.3s)
epoch: 1	train loss: 2998.392578125	(31.5s)
epoch: 2	train loss: 2220.1123046875	(31.3s)
epoch: 3	train loss: 1992.5614013671875	(31.5s)
epoch: 4	train loss: 1832.1944580078125	(31.5s)
epoch: 5	train loss: 2095.186767578125	(31.4s)
epoch: 6	train loss: 2307.287353515625	(31.6s)
epoch: 7	train loss: 1742.83544921875	(31.5s)
epoch: 8	train loss: 1955.6220703125	(31.5s)
epoch: 9	train loss: 2384.227294921875	(31.5s)
epoch: 10	train loss: 1689.800537109375	(31.3s)
epoch: 11	train loss: 1434.855224609375	(31.5s)
epoch: 12	train loss: 1269.8785400390625	(31.5s)
epoch: 13	train loss: 1443.14404296875	(31.5s)
epoch: 14	train loss: 1354.703369140625	(30.7s)
epoch: 15	train loss: 1404.31591796875	(30.5s)
epoch: 16	train loss: 2450.321044921875	(30.7s)
epoch: 17	train loss: 1950.762939453125	(31.4s)
epoch: 18	train loss: 1951.2952880859375	(31.7s)
epoch: 19	train loss: 1899.7679443359375	(31.4s)
epoch: 20	train loss: 1302.6591796875	(31.6s)
epoch: 21	train loss: 1131.6832275390625	(31.4s)
epoch: 22	train loss: 1118.4404296875	(31.6s)
epoch: 23	train loss: 1596.6510009765625	(31.6s)
epoch: 24	train loss: 2607.67626953125	(31.5s)
epoch: 25	train loss: 1760.20068359375	(31.6s)
epoch: 26	train loss: 1319.274169921875	(31.4s)
epoch: 27	train loss: 1113.3397216796875	(31.4s)
epoch: 28	train loss: 1341.0777587890625	(31.7s)
epoch: 29	train loss: 977.9013671875	(31.5s)
epoch: 30	train loss: 1086.53662109375	(31.5s)
epoch: 31	train loss: 920.866455078125	(31.5s)
epoch: 32	train loss: 899.4155883789062	(31.4s)
epoch: 33	train loss: 845.536865234375	(31.5s)
epoch: 34	train loss: 830.2987060546875	(31.5s)
epoch: 35	train loss: 1508.181884765625	(31.6s)
epoch: 36	train loss: 1860.073974609375	(31.5s)
epoch: 37	train loss: 2055.77197265625	(31.5s)
epoch: 38	train loss: 1203.4022216796875	(31.5s)
epoch: 39	train loss: 874.9717407226562	(31.5s)
epoch: 40	train loss: 776.0528564453125	(31.5s)
epoch: 41	train loss: 822.8435668945312	(31.6s)
epoch: 42	train loss: 1170.4486083984375	(31.6s)
epoch: 43	train loss: 1736.7628173828125	(31.4s)
epoch: 44	train loss: 880.314697265625	(30.7s)
epoch: 45	train loss: 1124.8721923828125	(30.5s)
epoch: 46	train loss: 1827.673095703125	(30.5s)
epoch: 47	train loss: 1195.577880859375	(31.3s)
epoch: 48	train loss: 1211.8778076171875	(31.6s)
epoch: 49	train loss: 2773.660888671875	(31.5s)
epoch: 50	train loss: 1243.35693359375	(31.3s)
epoch: 51	train loss: 982.2702026367188	(31.5s)
epoch: 52	train loss: 791.273193359375	(31.6s)
epoch: 53	train loss: 830.0469360351562	(31.4s)
epoch: 54	train loss: 1249.7640380859375	(31.6s)
epoch: 55	train loss: 1130.0667724609375	(31.5s)
epoch: 56	train loss: 1346.716552734375	(31.4s)
epoch: 57	train loss: 1867.250244140625	(31.5s)
epoch: 58	train loss: 856.4759521484375	(31.6s)
epoch: 59	train loss: 580.798095703125	(31.5s)
epoch: 60	train loss: 501.71014404296875	(31.4s)
epoch: 61	train loss: 612.1318969726562	(31.6s)
epoch: 62	train loss: 788.0338745117188	(31.4s)
epoch: 63	train loss: 853.9606323242188	(31.5s)
epoch: 64	train loss: 785.1376953125	(31.6s)
epoch: 65	train loss: 1829.57177734375	(31.5s)
epoch: 66	train loss: 1047.9129638671875	(31.6s)
epoch: 67	train loss: 915.0148315429688	(31.5s)
epoch: 68	train loss: 673.525146484375	(31.4s)
epoch: 69	train loss: 555.9948120117188	(31.7s)
epoch: 70	train loss: 697.0570678710938	(31.5s)
epoch: 71	train loss: 2215.889404296875	(31.5s)
epoch: 72	train loss: 2356.54248046875	(31.5s)
epoch: 73	train loss: 1013.6055908203125	(31.5s)
epoch: 74	train loss: 1050.8441162109375	(31.4s)
epoch: 75	train loss: 655.4993286132812	(31.6s)
epoch: 76	train loss: 646.16259765625	(31.6s)
epoch: 77	train loss: 474.9530029296875	(31.4s)
epoch: 78	train loss: 772.4442749023438	(30.8s)
epoch: 79	train loss: 1314.7725830078125	(30.5s)
epoch: 80	train loss: 849.9990234375	(30.5s)
epoch: 81	train loss: 1101.6300048828125	(31.0s)
epoch: 82	train loss: 1414.966552734375	(31.3s)
epoch: 83	train loss: 1736.1490478515625	(31.6s)
epoch: 84	train loss: 961.36767578125	(31.4s)
epoch: 85	train loss: 771.7356567382812	(31.5s)
epoch: 86	train loss: 1065.1856689453125	(31.6s)
epoch: 87	train loss: 1027.5673828125	(31.5s)
epoch: 88	train loss: 564.118896484375	(31.5s)
epoch: 89	train loss: 585.049560546875	(31.5s)
epoch: 90	train loss: 357.205322265625	(31.6s)
epoch: 91	train loss: 3127.53759765625	(31.5s)
epoch: 92	train loss: 1142.429931640625	(31.6s)
epoch: 93	train loss: 1272.9779052734375	(31.5s)
epoch: 94	train loss: 1266.638671875	(31.4s)
epoch: 95	train loss: 1237.681396484375	(31.6s)
epoch: 96	train loss: 674.8063354492188	(31.4s)
epoch: 97	train loss: 613.6880493164062	(31.4s)
epoch: 98	train loss: 770.7084350585938	(31.7s)
epoch: 99	train loss: 621.920166015625	(31.5s)
Evaluating model on 200 episodes
0.0005162805032846841
0.00015860172293792453
0.00045852056403170786
0.0004842404250666732
0.0005434586528281216
0.0004181939674860284
0.00017743270291248336
0.0008629591353231566
0.0005920220975212942
0.000205566185892773
0.0005985826353245447
0.0004469591032760106
0.00045002293038785206
0.00011363662751288454
0.0006291362844256747
0.0009171923423256137
0.0010584125011155266
0.0005646191665828854
0.0009427396043065528
0.00036542166511329316
0.00038326361349887026
0.0005981179165246431
0.00024943694511926477
0.0005533817362296153
0.0006921194487100971
0.00033055291441996815
0.0005828994185971428
0.0008025950285039417
0.0001969405413547065
0.0007618805720236802
0.00035142402430210495
0.0007468053134971342
0.00025110860665985716
0.0005979716455450605
0.0003890522257830581
0.0002209781384294729
0.00032884906058825434
0.000537057681760737
0.0008942599410789519
0.0004504699637285133
0.00041222602612833725
0.00035647390201726616
0.0005369459353527485
0.0007888228403771791
0.0005329907564373571
0.0007596791884572988
0.0006329530742732459
0.0005503094225252211
0.0008922515742022753
0.0005198556936403082
0.0005449047656535792
0.0007278796693412914
0.0007178026222131198
0.0008396899968374782
0.0005485180521962162
0.000757096492088749
0.00016152852433813015
0.00038905713076279165
0.00039976610514713684
0.0004528511772150523
0.0004171457604601967
0.00047153163263026406
0.0006691575573703241
0.0007822061989372741
0.00027930469258578903
0.0005295665055670945
0.0007620081441200454
0.0003533572115033636
0.0005616526072676956
0.0002961194220713676
0.0001231037554134673
0.0005389121319644295
0.0006235255368665094
0.0004163132168820272
0.0008006164057911115
0.0013459719557431526
0.000876450130900511
0.0008313445234671235
0.0007428896400857152
0.00033415671532566193
0.000490204352753344
0.000573421890408099
0.0006478281954969134
0.00021616112451952444
0.0003690279740428947
0.0007417486022960576
0.00021754646278328865
0.0004744056281456973
0.0010519715533519047
0.0006989957185266706
0.00034307429102583507
0.00030175893844999437
0.00017571812054484326
0.00010187547539115055
0.0004724553975967482
0.00034005559306630556
0.0004335522079439697
0.00014038005156180589
0.0003815984855464194
0.00021914375655138302
0.00036149166586104303
0.0005020242426780896
0.0004385514588474983
0.0002723237401369261
0.0005858901564399849
0.00044344809339236235
0.00037045127946839785
0.000654441878881092
0.00044666052190223127
0.0002872754130294197
0.0008203253083218657
0.0007578816668518067
0.0005338805906996475
0.0005330746121935112
0.001010329933402924
0.0005938066330296957
0.0008511406093001572
0.0002588243700301973
0.0010261191026377976
0.0002337210694699584
0.0005045960349227905
0.0004266615768266582
0.00031145201546678436
0.0007411196586417645
0.0004370166513278188
0.0004270778772479389
0.0010337963904021308
0.0007889589173671617
0.0008212137753199992
0.0005122445803159105
0.0006752582453373179
0.0006302875376954594
0.0005068653908387609
0.0007817377992614638
0.0005839528029537177
0.0008969606951723108
0.0003972096894932819
0.0002937795928232845
0.0008202778242775821
0.0007459980213966713
0.0004655086047611857
0.00047495580439807294
0.00023341656333286664
0.0005784014532158229
0.0005820005694341637
0.0005431214513009763
0.002185881138107756
0.0005844812702499829
0.0016299798142946592
0.00045619673230450845
0.0003330692006784375
0.00023943997838124723
0.00046767199217776157
0.0006374445051733346
0.0004289516131797831
0.0002903139407603054
0.0007037945780211885
0.0003559991805054273
0.0010880801919190544
0.0010104420465116466
0.0008500963760760046
0.00044761147648387123
0.00025923434486685437
0.0008220329711128635
0.000231748913241366
0.0004183504537043821
0.000478453110645205
0.000720647250969266
0.0004032927212392678
0.00013508878549016247
0.00027528576926918254
0.0007115120076377935
0.0006297216770848456
0.00035927239933580097
0.0005117430246173171
0.000888777832353133
0.0005272128090955844
0.0009615165955539043
0.0015760354028265687
0.0007821282058156451
0.0009465130757746465
0.0006331171987932811
0.00047162038578291333
0.00026005440813605676
0.00032721452948357584
0.0006050142008007242
0.0007215081580976733
0.00043070549403031083
0.0004926563527381708
0.0005135530929972167
0.0004264207897449523
0.0012443938981959945
Solved 192/200 episodes
0.0005388041791626884
Evaluated model in 48.1 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-10
Round 11
Generated trajectories in 109.0 seconds
epoch: 0	train loss: 3508.272216796875	(31.5s)
epoch: 1	train loss: 2205.81884765625	(31.3s)
epoch: 2	train loss: 2100.8330078125	(31.6s)
epoch: 3	train loss: 2023.5477294921875	(31.6s)
epoch: 4	train loss: 2300.890625	(31.4s)
epoch: 5	train loss: 2063.5283203125	(31.6s)
epoch: 6	train loss: 1973.9603271484375	(31.6s)
epoch: 7	train loss: 1680.265380859375	(31.4s)
epoch: 8	train loss: 1423.5938720703125	(30.7s)
epoch: 9	train loss: 1527.40869140625	(30.4s)
epoch: 10	train loss: 1729.289794921875	(30.4s)
epoch: 11	train loss: 1380.315185546875	(30.5s)
epoch: 12	train loss: 1354.36962890625	(31.1s)
epoch: 13	train loss: 1057.864013671875	(31.0s)
epoch: 14	train loss: 1276.0325927734375	(31.3s)
epoch: 15	train loss: 1276.0111083984375	(31.7s)
epoch: 16	train loss: 1313.341796875	(31.5s)
epoch: 17	train loss: 1031.1622314453125	(31.6s)
epoch: 18	train loss: 1452.796142578125	(31.5s)
epoch: 19	train loss: 2783.77685546875	(31.6s)
epoch: 20	train loss: 1963.7120361328125	(31.6s)
epoch: 21	train loss: 1065.9776611328125	(31.6s)
epoch: 22	train loss: 1437.6270751953125	(31.7s)
epoch: 23	train loss: 1955.383544921875	(31.6s)
epoch: 24	train loss: 1313.5421142578125	(31.6s)
epoch: 25	train loss: 1082.4278564453125	(31.5s)
epoch: 26	train loss: 1844.8248291015625	(31.6s)
epoch: 27	train loss: 1904.9515380859375	(31.6s)
epoch: 28	train loss: 983.7018432617188	(30.6s)
epoch: 29	train loss: 1234.4132080078125	(31.6s)
epoch: 30	train loss: 929.2854614257812	(31.5s)
epoch: 31	train loss: 803.3163452148438	(31.6s)
epoch: 32	train loss: 1652.05029296875	(31.6s)
epoch: 33	train loss: 961.732666015625	(31.6s)
epoch: 34	train loss: 1107.6778564453125	(31.6s)
epoch: 35	train loss: 1205.6400146484375	(31.6s)
epoch: 36	train loss: 1026.35302734375	(31.6s)
epoch: 37	train loss: 927.0726318359375	(31.4s)
epoch: 38	train loss: 1101.04833984375	(31.6s)
epoch: 39	train loss: 1188.224853515625	(31.6s)
epoch: 40	train loss: 963.2069091796875	(31.5s)
epoch: 41	train loss: 965.2426147460938	(31.6s)
epoch: 42	train loss: 785.0177001953125	(31.4s)
epoch: 43	train loss: 885.4552001953125	(31.6s)
epoch: 44	train loss: 934.7244873046875	(31.5s)
epoch: 45	train loss: 713.1573486328125	(31.6s)
epoch: 46	train loss: 1534.9857177734375	(31.5s)
epoch: 47	train loss: 1109.78466796875	(31.6s)
epoch: 48	train loss: 1623.9083251953125	(31.2s)
epoch: 49	train loss: 1794.9293212890625	(30.5s)
epoch: 50	train loss: 1056.7952880859375	(31.3s)
epoch: 51	train loss: 641.42041015625	(31.7s)
epoch: 52	train loss: 575.43994140625	(31.5s)
epoch: 53	train loss: 687.0712280273438	(31.6s)
epoch: 54	train loss: 2254.19482421875	(31.5s)
epoch: 55	train loss: 945.8883056640625	(31.6s)
epoch: 56	train loss: 616.6090698242188	(31.6s)
epoch: 57	train loss: 1025.6414794921875	(31.5s)
epoch: 58	train loss: 820.9769897460938	(31.6s)
epoch: 59	train loss: 851.4732055664062	(31.6s)
epoch: 60	train loss: 782.2401733398438	(31.5s)
epoch: 61	train loss: 581.5551147460938	(31.5s)
epoch: 62	train loss: 561.7112426757812	(31.6s)
epoch: 63	train loss: 580.0140991210938	(31.7s)
epoch: 64	train loss: 998.8909912109375	(31.5s)
epoch: 65	train loss: 690.5110473632812	(31.6s)
epoch: 66	train loss: 656.1416015625	(31.4s)
epoch: 67	train loss: 1753.6683349609375	(31.6s)
epoch: 68	train loss: 2293.11376953125	(31.7s)
epoch: 69	train loss: 1042.7022705078125	(31.4s)
epoch: 70	train loss: 846.5504760742188	(31.7s)
epoch: 71	train loss: 1013.142578125	(31.1s)
epoch: 72	train loss: 1435.9013671875	(30.6s)
epoch: 73	train loss: 900.16015625	(30.6s)
epoch: 74	train loss: 745.725341796875	(31.4s)
epoch: 75	train loss: 1116.5419921875	(31.7s)
epoch: 76	train loss: 2877.485107421875	(31.5s)
epoch: 77	train loss: 817.7549438476562	(31.6s)
epoch: 78	train loss: 704.7973022460938	(31.4s)
epoch: 79	train loss: 629.6491088867188	(31.6s)
epoch: 80	train loss: 548.2135009765625	(31.6s)
epoch: 81	train loss: 303.5097351074219	(31.6s)
epoch: 82	train loss: 414.6693115234375	(31.6s)
epoch: 83	train loss: 797.479248046875	(31.5s)
epoch: 84	train loss: 1496.1822509765625	(31.7s)
epoch: 85	train loss: 1198.621337890625	(31.5s)
epoch: 86	train loss: 546.26708984375	(31.6s)
epoch: 87	train loss: 647.728271484375	(31.7s)
epoch: 88	train loss: 1261.303955078125	(31.5s)
epoch: 89	train loss: 867.5671997070312	(31.6s)
epoch: 90	train loss: 556.8848266601562	(31.4s)
epoch: 91	train loss: 571.4420166015625	(31.6s)
epoch: 92	train loss: 772.2090454101562	(31.7s)
epoch: 93	train loss: 2225.0537109375	(31.5s)
epoch: 94	train loss: 1540.5322265625	(31.7s)
epoch: 95	train loss: 910.8854370117188	(31.4s)
epoch: 96	train loss: 673.220458984375	(31.6s)
epoch: 97	train loss: 793.0433959960938	(31.7s)
epoch: 98	train loss: 673.015869140625	(30.8s)
epoch: 99	train loss: 563.8890991210938	(30.7s)
Evaluating model on 200 episodes
0.00024506331311613394
0.0007052204553474439
0.00026119028279936174
0.00027853607475423673
0.0004953710595145822
0.0007969907702924198
0.0008399725210438191
0.0004083839693131657
0.00031187222563911583
0.0005925065804244935
0.000414599446230568
0.0004037841912011916
0.00040541876975252165
0.0001923539229551352
0.00016170147839034144
0.00020168483225509255
0.00019975963656785703
0.000725684954390586
0.00047007623626476355
0.0006144296263041074
0.0006737617424313122
0.00044274287602092955
0.0004222359226938958
0.0005938610191454549
0.00036289270392094646
0.00039462349008858466
0.00047044561075313985
0.0004956199304265633
0.00039380813204843436
0.00032426699751588535
0.00032584434372893154
0.0001554779333901024
0.0004908369771576624
0.00035539548589045097
0.001375869448731955
0.0004709180555651341
0.00043989530717226445
0.0003380856329992336
0.0006328173144400794
0.0007154010468326533
0.000820511864756352
0.00047770272019912835
0.0005870157761066742
0.0005241918110446672
0.00043106253069709056
0.00033946747239847004
0.0010131451144843595
0.00017006190533720657
0.0003986256544356531
0.00038657937414730744
0.00026262980959886176
0.0004710148613311402
0.0005007994940829771
0.0004942737705277978
0.0008608487182557482
0.00032432810861026057
0.0008072542004811112
0.0007073723439080983
0.0005542032019461441
0.0003459424527742764
0.0004903198434606436
0.00041650934293245784
0.0006403384282407387
0.0012208751389152894
6.345965486313314e-05
0.00022859409083056644
0.00027241841295714647
0.000495855654446045
0.00031910378834254516
0.0017237217754668295
0.0007957227226143004
0.0008689135831208778
0.0003681206954669588
0.0003563373269643259
0.0006217026995308026
0.00040598036165257715
0.000161203114600994
0.0007361152566729635
0.0004308778693990058
0.0004331710365634002
0.00044286120471172453
0.00038043322806165984
0.00038584498278479174
0.0005957929933196638
0.0008270636490124161
0.0003633809319029696
0.0004535670198658243
0.00015919556507368624
0.0003693040047210161
0.0002835887204128085
0.0007063946638683976
0.00011575058047266353
0.0008050505668506958
0.00038840398295948177
9.014602058717476e-05
0.0006346966296074573
0.0004897928565014808
0.001223029752769175
0.0007633703495038821
0.00030870174445378024
0.00038658707419219555
0.0005045524491328656
0.0007151265888523994
0.0003438280563791017
0.0001374615776117223
0.00020920697945257417
0.001083641373043065
0.0005369587151151444
0.0006496259451783512
0.0002167735982848998
0.0011072765324074267
0.0007498591615330951
0.00042716849594188924
0.00036467222826104263
0.0005439306143038582
0.0005789397706077738
0.00023852281579960478
0.0010060897169672028
0.00038007631181764215
0.00029745758704918373
0.0003861208801236689
0.0002546155744408009
0.00037401512443986055
0.0005117831890803406
0.0004824421386729227
0.00023640790153110988
0.0002522691127524013
0.00014120607135593768
0.0006474985048380741
0.0005680247827163279
0.0007355054247344731
0.0003620691294916311
0.00036862905047504074
0.0005336874689182087
0.0004195457508856258
0.0009027670937939547
0.0004906911214675347
0.0005103315858738417
0.0004312079711650897
0.000571562664000729
0.00029398029856666754
0.0006504578087818421
0.0009714635521699425
0.00040759003636862067
0.000593662718399303
0.00039088105268092477
0.0009416879560509498
0.0005604118335983143
0.00018967272407719756
0.00036162303234549097
0.00041623922537409644
0.00038696082258736297
0.0003892604323012957
0.0006148710974318214
0.0006423363920475822
0.00023568854591688322
0.00041334297964654284
0.0002976517856268401
0.0009052312479686788
0.00039077967918880557
0.0006887034158353344
0.00027166480090686076
0.0008238469088796708
0.00036898845472023824
0.0005302340738395557
0.0008022978299777606
0.00043823393429299947
0.00034593782416217995
0.0012850681854615687
0.00041824998546265607
0.0003586765147841876
0.00031515195472106176
0.0003478437347089661
0.0002900790790716138
0.0003957452037346749
0.0004881543462895359
0.0005038179090856179
0.0007764785595182647
0.00022855542378734256
0.0006934708367528704
0.000532970713376582
0.0003454078867600236
0.0003773748318659879
0.00061406878812077
0.0004585984618188377
0.0005726359132388418
0.001213518049917184
0.000416756457761275
0.001649657858069986
9.973658966156931e-05
0.0009254723161575384
0.0003049847337981766
0.0006432467788337297
0.00022257876506283418
Solved 194/200 episodes
0.0004918613911068513
Evaluated model in 45.9 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-11
Round 12
Generated trajectories in 110.7 seconds
epoch: 0	train loss: 2977.20849609375	(31.5s)
epoch: 1	train loss: 2021.801513671875	(31.6s)
epoch: 2	train loss: 1614.9097900390625	(31.7s)
epoch: 3	train loss: 1300.7900390625	(31.7s)
epoch: 4	train loss: 1693.7689208984375	(31.9s)
epoch: 5	train loss: 1595.75830078125	(31.6s)
epoch: 6	train loss: 2138.603271484375	(31.8s)
epoch: 7	train loss: 1298.3421630859375	(31.7s)
epoch: 8	train loss: 1212.5355224609375	(31.7s)
epoch: 9	train loss: 1305.659912109375	(31.9s)
epoch: 10	train loss: 1715.0926513671875	(31.7s)
epoch: 11	train loss: 1004.8375244140625	(31.7s)
epoch: 12	train loss: 836.2394409179688	(31.6s)
epoch: 13	train loss: 838.9811401367188	(31.7s)
epoch: 14	train loss: 1062.2364501953125	(31.6s)
epoch: 15	train loss: 1222.3187255859375	(31.9s)
epoch: 16	train loss: 1607.4332275390625	(31.6s)
epoch: 17	train loss: 2091.48974609375	(31.9s)
epoch: 18	train loss: 1723.2347412109375	(31.8s)
epoch: 19	train loss: 1512.3387451171875	(31.7s)
epoch: 20	train loss: 789.8885498046875	(31.9s)
epoch: 21	train loss: 687.9689331054688	(31.8s)
epoch: 22	train loss: 745.799560546875	(31.0s)
epoch: 23	train loss: 1093.372802734375	(30.7s)
epoch: 24	train loss: 928.4385375976562	(30.7s)
epoch: 25	train loss: 917.8573608398438	(31.3s)
epoch: 26	train loss: 1370.91748046875	(31.7s)
epoch: 27	train loss: 908.018798828125	(31.9s)
epoch: 28	train loss: 826.2747802734375	(31.6s)
epoch: 29	train loss: 1604.18505859375	(31.8s)
epoch: 30	train loss: 870.0570068359375	(31.7s)
epoch: 31	train loss: 830.87548828125	(31.6s)
epoch: 32	train loss: 1966.2587890625	(31.8s)
epoch: 33	train loss: 1309.9071044921875	(31.8s)
epoch: 34	train loss: 864.1683349609375	(31.8s)
epoch: 35	train loss: 830.2330322265625	(31.9s)
epoch: 36	train loss: 455.538818359375	(31.6s)
epoch: 37	train loss: 506.802001953125	(31.8s)
epoch: 38	train loss: 1188.1727294921875	(31.7s)
epoch: 39	train loss: 617.0066528320312	(31.8s)
epoch: 40	train loss: 852.27099609375	(31.9s)
epoch: 41	train loss: 719.103515625	(31.8s)
epoch: 42	train loss: 920.3196411132812	(31.7s)
epoch: 43	train loss: 716.6328735351562	(31.6s)
epoch: 44	train loss: 1142.6500244140625	(31.8s)
epoch: 45	train loss: 868.9178466796875	(31.9s)
epoch: 46	train loss: 3288.472412109375	(31.8s)
epoch: 47	train loss: 1160.6539306640625	(31.9s)
epoch: 48	train loss: 908.0692138671875	(31.7s)
epoch: 49	train loss: 623.8052368164062	(31.9s)
epoch: 50	train loss: 583.1182861328125	(31.7s)
epoch: 51	train loss: 413.67071533203125	(31.1s)
epoch: 52	train loss: 594.492919921875	(30.7s)
epoch: 53	train loss: 805.8385620117188	(30.7s)
epoch: 54	train loss: 1022.7203369140625	(30.9s)
epoch: 55	train loss: 902.3422241210938	(31.3s)
epoch: 56	train loss: 1231.9119873046875	(30.9s)
epoch: 57	train loss: 749.5777587890625	(31.0s)
epoch: 58	train loss: 702.8538208007812	(30.9s)
epoch: 59	train loss: 566.9630126953125	(31.1s)
epoch: 60	train loss: 726.9501342773438	(30.7s)
epoch: 61	train loss: 1999.2376708984375	(30.9s)
epoch: 62	train loss: 1439.2003173828125	(31.5s)
epoch: 63	train loss: 665.3389892578125	(31.8s)
epoch: 64	train loss: 771.1015625	(31.9s)
epoch: 65	train loss: 599.7140502929688	(31.7s)
epoch: 66	train loss: 562.8214721679688	(31.6s)
epoch: 67	train loss: 361.0813903808594	(31.7s)
epoch: 68	train loss: 291.1257629394531	(31.8s)
epoch: 69	train loss: 2453.11181640625	(31.8s)
epoch: 70	train loss: 984.1361083984375	(31.8s)
epoch: 71	train loss: 1088.0152587890625	(31.8s)
epoch: 72	train loss: 558.7012329101562	(31.7s)
epoch: 73	train loss: 456.8714599609375	(31.8s)
epoch: 74	train loss: 1270.67138671875	(31.7s)
epoch: 75	train loss: 695.1808471679688	(31.8s)
epoch: 76	train loss: 374.8238220214844	(31.5s)
epoch: 77	train loss: 774.8034057617188	(30.9s)
epoch: 78	train loss: 555.70654296875	(31.8s)
epoch: 79	train loss: 711.422607421875	(31.7s)
epoch: 80	train loss: 552.676513671875	(31.6s)
epoch: 81	train loss: 817.5804443359375	(31.8s)
epoch: 82	train loss: 538.5595703125	(31.6s)
epoch: 83	train loss: 470.2651062011719	(31.7s)
epoch: 84	train loss: 1100.7615966796875	(31.7s)
epoch: 85	train loss: 1080.7314453125	(31.6s)
epoch: 86	train loss: 694.3194580078125	(31.6s)
epoch: 87	train loss: 734.9070434570312	(31.7s)
epoch: 88	train loss: 605.7789306640625	(31.7s)
epoch: 89	train loss: 495.369140625	(31.8s)
epoch: 90	train loss: 907.4920654296875	(31.6s)
epoch: 91	train loss: 261.0916748046875	(31.6s)
epoch: 92	train loss: 168.84259033203125	(31.8s)
epoch: 93	train loss: 1138.555419921875	(31.7s)
epoch: 94	train loss: 1087.817138671875	(31.7s)
epoch: 95	train loss: 420.89825439453125	(31.8s)
epoch: 96	train loss: 751.0286865234375	(31.6s)
epoch: 97	train loss: 451.941162109375	(31.8s)
epoch: 98	train loss: 2658.912841796875	(31.6s)
epoch: 99	train loss: 1043.2576904296875	(31.7s)
Evaluating model on 200 episodes
0.0012255944392462778
0.0007817029027688532
0.0020324963833055726
0.0006258895164990008
0.000530702572307616
0.0006338046574195946
0.0012267074196729698
0.0008425634490413359
0.0005190232867562372
0.0005768939694383374
0.0008391478883627965
0.00041463381553925996
0.0005445981414595735
0.00017913851138473547
0.0006504202447104035
0.00042041796223202255
0.0003506337776674296
0.0008935190271813553
0.0008398744708317501
0.0009547534042978537
0.0008369854534976184
0.0009111028081639921
0.0009926388392744098
0.0004409317876014116
0.00034965367437204503
0.0006097176224607716
0.0006303678269264141
0.0006843906689799483
0.0007642242671863642
0.0002689217322208256
0.0006158327109062611
0.0005751693298324509
0.0007586923323910206
0.0004846137650019955
0.00015853410419507642
0.0005519623923379078
0.0009787307501483156
0.000444570033063688
0.0005094719717480648
0.0003317816439084709
0.0006970821911838559
0.0007130451629468313
0.0008175836538772701
0.001203500334304408
0.0012128373076328509
0.0007652801925562219
0.00044616625183896923
0.0005951774916226255
0.0006671334993890403
0.0006288164830253021
0.0009150399298050616
0.0007934313182852713
0.0002973149330500746
0.00040665916305572736
0.0014283140620176234
0.0010509893045309582
0.0007059439208205731
0.0009948091644522113
0.0006183716553745659
0.0003822586974098973
0.0011155571261042495
0.0002922020410665027
0.000710309842058147
0.0013292978538477958
0.0009565377429152572
0.00029566223433489955
0.0008986753964942787
0.0006319878215503262
0.0019142816034638179
0.00041794349645466643
0.0011492749563331017
0.0005280087918890786
0.0009261953982786508
0.0005820650361556545
0.001371638364920121
0.0008752217434812337
0.0006679684303283769
0.0004572540366199165
0.0005969338642898947
0.00039108665280309653
0.0016965311187959742
0.000834862355354151
0.00046118348884057787
0.0011105688659621851
0.0005273723070409876
0.0006094352600314323
0.00021024507994601058
0.00026210771979094714
0.0004102487252814833
0.0005723299857436359
0.0003416431785202197
0.0004744967952016547
0.0009126158792241767
0.00041572572796818956
0.00041403121952801787
0.0009815570349177426
0.0006098484912469687
0.00037199271433304863
0.0005190535167685084
0.00038157346546086046
0.0005199058102464603
0.00036289268402470046
0.0006096409461601329
0.00031048493451635295
0.0004155058430418042
0.0004993564762480673
0.0005733198415024768
0.0005430784843026361
0.0006462873732447787
0.0007488567563314064
0.0009784960461079858
0.0006870249015201839
0.0006678296536964838
0.00048590463037119963
0.0008925866740507141
0.0003930056319144829
0.0005183708788081276
0.0007027385515317766
0.0014831513649719104
0.00059177257283712
0.0005958550804621154
0.0006651684535551216
0.0004942489787801471
0.0010117655238526127
0.0006436467234272961
0.0006312991540003818
0.00023524439545023293
0.0007935479285758144
0.0007894112307510043
0.0007689365145779447
0.0001458240309987629
0.0008569499751143721
0.0010021933729149168
0.0007317462714127032
0.0011897146185089827
0.000715106811501597
0.0005418946388241844
0.0011478517738093311
0.00039464710408975844
0.00038326460128246254
0.0012675198286160594
0.00045422976199915865
0.00022582546534977155
0.0005846296075712114
0.00046375993718226837
0.00041546222594964394
0.0009649200778767408
0.00046471715222651255
0.000481776745329154
0.0008449035030667694
0.0008617941545404998
0.0013561441784466816
0.0003934685996682674
0.0005238171112479552
0.0007184963785784064
0.0005633056642396053
0.0006539314045868297
0.0008733149489772688
0.0009575578664160796
0.0007885325566982801
0.000404518607839626
0.0003598723424682462
0.0009150252050788437
0.0008674529814781732
0.0010596918995221271
0.0006057525640951806
0.000719366912426267
0.0006444929976543575
0.001041647427843165
0.000397663087426281
0.0003482105551354055
0.00021618554480179835
0.0002953771386557946
0.00011079081816895833
0.00022870778548167436
0.0004733653719692181
0.0007201609130954074
0.0003976775090753411
0.0011575912607700697
0.0009414639571332373
0.0006718808358527895
0.0006562747854072118
0.00026236431407456763
0.000175800365858387
0.0004183313308749348
0.0004937848871122696
0.0006400674027580473
0.0003032246126879371
0.0011588121435579524
0.0004858417801837537
0.0004999918674002402
0.0007868286504562115
0.0006184517812168148
0.0006801841427280427
0.0011047750302566328
0.0009248134369687143
Solved 196/200 episodes
0.0006627958314413229
Evaluated model in 49.0 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-12
Round 13
Generated trajectories in 107.6 seconds
epoch: 0	train loss: 2798.290283203125	(30.5s)
epoch: 1	train loss: 1609.592041015625	(31.0s)
epoch: 2	train loss: 1372.8363037109375	(31.5s)
epoch: 3	train loss: 1764.847900390625	(31.4s)
epoch: 4	train loss: 1364.2021484375	(31.5s)
epoch: 5	train loss: 1101.6234130859375	(31.6s)
epoch: 6	train loss: 1083.785888671875	(31.5s)
epoch: 7	train loss: 1479.5604248046875	(31.5s)
epoch: 8	train loss: 1091.0184326171875	(31.5s)
epoch: 9	train loss: 1272.1005859375	(31.6s)
epoch: 10	train loss: 760.470947265625	(31.5s)
epoch: 11	train loss: 626.5008544921875	(31.5s)
epoch: 12	train loss: 2699.76806640625	(31.5s)
epoch: 13	train loss: 2038.0997314453125	(31.4s)
epoch: 14	train loss: 1101.805908203125	(31.5s)
epoch: 15	train loss: 1030.5655517578125	(31.6s)
epoch: 16	train loss: 765.0670776367188	(31.6s)
epoch: 17	train loss: 599.1818237304688	(31.6s)
epoch: 18	train loss: 566.5767211914062	(31.4s)
epoch: 19	train loss: 898.9057006835938	(31.6s)
epoch: 20	train loss: 1182.376953125	(31.6s)
epoch: 21	train loss: 1984.824951171875	(31.5s)
epoch: 22	train loss: 1565.7900390625	(31.6s)
epoch: 23	train loss: 912.907958984375	(31.5s)
epoch: 24	train loss: 741.7408447265625	(31.5s)
epoch: 25	train loss: 587.237548828125	(31.5s)
epoch: 26	train loss: 605.29150390625	(31.7s)
epoch: 27	train loss: 727.872314453125	(31.6s)
epoch: 28	train loss: 877.6695556640625	(31.5s)
epoch: 29	train loss: 537.1687622070312	(31.6s)
epoch: 30	train loss: 897.5801391601562	(31.5s)
epoch: 31	train loss: 1119.1456298828125	(31.4s)
epoch: 32	train loss: 1355.4298095703125	(30.8s)
epoch: 33	train loss: 1232.69677734375	(30.6s)
epoch: 34	train loss: 600.9349365234375	(31.2s)
epoch: 35	train loss: 532.8406982421875	(31.5s)
epoch: 36	train loss: 594.879638671875	(31.6s)
epoch: 37	train loss: 507.212890625	(31.7s)
epoch: 38	train loss: 1034.818359375	(31.7s)
epoch: 39	train loss: 1632.94140625	(31.6s)
epoch: 40	train loss: 1869.5340576171875	(31.6s)
epoch: 41	train loss: 616.5628662109375	(31.7s)
epoch: 42	train loss: 520.8302612304688	(31.8s)
epoch: 43	train loss: 348.44293212890625	(31.7s)
epoch: 44	train loss: 461.298828125	(31.8s)
epoch: 45	train loss: 1477.7806396484375	(31.6s)
epoch: 46	train loss: 488.81488037109375	(31.7s)
epoch: 47	train loss: 2740.059814453125	(31.5s)
epoch: 48	train loss: 807.3824462890625	(31.8s)
epoch: 49	train loss: 649.1865234375	(31.7s)
epoch: 50	train loss: 435.7677001953125	(31.5s)
epoch: 51	train loss: 612.2803344726562	(31.7s)
epoch: 52	train loss: 709.5218505859375	(31.5s)
epoch: 53	train loss: 622.92578125	(31.7s)
epoch: 54	train loss: 374.8388671875	(31.7s)
epoch: 55	train loss: 729.4111328125	(31.7s)
epoch: 56	train loss: 377.701904296875	(31.4s)
epoch: 57	train loss: 708.6407470703125	(30.6s)
epoch: 58	train loss: 928.3225708007812	(30.7s)
epoch: 59	train loss: 468.1023864746094	(31.0s)
epoch: 60	train loss: 334.018310546875	(31.4s)
epoch: 61	train loss: 459.70477294921875	(31.7s)
epoch: 62	train loss: 1479.962890625	(31.5s)
epoch: 63	train loss: 1177.9940185546875	(31.6s)
epoch: 64	train loss: 1388.2178955078125	(31.8s)
epoch: 65	train loss: 607.1773071289062	(31.5s)
epoch: 66	train loss: 1160.295654296875	(31.7s)
epoch: 67	train loss: 531.1931762695312	(31.6s)
epoch: 68	train loss: 944.9046630859375	(31.5s)
epoch: 69	train loss: 545.5357055664062	(31.8s)
epoch: 70	train loss: 898.52490234375	(31.6s)
epoch: 71	train loss: 592.3707885742188	(31.8s)
epoch: 72	train loss: 674.3209228515625	(31.6s)
epoch: 73	train loss: 317.8728942871094	(31.5s)
epoch: 74	train loss: 177.00238037109375	(31.7s)
epoch: 75	train loss: 204.56546020507812	(31.7s)
epoch: 76	train loss: 468.2919006347656	(31.7s)
epoch: 77	train loss: 2922.779541015625	(31.6s)
epoch: 78	train loss: 822.8660888671875	(31.5s)
epoch: 79	train loss: 674.0460815429688	(31.6s)
epoch: 80	train loss: 765.9475708007812	(31.7s)
epoch: 81	train loss: 1171.05322265625	(31.6s)
epoch: 82	train loss: 468.0600280761719	(31.7s)
epoch: 83	train loss: 405.28680419921875	(31.7s)
epoch: 84	train loss: 760.5650024414062	(31.4s)
epoch: 85	train loss: 573.0863647460938	(31.7s)
epoch: 86	train loss: 201.2521209716797	(31.7s)
epoch: 87	train loss: 869.3250122070312	(31.5s)
epoch: 88	train loss: 691.016357421875	(30.8s)
epoch: 89	train loss: 683.41357421875	(30.5s)
epoch: 90	train loss: 497.3887023925781	(30.6s)
epoch: 91	train loss: 1078.6383056640625	(31.1s)
epoch: 92	train loss: 684.014404296875	(31.5s)
epoch: 93	train loss: 1415.175537109375	(31.7s)
epoch: 94	train loss: 946.5010986328125	(31.5s)
epoch: 95	train loss: 323.4627380371094	(31.6s)
epoch: 96	train loss: 380.10760498046875	(31.7s)
epoch: 97	train loss: 409.80694580078125	(31.7s)
epoch: 98	train loss: 915.153076171875	(31.8s)
epoch: 99	train loss: 510.1648254394531	(31.5s)
Evaluating model on 200 episodes
0.00025355462168911747
0.0006578800907196378
0.0002616733186187048
0.00024042816183846924
0.0006577439193200047
0.0005180076030807289
0.0005188694334802593
0.0003150310055319166
0.00026761263003348943
0.000394259842453591
0.0002467263951219042
0.00046279898564145175
0.0005668184449960892
0.0003557590130452916
0.0005827003162329643
0.0004298270818253513
0.0004541918932816424
0.000588588653045008
7.64002967343913e-05
0.0005460535068873708
0.0005190056640108546
0.000221942789894889
0.0006467906637442953
0.0006225063969193596
0.00030098694090459804
0.0004993962825210474
0.0004709900810553336
0.0003002529801960918
0.00020249509382735525
0.00025254771400582017
0.00036269135993139713
0.00040954892433389965
0.00038178939201530097
0.00034304339358944463
0.0003757755857805023
0.0005144250739942349
0.00029649406217989357
0.000142258356289757
0.00031766932723777634
0.00039987122695225375
0.00039208072231976195
0.0002166587655665353
0.00014380848961081938
0.00019763535339520518
0.00040083885369313066
0.00024765756819533635
0.00034637821964780515
0.00017501504933332198
0.0004136956400139348
0.00042535765286781196
0.0006967583086634477
0.00039566864031639263
0.000159754448266843
0.0005859774162562451
0.0002112087167915888
0.00011536331434259549
0.0003220473469506639
0.000533137145521323
0.0002202733451971047
0.0002950047905067228
0.0003678733955244492
0.0005697942553910473
0.00043163078913968764
0.000410923051458667
0.0002239274332061862
0.00019933303528454243
0.0003761218634394936
0.00043185139475099277
0.00025234433572525933
0.0003791664890048283
0.0004823692266338343
0.0001179912298994168
0.000583443115465343
0.0003588833327638794
0.00033610741832793185
0.0008248677711009123
0.0003230654647663671
0.0004910692959713613
0.0005372898114728742
0.00029701563234993953
0.00041900130586496005
0.0004236060894332435
0.00039189592948316894
0.0005924136775092848
0.0007927461894637201
0.0004720105173166505
0.0005253925174687977
0.0006775519142977898
0.0006314271681317637
0.00025641466507093154
0.0004493536402903907
0.00021713650262840517
0.00030551277322956596
0.0008063288778780239
0.0010195553421748198
0.000507523623355155
0.0004571962484718555
0.00023068874018944744
0.0002256107688569295
0.00023315840480790938
0.00024752301433181854
0.0005821994606776571
0.0005548560567426055
0.00028226380572959897
0.000128273928307213
0.00033665612201210545
0.0004516731375285114
0.0002492172300586996
0.0006622118581778041
0.00044392143977997227
0.0001840881068346789
0.0002979368685339944
0.00011458238464001624
0.00022553067490087799
0.0004166308074424953
0.0006972848281553265
0.0003085479958407142
0.0006457788157250535
0.00035115412260090713
0.0005323044454996761
0.00015741969784886027
0.00030310137403180955
0.0003936295904168219
0.00059680380548922
0.00047375802831503767
0.0002161230883592163
0.000922280056438467
0.00016630843287123793
0.0004858202390333526
0.0007291475480647372
0.000653173662173905
0.000617637795202379
0.0003291974304728582
0.00021523172047939929
0.00013008853430720692
0.00018531952196099155
0.0004735279007945792
0.00044376722346189327
0.0004727536868466857
0.0006170626114908373
0.0006682845853272459
0.001012643577041672
0.00025710441351683197
0.00033504823805843544
0.0002183312259755136
0.0002749510707002153
0.0007188547910175203
0.00014232790484432675
0.0005211064170517991
0.0002405037005795246
0.0002826024550965182
0.00027643102584122624
0.00013360809546276485
0.0002512348895503568
0.0007376124424101565
0.00024152770771145108
0.0003487524426721696
0.0006246337822052407
0.00044404223787681147
8.116842032753387e-05
9.93702809785685e-05
0.00048792339701073865
0.0004167409937508637
8.366883662347391e-05
0.0004401278315071977
0.000617646856214075
0.0009746459067577638
0.00023424895523651055
0.0001974797807419878
0.00036178692324837256
0.0007956245777817317
0.000407787599254769
0.00039062464854093076
0.000757224119449044
0.0006027875246777862
0.0006383604070585782
0.00034698902852955813
0.0005490196385008775
0.0003684499440948587
0.0005078534261429013
0.0007477522800140182
0.00035629204592169117
0.0006515014606217544
0.0002316395573037274
0.00017804310384515423
0.00039426329810036503
0.0003036626081146616
0.0004109881387453892
0.0002286918372472148
0.0005700892143067904
0.00011810425015331323
0.0003162676527684501
Solved 192/200 episodes
0.00038995266073053766
Evaluated model in 47.8 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-13
Round 14
Generated trajectories in 111.2 seconds
epoch: 0	train loss: 2482.042724609375	(31.6s)
epoch: 1	train loss: 1295.7415771484375	(31.5s)
epoch: 2	train loss: 1240.1561279296875	(31.7s)
epoch: 3	train loss: 1012.0350341796875	(31.7s)
epoch: 4	train loss: 767.4628295898438	(31.5s)
epoch: 5	train loss: 1144.8115234375	(31.7s)
epoch: 6	train loss: 879.9270629882812	(31.7s)
epoch: 7	train loss: 849.9822998046875	(31.5s)
epoch: 8	train loss: 906.51123046875	(31.7s)
epoch: 9	train loss: 780.6188354492188	(31.6s)
epoch: 10	train loss: 702.9115600585938	(31.5s)
epoch: 11	train loss: 1370.2430419921875	(31.6s)
epoch: 12	train loss: 1874.19189453125	(31.6s)
epoch: 13	train loss: 1316.3038330078125	(31.6s)
epoch: 14	train loss: 595.751220703125	(31.6s)
epoch: 15	train loss: 931.0357055664062	(31.5s)
epoch: 16	train loss: 431.30462646484375	(31.3s)
epoch: 17	train loss: 805.6466064453125	(30.9s)
epoch: 18	train loss: 1114.1124267578125	(30.7s)
epoch: 19	train loss: 594.835693359375	(30.6s)
epoch: 20	train loss: 1023.9235229492188	(30.7s)
epoch: 21	train loss: 1037.5970458984375	(31.2s)
epoch: 22	train loss: 1635.0633544921875	(30.9s)
epoch: 23	train loss: 1002.9521484375	(31.0s)
epoch: 24	train loss: 946.4795532226562	(31.1s)
epoch: 25	train loss: 1323.6671142578125	(31.4s)
epoch: 26	train loss: 1166.317138671875	(31.5s)
epoch: 27	train loss: 641.5162353515625	(31.5s)
epoch: 28	train loss: 520.5330810546875	(31.7s)
epoch: 29	train loss: 552.9933471679688	(31.5s)
epoch: 30	train loss: 526.4085693359375	(31.6s)
epoch: 31	train loss: 537.2329711914062	(31.4s)
epoch: 32	train loss: 706.079833984375	(31.6s)
epoch: 33	train loss: 2086.30615234375	(31.6s)
epoch: 34	train loss: 1053.36962890625	(31.5s)
epoch: 35	train loss: 699.5078735351562	(31.7s)
epoch: 36	train loss: 366.76422119140625	(31.5s)
epoch: 37	train loss: 845.8535766601562	(31.5s)
epoch: 38	train loss: 294.4032287597656	(31.5s)
epoch: 39	train loss: 1074.146240234375	(31.6s)
epoch: 40	train loss: 438.4146423339844	(31.6s)
epoch: 41	train loss: 646.885986328125	(31.5s)
epoch: 42	train loss: 551.33447265625	(31.0s)
epoch: 43	train loss: 537.6881103515625	(31.1s)
epoch: 44	train loss: 657.1611938476562	(31.6s)
epoch: 45	train loss: 1025.4730224609375	(31.6s)
epoch: 46	train loss: 1103.0360107421875	(31.6s)
epoch: 47	train loss: 867.2639770507812	(31.5s)
epoch: 48	train loss: 343.6966857910156	(31.6s)
epoch: 49	train loss: 445.7773132324219	(31.5s)
epoch: 50	train loss: 383.8226623535156	(31.7s)
epoch: 51	train loss: 338.0	(31.6s)
epoch: 52	train loss: 884.3885498046875	(31.7s)
epoch: 53	train loss: 880.886962890625	(31.4s)
epoch: 54	train loss: 1237.6971435546875	(31.6s)
epoch: 55	train loss: 1192.273681640625	(31.5s)
epoch: 56	train loss: 1213.77099609375	(31.6s)
epoch: 57	train loss: 1177.3150634765625	(31.7s)
epoch: 58	train loss: 640.9312744140625	(31.5s)
epoch: 59	train loss: 523.288818359375	(31.6s)
epoch: 60	train loss: 342.0536804199219	(31.5s)
epoch: 61	train loss: 579.0880126953125	(31.6s)
epoch: 62	train loss: 375.15032958984375	(31.0s)
epoch: 63	train loss: 233.5979461669922	(30.6s)
epoch: 64	train loss: 299.9577331542969	(31.5s)
epoch: 65	train loss: 2172.634033203125	(31.5s)
epoch: 66	train loss: 888.1600341796875	(31.6s)
epoch: 67	train loss: 419.1153259277344	(31.7s)
epoch: 68	train loss: 771.779541015625	(31.6s)
epoch: 69	train loss: 690.3414306640625	(31.7s)
epoch: 70	train loss: 419.2704772949219	(31.6s)
epoch: 71	train loss: 649.1869506835938	(31.6s)
epoch: 72	train loss: 746.71826171875	(31.7s)
epoch: 73	train loss: 497.1148681640625	(31.7s)
epoch: 74	train loss: 313.8785400390625	(31.7s)
epoch: 75	train loss: 243.6338653564453	(31.7s)
epoch: 76	train loss: 131.361328125	(31.7s)
epoch: 77	train loss: 728.2726440429688	(31.6s)
epoch: 78	train loss: 1687.8319091796875	(31.6s)
epoch: 79	train loss: 811.009765625	(31.7s)
epoch: 80	train loss: 1402.4517822265625	(31.6s)
epoch: 81	train loss: 472.1505126953125	(31.3s)
epoch: 82	train loss: 569.2407836914062	(30.6s)
epoch: 83	train loss: 339.1518249511719	(30.8s)
epoch: 84	train loss: 225.73719787597656	(31.6s)
epoch: 85	train loss: 221.3567657470703	(31.7s)
epoch: 86	train loss: 455.29052734375	(31.7s)
epoch: 87	train loss: 735.839599609375	(31.5s)
epoch: 88	train loss: 624.2218627929688	(31.6s)
epoch: 89	train loss: 1069.2379150390625	(31.8s)
epoch: 90	train loss: 962.9003295898438	(31.7s)
epoch: 91	train loss: 460.18365478515625	(31.6s)
epoch: 92	train loss: 253.39109802246094	(31.6s)
epoch: 93	train loss: 634.6298828125	(31.7s)
epoch: 94	train loss: 223.26577758789062	(31.6s)
epoch: 95	train loss: 245.7898712158203	(31.7s)
epoch: 96	train loss: 734.3012084960938	(31.6s)
epoch: 97	train loss: 617.4725952148438	(31.7s)
epoch: 98	train loss: 569.9765625	(31.5s)
epoch: 99	train loss: 429.9769592285156	(31.6s)
Evaluating model on 200 episodes
0.0005235373910961374
0.0005746640113954982
0.00033087592620224885
0.0002981585431760842
0.00043030535760087385
0.00033350914818583985
0.00016404270670887934
0.0005858921520484727
0.0002956483676825883
0.000883462773023947
0.0003670969278459779
0.0007257363527060079
0.00035945558128707274
0.0003200203891537967
0.0005251106670454776
0.0003363311210532196
0.0007489142405628466
0.0003070661688171741
0.0001771637057415852
0.0005619451822490697
0.0007409167537947393
0.000851937286744728
0.0004261008504857143
0.0003910300273933639
0.0007550617478652081
0.0007704538850500992
0.0006067886633120776
0.000324737374914828
0.0008138016953939529
0.000563761774461263
0.0003060536386588605
0.0002765587308507141
0.0005182645413697173
0.00031420551692592976
0.00036512266517872504
0.0007261749758202184
0.00048757933457939845
0.00031873467431372166
0.0004845324374855409
9.12543930553511e-05
0.00024994423132408303
0.0007518732052077488
0.00024130924519676742
0.0004636866370143859
0.00026619465910805636
0.0008130215479468461
0.00023212168174982902
0.00040888051955789706
0.00021667244658601703
0.0005124141696594759
0.00044356656064447827
0.000636621859484876
0.00034221286937520886
0.00021728021965827793
0.0010416684479637565
0.0003537815839669801
0.00044601689129003237
0.0007713816291167556
0.00047318317898647035
0.00047292683893829235
0.0005124700979346623
0.000666759517310098
0.0004409669934102567
0.0009707429130335535
0.00047749477735123944
0.00012077720593879348
0.0004528159966352889
0.0004644250707964862
0.0005582538151429617
0.0011890183416198852
0.00032790519026093534
0.0004650061626558329
0.0005447019270453287
0.0002345864517405129
0.000279483185238405
0.0009117134451164594
0.0005206504448551641
0.0006701851398247527
0.00023630434293409053
0.001047037099711955
0.0004679686007226077
0.00043181069296384947
0.0005200402450821551
0.0008518926779793724
0.0003310767356197175
0.0004295516845963397
0.0008906563271011692
0.00026592953039653364
0.0005013095016895224
0.0003013225130023391
0.00033228701197783394
0.0005150588669728736
0.00020352013487152161
0.0001666114399591202
0.0005926546756849607
0.0007147589227416676
0.00018665868564686903
0.00038259204447967934
0.00022744990507673815
0.0004507659280286382
0.000339446489912613
0.0005422403020020284
0.00025869644337035426
0.00038683665689649596
0.00013590711387223564
0.00026716121759307043
0.00041922698416713573
0.0005444450656568733
0.0003644993435695402
0.0004222083717697962
0.0005852817803315702
0.00048305140781474835
0.0007673492448073096
0.0006198659022629727
0.0003606189288423896
0.0007208943626600168
0.0005163644822129176
0.0005353228840409857
0.0006675467109542094
0.0002640475351290661
0.00029296944448058067
0.0006102359682798222
0.00042835994837433746
0.00031384254221988016
0.0001889280074465205
0.0004489530761670072
0.000717395669694683
0.000441652087768792
0.0003249374771928016
0.00023662730902949326
0.0006071967025491176
0.0003564542280598645
0.000269485197228884
0.0005286229460479161
0.00017449681555652724
0.000320986878759868
0.0007857139430766607
0.0005037798883417313
0.0005326507945255798
0.00040893078895511706
0.000306337320752563
0.00043800694804399667
0.000323205901870758
0.0007079327810499997
0.0003072182286044923
0.0003613636497700807
0.0003855253655327782
0.0004938718724588398
0.0004064417069002957
0.0005656010968474826
0.0003228384023039384
0.0003944469013019799
0.000685171043369337
0.0006786812260622315
0.0006902245287121678
0.0005135402852396336
0.000245519454741346
0.00021101830679981504
0.0004946494803816142
0.0006460135628003627
0.0004659651504728875
0.0008225628636561547
0.0005583385373029159
0.0007755688328381019
0.00029578490120324967
0.0004394045299704885
0.0005053499324011076
0.0003539529327209686
0.0006712479826470977
0.00042244913238391745
0.00036969714165024925
0.0003348346877389523
0.000444950012024492
0.0005565272512058073
0.0007921217975308537
0.001146401646901327
0.000243810885371608
0.00047651900494353566
0.00032960646645237415
0.0006110503474587858
0.0002489081064140919
0.0004759817898675151
0.0006862105211439484
0.0010038492217366486
0.00039324119784901594
0.0006580244103436901
0.00044137757716574316
0.00023090093499275908
0.0007161274535737111
0.00041868471556550833
0.0006714171288628925
0.00016834579273563576
0.00025060081274065084
0.00023248232949602728
0.0003449464229561272
0.0005538067696761572
0.000483940465953007
0.0006119372620129358
0.0004379990561669968
Solved 199/200 episodes
0.0004735387267922827
Evaluated model in 52.5 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-14
Round 15
Generated trajectories in 106.9 seconds
epoch: 0	train loss: 1611.1885986328125	(30.2s)
epoch: 1	train loss: 906.5682373046875	(30.2s)
epoch: 2	train loss: 810.5855712890625	(30.9s)
epoch: 3	train loss: 1034.5706787109375	(31.4s)
epoch: 4	train loss: 1289.7054443359375	(31.5s)
epoch: 5	train loss: 846.39404296875	(31.4s)
epoch: 6	train loss: 1277.1986083984375	(31.5s)
epoch: 7	train loss: 850.4526977539062	(31.3s)
epoch: 8	train loss: 920.9649658203125	(31.5s)
epoch: 9	train loss: 1157.4862060546875	(31.4s)
epoch: 10	train loss: 608.581787109375	(31.4s)
epoch: 11	train loss: 648.3294677734375	(31.5s)
epoch: 12	train loss: 596.6372680664062	(31.2s)
epoch: 13	train loss: 696.2402954101562	(31.5s)
epoch: 14	train loss: 909.3087768554688	(31.4s)
epoch: 15	train loss: 441.4491882324219	(31.4s)
epoch: 16	train loss: 369.6266174316406	(31.4s)
epoch: 17	train loss: 674.1771240234375	(31.3s)
epoch: 18	train loss: 1086.4339599609375	(31.5s)
epoch: 19	train loss: 1197.9703369140625	(31.3s)
epoch: 20	train loss: 1084.529052734375	(31.5s)
epoch: 21	train loss: 950.4292602539062	(31.6s)
epoch: 22	train loss: 840.4989013671875	(31.4s)
epoch: 23	train loss: 369.5445861816406	(31.4s)
epoch: 24	train loss: 383.263916015625	(31.3s)
epoch: 25	train loss: 1494.3939208984375	(31.5s)
epoch: 26	train loss: 497.9579772949219	(31.4s)
epoch: 27	train loss: 314.9021911621094	(31.1s)
epoch: 28	train loss: 308.5869140625	(30.5s)
epoch: 29	train loss: 424.8235778808594	(30.3s)
epoch: 30	train loss: 604.5296630859375	(30.5s)
epoch: 31	train loss: 597.3703002929688	(31.2s)
epoch: 32	train loss: 628.37158203125	(31.5s)
epoch: 33	train loss: 412.22509765625	(31.4s)
epoch: 34	train loss: 427.24383544921875	(31.5s)
epoch: 35	train loss: 475.3459167480469	(31.5s)
epoch: 36	train loss: 647.4861450195312	(31.4s)
epoch: 37	train loss: 1939.1192626953125	(31.5s)
epoch: 38	train loss: 1238.6748046875	(31.4s)
epoch: 39	train loss: 431.7638854980469	(31.4s)
epoch: 40	train loss: 452.66998291015625	(31.5s)
epoch: 41	train loss: 188.53086853027344	(31.4s)
epoch: 42	train loss: 216.22996520996094	(31.5s)
epoch: 43	train loss: 563.4580688476562	(31.5s)
epoch: 44	train loss: 270.1016845703125	(31.4s)
epoch: 45	train loss: 316.2567138671875	(31.4s)
epoch: 46	train loss: 1673.808837890625	(31.4s)
epoch: 47	train loss: 1298.106689453125	(31.5s)
epoch: 48	train loss: 806.571533203125	(31.4s)
epoch: 49	train loss: 246.93960571289062	(31.4s)
epoch: 50	train loss: 142.67013549804688	(31.5s)
epoch: 51	train loss: 98.9194107055664	(31.4s)
epoch: 52	train loss: 99.44707489013672	(31.5s)
epoch: 53	train loss: 416.5325622558594	(31.4s)
epoch: 54	train loss: 417.7397766113281	(31.4s)
epoch: 55	train loss: 663.8566284179688	(31.4s)
epoch: 56	train loss: 704.6348876953125	(31.0s)
epoch: 57	train loss: 768.2874755859375	(30.5s)
epoch: 58	train loss: 1279.049560546875	(30.5s)
epoch: 59	train loss: 1036.76513671875	(30.4s)
epoch: 60	train loss: 448.1420593261719	(31.1s)
epoch: 61	train loss: 804.3525390625	(31.1s)
epoch: 62	train loss: 928.3192749023438	(30.6s)
epoch: 63	train loss: 569.8721313476562	(30.7s)
epoch: 64	train loss: 334.0631103515625	(30.4s)
epoch: 65	train loss: 763.301513671875	(30.5s)
epoch: 66	train loss: 492.9998779296875	(31.0s)
epoch: 67	train loss: 340.862060546875	(30.9s)
epoch: 68	train loss: 476.1827087402344	(31.2s)
epoch: 69	train loss: 476.57958984375	(31.4s)
epoch: 70	train loss: 225.39244079589844	(31.4s)
epoch: 71	train loss: 84.68072509765625	(31.4s)
epoch: 72	train loss: 219.61277770996094	(31.4s)
epoch: 73	train loss: 1049.9580078125	(31.5s)
epoch: 74	train loss: 918.632568359375	(31.4s)
epoch: 75	train loss: 497.9890441894531	(31.4s)
epoch: 76	train loss: 798.8455200195312	(31.3s)
epoch: 77	train loss: 344.74224853515625	(30.5s)
epoch: 78	train loss: 411.69110107421875	(31.4s)
epoch: 79	train loss: 1364.3118896484375	(31.4s)
epoch: 80	train loss: 373.5807800292969	(31.5s)
epoch: 81	train loss: 739.8782348632812	(31.3s)
epoch: 82	train loss: 222.57545471191406	(31.4s)
epoch: 83	train loss: 79.59917449951172	(31.6s)
epoch: 84	train loss: 278.9208984375	(31.4s)
epoch: 85	train loss: 1408.8583984375	(31.5s)
epoch: 86	train loss: 319.4386901855469	(31.3s)
epoch: 87	train loss: 132.29103088378906	(30.5s)
epoch: 88	train loss: 193.1337890625	(30.8s)
epoch: 89	train loss: 308.3046569824219	(31.5s)
epoch: 90	train loss: 383.2735290527344	(31.5s)
epoch: 91	train loss: 861.5259399414062	(31.4s)
epoch: 92	train loss: 350.6522521972656	(31.5s)
epoch: 93	train loss: 354.124267578125	(31.4s)
epoch: 94	train loss: 583.7720336914062	(31.5s)
epoch: 95	train loss: 813.5956420898438	(31.5s)
epoch: 96	train loss: 415.82183837890625	(31.4s)
epoch: 97	train loss: 255.43222045898438	(31.4s)
epoch: 98	train loss: 163.7755584716797	(31.2s)
epoch: 99	train loss: 249.2737274169922	(30.5s)
Evaluating model on 200 episodes
0.0002666893821177447
0.0006484487269557102
8.726259129616664e-05
9.84844420599984e-05
9.322144863912591e-05
0.0004770174282138878
0.0007395954424282535
0.00014506391098620953
0.00042443949179517314
0.00039401586904967836
0.0004270524059847958
0.0008560726191717549
0.0004935568519743226
0.00016903012143571955
0.0002624104711899503
0.0005559032986853066
0.0003535144844742414
0.00021443398974143848
0.00015344187760582336
0.00025303572638222247
0.00011341891190063002
0.0005957858173530894
0.0005599956920460417
0.0002937842942254279
0.0006940652376417549
0.0003304396361658512
0.00041755207829347843
0.0005682275618710264
0.0010317453153321468
0.0002290714779788956
0.0003263768893383393
0.0005855443028224272
0.0006616094203006165
0.0002904914845566964
0.0007401390945661286
0.0009327757251513605
0.0004015631320486902
0.0005967604391230452
0.0007403824426243469
0.0004013350686503915
0.0003468605800662772
0.00030554512107735957
0.0003178207805141373
0.0002302872003551952
0.0004631121994480641
0.0003403857506580405
0.00035226132682250215
0.0004625393222441845
8.54701065691188e-05
0.0004063781570948777
9.949678143825354e-05
0.00041251382108606685
0.00042256318317868136
0.00012933785319546587
0.00032105494599442145
0.0007787066617311212
0.0009081498856642448
0.000584413569365432
0.00022983804785112357
0.0005578952708068149
0.00037691329238701395
0.0003938813566492172
0.00031573136497032607
0.0003447817487085558
0.0002631578157073316
0.0003029915885987145
0.0005327194491352808
0.00039920909090908124
0.00027763886297786183
0.00042129968278459273
0.00035685485063368104
0.00046789570840126925
0.00018197692968343566
0.0004335945079674275
0.0002788653781118972
0.0003845079424092208
0.0002792847815726418
0.00028109633415718645
0.00042262128866039147
0.00018876377708650018
0.00029336236821109196
0.00045826771456631833
0.0004625729363718111
0.00012751782472097308
0.0006343695815000198
0.00011169202662131283
0.0005747557386712287
0.0002964723140895989
0.0003547815444449043
0.00036238495242287174
0.00033974108229248767
0.0007426538347772293
0.0005136595122919595
0.0003986486743391045
0.0005764434114368029
0.0008359254315586267
0.0009494149963897003
0.0006873868137972749
0.00019937216066781812
0.0005576665476084976
0.0005301897400840971
0.000460334392398311
0.00035054941828447126
0.000461256733238958
0.0005385408013959628
0.0003378396376043067
0.000509371890575494
0.0002422792601594535
0.00040853730345025683
0.0004938518395647407
0.0005860258417555997
0.0003489861612014584
0.00021694747036755352
0.0002608447184684337
0.00037885018893575763
0.00036808560585284243
0.0003501411092656781
0.00024252789667116302
0.0002148950634364155
0.0002631404947048092
0.00035688544671251695
0.00038705630570872856
0.00029291983516416693
0.00034782931070846023
0.0002683775942387001
0.000335121394982707
0.0004643203631581451
0.0003795984117118678
0.00025742486591947715
0.00024013819953931184
0.00036312544034444727
0.00031068605377575817
6.855517895019148e-05
0.00017449831119409755
0.00031435088515406406
0.0005175213875511819
0.00025432743411958934
0.0008147101854293022
6.18444983047084e-05
0.0006966612670242155
0.0003043740048451582
0.0004317899478653841
0.00010467020570104069
0.00024134282576789812
0.0006878477107774314
0.00037493041150703314
0.00025845457787454507
0.00023202693434238125
0.0002645755665266084
0.0003394219773124658
0.0005567533180115076
0.00011696754318109015
0.00033338724197164995
0.0007177795612898825
0.0007415166308783228
0.0004613119175211558
0.00030243396148372473
0.0006291334402270703
0.00043196672908783914
0.00021949301293661296
0.00013417824302450754
0.00035255111826396995
0.00018969541005472738
0.00032478220896337007
0.0005336449949785068
8.602737729875919e-05
0.00035993126952551233
0.00015463549190523432
0.0006720058649772822
0.0008059918372429606
0.00019786253596976167
0.00033829988283149294
0.00040473770786775276
3.870232291471136e-05
0.0005052947575071206
0.00035452572161454515
0.0005862898407775157
0.00018651146010597585
0.00041519335064506215
0.0005704181335204339
0.00039994302008408613
0.0007251536302157127
0.0001267661245947238
0.0003080735201258315
0.00040114632871942983
0.00021751750034087783
0.00041557053125209427
0.0001629670460505832
0.0004387070407436554
0.0003798234698074844
0.00021536433120094114
0.00023778169361321488
0.00046841335029707156
0.0001493838567925691
0.0003563489980024315
0.0002103095353959361
Solved 196/200 episodes
0.00038201567674536296
Evaluated model in 45.3 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-15
Round 16
Generated trajectories in 109.9 seconds
epoch: 0	train loss: 2230.911865234375	(31.4s)
epoch: 1	train loss: 1309.6580810546875	(31.3s)
epoch: 2	train loss: 1078.8369140625	(31.6s)
epoch: 3	train loss: 818.753173828125	(31.7s)
epoch: 4	train loss: 899.058349609375	(31.5s)
epoch: 5	train loss: 1181.23681640625	(31.5s)
epoch: 6	train loss: 793.9644165039062	(31.5s)
epoch: 7	train loss: 828.7027587890625	(31.6s)
epoch: 8	train loss: 615.3038330078125	(31.0s)
epoch: 9	train loss: 653.89892578125	(30.6s)
epoch: 10	train loss: 728.1014404296875	(30.5s)
epoch: 11	train loss: 605.2447509765625	(31.0s)
epoch: 12	train loss: 1268.09033203125	(31.6s)
epoch: 13	train loss: 853.1635131835938	(31.7s)
epoch: 14	train loss: 574.7828369140625	(31.5s)
epoch: 15	train loss: 1048.1534423828125	(31.4s)
epoch: 16	train loss: 545.5863647460938	(31.4s)
epoch: 17	train loss: 481.8377380371094	(31.5s)
epoch: 18	train loss: 414.70263671875	(31.6s)
epoch: 19	train loss: 372.90185546875	(31.5s)
epoch: 20	train loss: 829.9511108398438	(31.5s)
epoch: 21	train loss: 620.1095581054688	(31.5s)
epoch: 22	train loss: 635.1830444335938	(31.6s)
epoch: 23	train loss: 406.0046691894531	(31.0s)
epoch: 24	train loss: 985.880126953125	(30.5s)
epoch: 25	train loss: 1815.028076171875	(30.5s)
epoch: 26	train loss: 828.5926513671875	(31.0s)
epoch: 27	train loss: 530.0440063476562	(31.5s)
epoch: 28	train loss: 438.6580505371094	(31.6s)
epoch: 29	train loss: 656.9073486328125	(31.6s)
epoch: 30	train loss: 824.408203125	(31.5s)
epoch: 31	train loss: 773.4125366210938	(31.4s)
epoch: 32	train loss: 979.4151611328125	(31.6s)
epoch: 33	train loss: 294.0535888671875	(31.7s)
epoch: 34	train loss: 220.71932983398438	(31.5s)
epoch: 35	train loss: 419.0372619628906	(31.6s)
epoch: 36	train loss: 673.8284912109375	(31.5s)
epoch: 37	train loss: 361.0315246582031	(31.6s)
epoch: 38	train loss: 269.8148498535156	(31.6s)
epoch: 39	train loss: 537.0213623046875	(31.5s)
epoch: 40	train loss: 414.0458679199219	(31.4s)
epoch: 41	train loss: 1297.0526123046875	(31.0s)
epoch: 42	train loss: 916.5390625	(30.6s)
epoch: 43	train loss: 513.500732421875	(30.7s)
epoch: 44	train loss: 600.6017456054688	(30.5s)
epoch: 45	train loss: 549.6195068359375	(31.2s)
epoch: 46	train loss: 413.0931396484375	(31.3s)
epoch: 47	train loss: 275.7546081542969	(31.2s)
epoch: 48	train loss: 163.63963317871094	(31.6s)
epoch: 49	train loss: 145.09649658203125	(31.5s)
epoch: 50	train loss: 75.6751937866211	(31.5s)
epoch: 51	train loss: 582.2132568359375	(31.5s)
epoch: 52	train loss: 1546.5335693359375	(31.6s)
epoch: 53	train loss: 412.7363586425781	(31.6s)
epoch: 54	train loss: 669.10400390625	(31.4s)
epoch: 55	train loss: 989.7659301757812	(31.5s)
epoch: 56	train loss: 1039.9263916015625	(31.5s)
epoch: 57	train loss: 513.5747680664062	(31.5s)
epoch: 58	train loss: 326.7542724609375	(31.0s)
epoch: 59	train loss: 320.8542785644531	(31.1s)
epoch: 60	train loss: 175.33676147460938	(31.5s)
epoch: 61	train loss: 285.3778991699219	(31.5s)
epoch: 62	train loss: 360.3703918457031	(31.5s)
epoch: 63	train loss: 1330.383544921875	(31.6s)
epoch: 64	train loss: 1212.048095703125	(31.5s)
epoch: 65	train loss: 753.9453125	(31.5s)
epoch: 66	train loss: 786.103271484375	(31.5s)
epoch: 67	train loss: 1136.8365478515625	(31.5s)
epoch: 68	train loss: 793.01416015625	(31.6s)
epoch: 69	train loss: 504.27618408203125	(31.5s)
epoch: 70	train loss: 351.8750915527344	(31.5s)
epoch: 71	train loss: 416.32257080078125	(30.8s)
epoch: 72	train loss: 300.8576354980469	(30.6s)
epoch: 73	train loss: 896.3853759765625	(31.5s)
epoch: 74	train loss: 588.6304931640625	(31.4s)
epoch: 75	train loss: 421.1159973144531	(31.5s)
epoch: 76	train loss: 863.9508666992188	(31.5s)
epoch: 77	train loss: 497.1163330078125	(31.5s)
epoch: 78	train loss: 799.794921875	(31.6s)
epoch: 79	train loss: 919.4217529296875	(31.4s)
epoch: 80	train loss: 401.7571105957031	(31.5s)
epoch: 81	train loss: 457.8370361328125	(31.5s)
epoch: 82	train loss: 501.5397644042969	(31.5s)
epoch: 83	train loss: 561.8091430664062	(31.6s)
epoch: 84	train loss: 267.34454345703125	(31.1s)
epoch: 85	train loss: 324.48321533203125	(30.5s)
epoch: 86	train loss: 203.71353149414062	(30.7s)
epoch: 87	train loss: 415.9769287109375	(31.4s)
epoch: 88	train loss: 347.68438720703125	(31.6s)
epoch: 89	train loss: 848.2990112304688	(31.4s)
epoch: 90	train loss: 418.9200744628906	(31.5s)
epoch: 91	train loss: 1030.802978515625	(31.4s)
epoch: 92	train loss: 1041.8226318359375	(31.5s)
epoch: 93	train loss: 698.42578125	(31.6s)
epoch: 94	train loss: 311.0062561035156	(31.4s)
epoch: 95	train loss: 617.4769287109375	(31.4s)
epoch: 96	train loss: 395.6141357421875	(31.5s)
epoch: 97	train loss: 641.0116577148438	(31.6s)
epoch: 98	train loss: 460.9181823730469	(31.6s)
epoch: 99	train loss: 350.6477355957031	(31.5s)
Evaluating model on 200 episodes
0.0007251592572477724
0.0002926521856920772
0.0002743071566025416
0.0003584500322225771
8.422047080279703e-05
0.0008328815484591443
0.00019934487488132183
0.0006768651356155301
0.0002352042266124954
0.00034495861114812535
8.113019279774343e-05
0.0005150434529544631
0.00025706399732631065
0.000423458860223036
6.868175100144551e-05
0.0004505517958816012
0.0003252206402066804
0.00018732863190962235
0.0005164171928246266
0.00034006153432666463
0.0005745018702525514
0.00024952360126917483
0.0004467884684231649
0.0004231842010139029
0.0004849027327215862
0.00029258648226396906
0.0004994756769598968
0.0003994610272558121
0.000285105394211244
0.00022777057660277933
0.00024258685984764102
0.0001422180902361441
0.00013921137391056013
0.0002208697732467871
0.00027226003219445427
0.00039900042096026783
0.00044019047835228354
0.00033445044561328057
0.00021148540217301857
0.0002277936408063397
0.00047274742026333583
0.0003070046960056061
0.0005446585917178752
0.0001529464205175048
0.0006262823390234435
0.0003127646058211602
0.0005263908414938421
0.0005000784287403804
0.00031410702474574836
0.00038061525922077836
0.0003396326269117148
0.0002089894910013976
0.0003379064433153335
0.0003863882945512634
0.0004477585213370124
0.000659456415708822
0.00048817694785516084
0.0005683526569555397
0.0006140535815575277
0.00023327095605054637
0.0003757543623426803
0.00017192837312904886
0.0003733378843106847
0.00022489170727827253
0.00032359643247266494
0.00010602614408223114
0.000368751414941004
0.00043701267392342355
0.0005328098466571677
0.0001172953138006118
0.0002571640380015846
0.0002838259326381376
0.0005033344317174245
0.0002920353457043615
0.00013020319350359315
0.0005136340550961904
0.0001937169802439367
0.0007247562808708718
0.0003371553110203725
0.00036895977227273266
0.00020100679234928974
0.0004622531108301094
0.0005610172219652061
0.0003608404864595426
0.0004937389098220434
0.00011017988100847058
0.00022130175161424524
0.00021547755432038684
0.00018379952395180762
0.0004317836067993761
0.0005411213816209372
0.00048199979101766423
0.0001343291387172485
0.00017737320712722765
0.00023761060153695078
0.0002701667175036467
0.00042136823663675
0.0002304417430044244
0.00021668248515467254
0.00019657099191510284
0.00015415875678508715
0.0002909825774319567
0.00021538570511190548
0.0004005423508628848
0.0002200706737009265
0.0002980815852424712
0.0007127754699164749
0.0003964018481686556
0.0008118527243352908
0.00031590885499186
0.00017913881474669325
0.00040192216837222705
0.0004607535780678518
0.000352128574216776
0.0007132927702286906
0.00016479564862453591
0.0009560603953344215
0.0004181928729344185
0.00031743925615046464
0.00041907331344830437
0.00043227315340614456
0.000314132922085264
0.00016849026027817413
0.0005897318753543206
9.454683388619081e-05
0.00023223176615121903
0.00041859936049554056
0.0005891562283594746
0.0001504651863797335
0.00015566900971239911
0.0006790425941289868
0.0004910003329813105
9.612550522433594e-05
0.00041166449418303657
0.00015964511868123038
0.0005000724205628001
0.000786152643336714
0.0005365776457243296
0.0005289277991430615
0.0003292053184743375
0.0002827961333575028
0.00011186371926896754
0.0007638476578774446
0.0002781859040320948
0.00018578662377422006
0.00023660311407266735
0.00024334773468126514
0.0004059157867671055
0.0004898395215439615
0.0002484496392046114
0.0003193986154588789
0.0007005616133798098
0.00034479526076556166
0.00033053136657317
0.0005865760061472732
7.174850761657581e-05
0.00020286210701669916
0.0005501586368457841
0.00011239413470320869
0.00020290270149416755
0.00030109232317375825
0.0002642988430579862
0.00029694959864303073
0.000647927922143156
0.0005274100565770823
0.00047128342900857836
0.0004457974637261941
0.00052068384546281
0.0001654614817079678
0.00020973341106582665
0.0003463837613108285
0.0004849315309911617
0.00034130262042708637
0.000619214543081291
0.0006866548288886215
0.00023619589844132023
0.0005511704307537002
0.00030540371852990137
0.0004596324680068602
0.00017552088310798068
0.00030332712909902123
0.0004735210100254615
0.0004120896813765285
0.00028282000245299596
0.000288202813726457
0.00021042143533336725
0.00021175723804844893
0.00046431650525846635
0.0003789194288064782
0.00031842018496228353
0.0003395712182054922
0.00026816264097065227
0.00026606132723827614
0.00021231487654723742
0.00037789674947648393
0.0006454704837040026
0.0004699169054068382
0.00045806111011888087
0.00027450840807432544
0.0001429088307094599
Solved 200/200 episodes
0.0003614807625832764
Evaluated model in 48.8 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-16
Round 17
Generated trajectories in 105.5 seconds
epoch: 0	train loss: 1571.4520263671875	(31.5s)
epoch: 1	train loss: 944.0921020507812	(31.5s)
epoch: 2	train loss: 979.8329467773438	(31.6s)
epoch: 3	train loss: 763.8617553710938	(31.7s)
epoch: 4	train loss: 1097.006591796875	(31.7s)
epoch: 5	train loss: 611.720947265625	(31.7s)
epoch: 6	train loss: 734.193359375	(31.8s)
epoch: 7	train loss: 1820.722900390625	(31.6s)
epoch: 8	train loss: 1023.6285400390625	(31.7s)
epoch: 9	train loss: 1043.34423828125	(31.7s)
epoch: 10	train loss: 598.7777709960938	(31.8s)
epoch: 11	train loss: 344.7621765136719	(31.8s)
epoch: 12	train loss: 278.1923522949219	(31.7s)
epoch: 13	train loss: 673.620361328125	(31.7s)
epoch: 14	train loss: 570.1854858398438	(31.6s)
epoch: 15	train loss: 1740.2232666015625	(31.2s)
epoch: 16	train loss: 841.08935546875	(30.7s)
epoch: 17	train loss: 507.2717590332031	(30.6s)
epoch: 18	train loss: 296.63800048828125	(31.0s)
epoch: 19	train loss: 272.51318359375	(31.3s)
epoch: 20	train loss: 893.0576782226562	(31.8s)
epoch: 21	train loss: 621.8016967773438	(31.8s)
epoch: 22	train loss: 408.15423583984375	(31.8s)
epoch: 23	train loss: 724.5095825195312	(31.8s)
epoch: 24	train loss: 761.8112182617188	(31.7s)
epoch: 25	train loss: 554.0126342773438	(31.8s)
epoch: 26	train loss: 458.9608154296875	(31.8s)
epoch: 27	train loss: 1502.1279296875	(31.8s)
epoch: 28	train loss: 865.7564697265625	(31.8s)
epoch: 29	train loss: 449.35784912109375	(31.7s)
epoch: 30	train loss: 203.5987548828125	(31.8s)
epoch: 31	train loss: 289.1357421875	(31.8s)
epoch: 32	train loss: 475.8021545410156	(31.8s)
epoch: 33	train loss: 489.4423828125	(31.4s)
epoch: 34	train loss: 942.9127807617188	(30.7s)
epoch: 35	train loss: 538.5029296875	(30.8s)
epoch: 36	train loss: 368.1222839355469	(30.8s)
epoch: 37	train loss: 553.892822265625	(31.4s)
epoch: 38	train loss: 504.90594482421875	(31.0s)
epoch: 39	train loss: 700.831298828125	(30.7s)
epoch: 40	train loss: 845.1106567382812	(30.8s)
epoch: 41	train loss: 610.6328125	(30.9s)
epoch: 42	train loss: 308.2449645996094	(30.8s)
epoch: 43	train loss: 590.822021484375	(30.9s)
epoch: 44	train loss: 844.057861328125	(30.8s)
epoch: 45	train loss: 865.3619384765625	(30.9s)
epoch: 46	train loss: 718.1929321289062	(31.5s)
epoch: 47	train loss: 869.606689453125	(31.7s)
epoch: 48	train loss: 1436.8858642578125	(31.9s)
epoch: 49	train loss: 359.7184753417969	(31.7s)
epoch: 50	train loss: 249.1455535888672	(31.8s)
epoch: 51	train loss: 142.38265991210938	(31.9s)
epoch: 52	train loss: 380.2192077636719	(31.8s)
epoch: 53	train loss: 1469.7161865234375	(31.8s)
epoch: 54	train loss: 653.784912109375	(31.0s)
epoch: 55	train loss: 868.5430908203125	(31.8s)
epoch: 56	train loss: 512.1444702148438	(31.8s)
epoch: 57	train loss: 262.4079895019531	(31.9s)
epoch: 58	train loss: 145.1193389892578	(31.9s)
epoch: 59	train loss: 88.04534149169922	(31.7s)
epoch: 60	train loss: 357.096435546875	(31.8s)
epoch: 61	train loss: 438.4129638671875	(31.7s)
epoch: 62	train loss: 934.41357421875	(31.6s)
epoch: 63	train loss: 778.3390502929688	(30.8s)
epoch: 64	train loss: 398.2853088378906	(31.1s)
epoch: 65	train loss: 322.8641357421875	(31.8s)
epoch: 66	train loss: 318.9126892089844	(31.7s)
epoch: 67	train loss: 1627.6007080078125	(31.8s)
epoch: 68	train loss: 1263.1805419921875	(31.9s)
epoch: 69	train loss: 590.9198608398438	(31.7s)
epoch: 70	train loss: 782.5283813476562	(31.8s)
epoch: 71	train loss: 520.7235717773438	(31.7s)
epoch: 72	train loss: 263.703857421875	(31.8s)
epoch: 73	train loss: 340.1190185546875	(31.8s)
epoch: 74	train loss: 446.4251403808594	(30.8s)
epoch: 75	train loss: 1791.6005859375	(30.8s)
epoch: 76	train loss: 530.1591796875	(31.3s)
epoch: 77	train loss: 480.38214111328125	(31.9s)
epoch: 78	train loss: 526.8001708984375	(31.9s)
epoch: 79	train loss: 265.4052734375	(31.8s)
epoch: 80	train loss: 155.79476928710938	(31.8s)
epoch: 81	train loss: 187.00814819335938	(31.7s)
epoch: 82	train loss: 147.9214324951172	(31.8s)
epoch: 83	train loss: 232.72232055664062	(31.9s)
epoch: 84	train loss: 1271.9453125	(31.7s)
epoch: 85	train loss: 318.6483154296875	(30.9s)
epoch: 86	train loss: 730.73681640625	(30.7s)
epoch: 87	train loss: 175.28890991210938	(31.2s)
epoch: 88	train loss: 420.1723937988281	(31.8s)
epoch: 89	train loss: 253.4864959716797	(31.7s)
epoch: 90	train loss: 508.7603759765625	(31.7s)
epoch: 91	train loss: 592.8218994140625	(31.7s)
epoch: 92	train loss: 1026.0830078125	(31.8s)
epoch: 93	train loss: 482.4122314453125	(31.8s)
epoch: 94	train loss: 787.8050537109375	(31.8s)
epoch: 95	train loss: 1004.2191162109375	(31.8s)
epoch: 96	train loss: 500.01629638671875	(31.6s)
epoch: 97	train loss: 460.33489990234375	(31.7s)
epoch: 98	train loss: 274.8492431640625	(31.4s)
epoch: 99	train loss: 368.4752502441406	(30.8s)
Evaluating model on 200 episodes
0.0003228258810850093
0.0004381589631734909
0.00025908845632597254
0.0003620509232057563
0.00028211900995042276
0.00038221647970631775
0.000392299085713323
0.0002680617167459873
0.0002805033829803427
0.00013823511768637937
0.0002552055764681427
0.0003038542642267809
0.00033350653426499446
0.00020366851094877347
0.0002531839207473115
0.0003584029572624731
0.0001625489312573336
0.00027413919794044513
0.0004224256990141839
0.0006114531419866994
0.00033240677316825895
0.00017808282990743117
0.0003096657095542272
0.0003192689599879821
0.00027321769685942045
0.0008280857606938038
0.00028375384863466024
0.00031807989773824374
0.000277253901819269
0.0006658913799684443
0.0008361819869833198
0.0009188680984758927
0.0005852313738432713
0.00037552942436983966
0.0005058779824375287
0.0004697924794527353
0.0004439050867187602
0.00015510357240634254
0.0004206519780871951
0.0001062118518530042
0.0003597352724203742
0.0007158101635958211
0.00024528477226868424
0.0002830527146565706
0.00022961292446173806
0.0001857425468188012
0.0007543798049027828
0.00018086787955932473
0.0002463010567589663
0.0006032405733182508
0.0005294574332517434
0.0005738884084556568
0.0003425097499010527
0.0001998723528231494
0.00035021639789394813
0.0001339262729137166
0.0004353395129697133
0.0005649535892838945
0.00039240201693333107
0.00017551329783600523
0.0007784933801693634
0.00014997997362418877
0.00017170749545130092
0.0007442657103768267
0.00028461652291298376
0.0003036694547517982
7.372972344027777e-05
0.0001711296425989866
0.0007844439972429357
0.00016759353346580092
0.0004348471900435658
0.0005664506901192236
0.0003456625651597278
0.000336092161887791
0.00043592042762611525
0.0004638846902606828
0.00010900714683388783
0.000207831213260392
0.0002027017912537717
0.0002467080906353658
0.00012226860363786246
0.00022523944592952225
0.00038287334960100623
0.00030367705067664455
0.0005651322737642605
0.0004200529637533431
0.00020088102251305062
0.00037418485138645066
0.00047693063928678527
0.00042270229914720403
0.0007343593994038303
0.00022032899027248345
0.00019950951605096634
0.00036699331553791126
0.00041564341686521573
0.00030120261307393264
0.0002569055974163348
0.00019082445204035407
0.0004601509899179004
0.0005494123564018921
0.0002560305401054981
0.0009085574236451066
0.00028349287617857044
0.00024944259495250957
0.0002636519800580572
0.0006170479246065952
0.0005581309730008467
0.0005228541587230746
0.0006779169587603331
0.0004506927860730732
0.000456093073311918
0.00023974314022138168
0.00014259697157588866
0.0003699540175148286
0.0008004123062467746
0.0005236118844060492
0.00022407629530789562
0.00017258287743970868
0.0004998969251413227
0.0001736571849553576
0.0002713150085513077
0.0002784422737965576
0.0004172399764814015
0.00026007591062807476
0.0004685900647928065
0.0002814120081211513
0.00031614293563025594
0.00038185945002049647
0.0002681607012859518
0.00023327007329498883
0.000272206295903743
0.0003613289636996342
0.00016800167063860607
0.0003767643827207697
8.867026223345998e-05
0.0003682045829615769
0.00022296091802773946
0.00022843303058834864
0.0007064001325819514
0.0007034668501703357
0.00029493304368531284
0.00021623466398943073
0.0007970731618115679
0.0009393620545344437
0.00038686475732232025
0.00047466525923785713
0.0003969870457998089
0.00011358136735647609
0.00021496098319263323
9.897592438144123e-05
0.00029891211901981893
0.00040924885238032753
7.514112924885315e-05
0.00043181133246117647
0.0003620873326326546
8.04204864834901e-05
0.00014834167251881973
0.00025853961324173724
0.0004599096029874329
0.00040233934363310584
7.244329061415137e-05
0.00027374558140553953
0.00043284893979068364
0.00040994402677271233
0.00028618945143534804
0.0007594423664808606
0.0005106407939415982
0.0003706990049977321
0.0003721948760357918
0.00034794362141433847
0.00010811337313043623
0.00023131947717047296
0.0004879301570326788
0.0003716645255086145
0.0002469011011625197
0.0002134061793951428
0.00012456954454347186
0.00015608115268150868
0.0004497910162172047
0.00021011570540643086
0.0003774967564303162
9.996913173256025e-05
0.0003178696781385672
0.0010552556163424419
0.0003800730826494853
0.0002591759648187993
0.00040380619563101
0.0003807361746213347
0.0003375973349742206
0.00015546604905027599
0.00030196483900605625
0.0005889845297133434
0.0006691079972370062
0.0004258115750417346
0.0002109716244289995
0.0004737940111954231
0.00026877724250821744
0.00015091489625900005
Solved 198/200 episodes
0.00035897329323826934
Evaluated model in 44.8 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-17
Round 18
Generated trajectories in 107.3 seconds
epoch: 0	train loss: 1668.468505859375	(31.3s)
epoch: 1	train loss: 799.207763671875	(31.3s)
epoch: 2	train loss: 1130.655517578125	(31.5s)
epoch: 3	train loss: 702.9423828125	(31.5s)
epoch: 4	train loss: 1060.9686279296875	(31.4s)
epoch: 5	train loss: 546.69677734375	(31.5s)
epoch: 6	train loss: 443.6203918457031	(31.6s)
epoch: 7	train loss: 557.521484375	(31.4s)
epoch: 8	train loss: 934.594482421875	(31.5s)
epoch: 9	train loss: 491.02435302734375	(31.3s)
epoch: 10	train loss: 334.7403564453125	(30.7s)
epoch: 11	train loss: 818.4589233398438	(30.7s)
epoch: 12	train loss: 542.0770874023438	(30.5s)
epoch: 13	train loss: 503.0150146484375	(30.8s)
epoch: 14	train loss: 688.059326171875	(31.1s)
epoch: 15	train loss: 1420.39453125	(30.9s)
epoch: 16	train loss: 471.02825927734375	(31.8s)
epoch: 17	train loss: 572.6611328125	(30.8s)
epoch: 18	train loss: 340.48724365234375	(31.4s)
epoch: 19	train loss: 226.02317810058594	(31.4s)
epoch: 20	train loss: 235.6379852294922	(31.6s)
epoch: 21	train loss: 751.054931640625	(31.7s)
epoch: 22	train loss: 546.4759521484375	(31.5s)
epoch: 23	train loss: 701.9457397460938	(31.6s)
epoch: 24	train loss: 517.63671875	(30.9s)
epoch: 25	train loss: 688.8319091796875	(31.2s)
epoch: 26	train loss: 167.5640869140625	(31.6s)
epoch: 27	train loss: 247.6212921142578	(31.6s)
epoch: 28	train loss: 563.6168823242188	(31.6s)
epoch: 29	train loss: 694.7490234375	(31.5s)
epoch: 30	train loss: 1137.420654296875	(31.6s)
epoch: 31	train loss: 786.2662963867188	(31.7s)
epoch: 32	train loss: 342.82086181640625	(30.8s)
epoch: 33	train loss: 755.180908203125	(30.7s)
epoch: 34	train loss: 377.6913146972656	(31.4s)
epoch: 35	train loss: 278.5120544433594	(31.6s)
epoch: 36	train loss: 397.0208435058594	(31.7s)
epoch: 37	train loss: 811.5507202148438	(31.6s)
epoch: 38	train loss: 574.673095703125	(31.6s)
epoch: 39	train loss: 1119.744384765625	(31.5s)
epoch: 40	train loss: 563.9236450195312	(31.6s)
epoch: 41	train loss: 606.41259765625	(31.4s)
epoch: 42	train loss: 890.5116577148438	(30.5s)
epoch: 43	train loss: 314.7738037109375	(30.6s)
epoch: 44	train loss: 186.9900360107422	(31.4s)
epoch: 45	train loss: 304.6289978027344	(31.6s)
epoch: 46	train loss: 1501.499267578125	(31.6s)
epoch: 47	train loss: 561.45751953125	(31.5s)
epoch: 48	train loss: 427.5987243652344	(31.6s)
epoch: 49	train loss: 291.8367919921875	(31.6s)
epoch: 50	train loss: 290.8990783691406	(31.6s)
epoch: 51	train loss: 317.01409912109375	(31.7s)
epoch: 52	train loss: 177.4436492919922	(31.3s)
epoch: 53	train loss: 424.6649169921875	(30.6s)
epoch: 54	train loss: 373.7491760253906	(30.7s)
epoch: 55	train loss: 429.1138916015625	(30.8s)
epoch: 56	train loss: 544.4160766601562	(31.4s)
epoch: 57	train loss: 200.9306640625	(31.5s)
epoch: 58	train loss: 845.1077270507812	(31.5s)
epoch: 59	train loss: 1045.465576171875	(31.6s)
epoch: 60	train loss: 517.2277221679688	(31.5s)
epoch: 61	train loss: 240.8612060546875	(31.5s)
epoch: 62	train loss: 188.56402587890625	(31.5s)
epoch: 63	train loss: 218.5041046142578	(31.6s)
epoch: 64	train loss: 162.3012237548828	(31.7s)
epoch: 65	train loss: 1263.1302490234375	(31.1s)
epoch: 66	train loss: 1010.8253173828125	(30.6s)
epoch: 67	train loss: 455.6977844238281	(30.5s)
epoch: 68	train loss: 157.94931030273438	(30.8s)
epoch: 69	train loss: 331.46856689453125	(31.4s)
epoch: 70	train loss: 1236.9793701171875	(31.5s)
epoch: 71	train loss: 513.962158203125	(31.6s)
epoch: 72	train loss: 456.0160217285156	(31.5s)
epoch: 73	train loss: 139.45655822753906	(31.6s)
epoch: 74	train loss: 537.9476318359375	(31.6s)
epoch: 75	train loss: 158.16915893554688	(31.5s)
epoch: 76	train loss: 340.59832763671875	(31.6s)
epoch: 77	train loss: 396.61376953125	(31.6s)
epoch: 78	train loss: 458.2808837890625	(31.6s)
epoch: 79	train loss: 177.16436767578125	(31.3s)
epoch: 80	train loss: 724.3304443359375	(30.5s)
epoch: 81	train loss: 386.77410888671875	(30.6s)
epoch: 82	train loss: 645.034912109375	(30.9s)
epoch: 83	train loss: 426.01519775390625	(31.2s)
epoch: 84	train loss: 1702.40673828125	(31.0s)
epoch: 85	train loss: 615.8053588867188	(30.4s)
epoch: 86	train loss: 745.9873046875	(30.5s)
epoch: 87	train loss: 183.50343322753906	(30.6s)
epoch: 88	train loss: 119.87860107421875	(30.5s)
epoch: 89	train loss: 237.05465698242188	(30.5s)
epoch: 90	train loss: 787.6015625	(30.4s)
epoch: 91	train loss: 519.82666015625	(30.5s)
epoch: 92	train loss: 616.5164184570312	(30.6s)
epoch: 93	train loss: 349.043212890625	(30.4s)
epoch: 94	train loss: 1061.5120849609375	(30.5s)
epoch: 95	train loss: 301.802734375	(30.5s)
epoch: 96	train loss: 402.3733215332031	(30.5s)
epoch: 97	train loss: 495.7294006347656	(30.6s)
epoch: 98	train loss: 390.505859375	(30.4s)
epoch: 99	train loss: 232.6754913330078	(30.5s)
Evaluating model on 200 episodes
0.00021377912917159848
0.0002409638111203094
0.0005142439296105295
0.00012431423506530423
0.00023410474979913408
0.0005929376306994527
0.00042077598745891007
0.0001592165209634307
0.0005344301648998832
0.00017477629961225224
0.00032735348197223047
0.0005101752814880456
0.00035537724841105955
0.0006713840941167156
0.000280433206626185
0.0007034841944568186
0.00012107782803964669
0.0004729425777854472
0.00041012811737440143
0.00028096559556918993
0.0002702770562195681
0.000596872229592312
0.00022098378919027336
0.00017654540087360974
0.00022326436642823168
0.00011719923067606655
0.00042002110338520043
0.0007782017330302753
0.00041401634986998814
0.0005743186325895706
0.00024441197479063703
0.00028724732326806517
0.00012695100809878568
0.00019587396094114713
0.0002277226759235481
0.00014410425766679205
0.00027649255110061077
0.00010269620037603133
0.0003690437259657691
0.0005917742487543451
0.00026522541461586287
0.0004473984516090272
0.00015195684093615615
0.00034100073056227603
0.00013885615368280924
0.00027558911385901463
0.00030086566447179767
0.0008843265157284965
0.00024203152534854863
0.00039573687051112455
0.0002300840804865503
0.0006626178387705295
0.0003541024897848339
0.0006129949401383783
0.00018717292673767696
0.0003663217154864565
0.0004140769502886211
0.00034973262248086303
0.0001930646426877675
0.00014858900138076732
0.0002243977804078491
0.00023293989693229378
0.00023827589308211827
0.0006618863228879945
0.0004629554251778245
0.00040579816413810477
0.00022000785726128885
0.0002711464750237134
0.00030801593383994013
0.00045834324044549516
0.0003475813453757925
0.0003627253142137003
0.0006445752729772581
0.00016962224981398322
0.00039535160657188886
0.0003172496732683309
0.0002958193678750831
0.0002784490889856291
0.00021989818895618604
6.116325273199306e-05
0.00029393139419354205
0.00037073765937141917
0.00021610597848158595
0.0003313161733785819
0.00029449469871502113
0.0005340172216771558
0.0002538160428546531
0.00043246099099073594
0.00036823777578320004
0.0002907351267822378
0.00021716347509936895
0.0003436504683520525
0.0003849610587394636
0.0005168528399575307
0.0004096159714897636
0.00038350845919748756
0.0005946935286829103
0.00016818938324988643
0.0004377681915294878
0.000381688743841964
0.00013779515112136406
0.0005167146745616964
0.00032711670190918894
0.00035908076440844324
0.00014508476751871058
0.00046302916802233086
0.00034358873143205796
0.0007136758202971672
0.0003231874216648326
0.00018353581154687245
0.0004402552198270213
0.00048477204339948145
0.000220341650795793
0.00036811078177834133
0.000264803309063508
0.00018141813584057114
0.0006536607791834589
0.00031472966222168
0.00013855947173624372
6.70586736226181e-05
0.0003668554150819892
0.0004643566328741144
0.00014011437613387697
0.00012817981213528482
0.0007853177709572871
0.00033952328186418964
0.00045068610965396755
0.0002820841982611455
0.00020720871179946698
0.00029147665350826696
0.0010328820771974279
0.0001924601334725263
0.00010310492293033999
9.719196835552187e-05
0.0001864535543245438
0.00042233999972397996
0.00038370529431552353
0.0004172181019854296
0.00029155136323017107
0.0004959664431589772
0.00046125516803806896
0.00012765083795329701
0.0002396428032481804
0.00023438810706011282
0.0003034400815522531
0.00017804159257566962
0.0004368480289745743
7.526207456762409e-05
0.0006814150437304096
0.000267465482061703
0.0005762610198230388
0.00016001387582065263
0.0003539260914193518
0.0004296039152690997
0.000353997804353412
0.00019086171708365256
0.00029883808902548696
0.00032207767290773603
0.0005031574319651728
0.000374568496908978
0.00045215834844232933
0.00014627002242052965
0.0001510035309001978
0.00023766263404381572
0.0002767773952576175
0.00018571053622350598
0.00022523990855916054
0.0004718066506939067
0.00015802138548017618
0.00039976109570381916
0.0006265316572931955
0.0002041100838141574
0.00025327403454866726
0.00028315253989387115
0.00032998497322535655
0.0008463644654015793
0.0002850893159423625
0.0003031498975313951
0.00022394362636371244
0.00045869259472391076
0.00016815484972856052
0.0005072672360816236
0.00011425413481447322
0.0002004112562493295
0.00020811754922078762
0.0004187665416762245
0.0002976695574107981
0.0003580533983848808
0.0004340169853564695
0.00025995372652687365
0.000649766216641486
0.00029185165385570144
0.0003714469292417837
0.0003901544354010656
0.00021442924765363821
0.00018113471614924492
0.0004656178671939415
0.000313458098331373
0.0001244625199272859
0.00029767529731148187
Solved 200/200 episodes
0.00033771025866878823
Evaluated model in 45.1 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-18
Round 19
Generated trajectories in 102.6 seconds
epoch: 0	train loss: 1843.172119140625	(30.5s)
epoch: 1	train loss: 1108.02587890625	(30.6s)
epoch: 2	train loss: 621.3062133789062	(30.6s)
epoch: 3	train loss: 658.2122192382812	(30.7s)
epoch: 4	train loss: 1034.35595703125	(30.6s)
epoch: 5	train loss: 751.2514038085938	(30.7s)
epoch: 6	train loss: 1179.409912109375	(30.8s)
epoch: 7	train loss: 735.5557250976562	(30.6s)
epoch: 8	train loss: 694.856201171875	(30.6s)
epoch: 9	train loss: 606.9209594726562	(30.7s)
epoch: 10	train loss: 664.6301879882812	(30.7s)
epoch: 11	train loss: 747.7817993164062	(30.8s)
epoch: 12	train loss: 444.4441833496094	(30.7s)
epoch: 13	train loss: 585.5333251953125	(30.7s)
epoch: 14	train loss: 587.6882934570312	(30.8s)
epoch: 15	train loss: 760.9585571289062	(30.7s)
epoch: 16	train loss: 350.2446594238281	(30.8s)
epoch: 17	train loss: 402.9925537109375	(30.5s)
epoch: 18	train loss: 585.6290283203125	(30.6s)
epoch: 19	train loss: 505.97576904296875	(30.6s)
epoch: 20	train loss: 421.4106750488281	(30.6s)
epoch: 21	train loss: 526.8706665039062	(30.6s)
epoch: 22	train loss: 867.13134765625	(30.6s)
epoch: 23	train loss: 1453.885986328125	(30.7s)
epoch: 24	train loss: 615.274169921875	(30.8s)
epoch: 25	train loss: 364.02447509765625	(30.7s)
epoch: 26	train loss: 219.4355926513672	(30.7s)
epoch: 27	train loss: 449.2457275390625	(30.7s)
epoch: 28	train loss: 522.390625	(30.7s)
epoch: 29	train loss: 1035.0224609375	(30.8s)
epoch: 30	train loss: 467.86456298828125	(30.6s)
epoch: 31	train loss: 471.89801025390625	(30.6s)
epoch: 32	train loss: 403.91668701171875	(30.8s)
epoch: 33	train loss: 248.79156494140625	(30.7s)
epoch: 34	train loss: 1672.7576904296875	(30.8s)
epoch: 35	train loss: 392.9212341308594	(30.6s)
epoch: 36	train loss: 409.5754089355469	(30.7s)
epoch: 37	train loss: 230.42640686035156	(30.8s)
epoch: 38	train loss: 197.67686462402344	(30.7s)
epoch: 39	train loss: 471.066650390625	(30.7s)
epoch: 40	train loss: 308.5439453125	(30.6s)
epoch: 41	train loss: 249.5140380859375	(30.7s)
epoch: 42	train loss: 1232.14697265625	(30.8s)
epoch: 43	train loss: 1051.15234375	(30.7s)
epoch: 44	train loss: 926.0000610351562	(30.7s)
epoch: 45	train loss: 594.8702392578125	(30.7s)
epoch: 46	train loss: 432.8316650390625	(30.8s)
epoch: 47	train loss: 252.6366424560547	(30.8s)
epoch: 48	train loss: 516.8602905273438	(30.7s)
epoch: 49	train loss: 191.0023956298828	(30.8s)
epoch: 50	train loss: 318.3365478515625	(30.8s)
epoch: 51	train loss: 990.2755737304688	(30.7s)
epoch: 52	train loss: 334.04913330078125	(30.8s)
epoch: 53	train loss: 1059.8116455078125	(30.6s)
epoch: 54	train loss: 960.6586303710938	(30.7s)
epoch: 55	train loss: 256.61810302734375	(30.9s)
epoch: 56	train loss: 1110.9930419921875	(30.7s)
epoch: 57	train loss: 1041.53369140625	(30.8s)
epoch: 58	train loss: 531.6951904296875	(30.6s)
epoch: 59	train loss: 200.08273315429688	(30.7s)
epoch: 60	train loss: 230.2093963623047	(30.8s)
epoch: 61	train loss: 319.6690979003906	(30.7s)
epoch: 62	train loss: 283.44525146484375	(30.7s)
epoch: 63	train loss: 600.0225219726562	(30.6s)
epoch: 64	train loss: 235.80545043945312	(30.8s)
epoch: 65	train loss: 701.8319091796875	(30.8s)
epoch: 66	train loss: 248.05178833007812	(30.7s)
epoch: 67	train loss: 651.0672607421875	(30.7s)
epoch: 68	train loss: 222.1011962890625	(30.7s)
epoch: 69	train loss: 375.20648193359375	(30.7s)
epoch: 70	train loss: 1318.4271240234375	(30.8s)
epoch: 71	train loss: 371.76422119140625	(30.6s)
epoch: 72	train loss: 213.53982543945312	(30.8s)
epoch: 73	train loss: 654.863525390625	(30.8s)
epoch: 74	train loss: 386.9031677246094	(30.7s)
epoch: 75	train loss: 399.260498046875	(30.8s)
epoch: 76	train loss: 627.7281494140625	(30.7s)
epoch: 77	train loss: 165.53396606445312	(30.7s)
epoch: 78	train loss: 96.72595977783203	(30.8s)
epoch: 79	train loss: 943.8272094726562	(30.7s)
epoch: 80	train loss: 736.0868530273438	(30.7s)
epoch: 81	train loss: 800.5640869140625	(30.6s)
epoch: 82	train loss: 705.9260864257812	(30.7s)
epoch: 83	train loss: 293.39056396484375	(30.8s)
epoch: 84	train loss: 407.69903564453125	(30.5s)
epoch: 85	train loss: 457.5039978027344	(30.7s)
epoch: 86	train loss: 362.64715576171875	(30.7s)
epoch: 87	train loss: 226.11756896972656	(30.7s)
epoch: 88	train loss: 105.29625701904297	(30.8s)
epoch: 89	train loss: 94.96514892578125	(30.7s)
epoch: 90	train loss: 93.57365417480469	(30.7s)
epoch: 91	train loss: 105.906005859375	(30.6s)
epoch: 92	train loss: 1483.9639892578125	(30.7s)
epoch: 93	train loss: 227.89349365234375	(30.8s)
epoch: 94	train loss: 247.83229064941406	(30.6s)
epoch: 95	train loss: 426.5807189941406	(30.7s)
epoch: 96	train loss: 146.06788635253906	(30.8s)
epoch: 97	train loss: 57.791709899902344	(30.7s)
epoch: 98	train loss: 37.929161071777344	(30.8s)
epoch: 99	train loss: 31.467573165893555	(30.6s)
Evaluating model on 200 episodes
0.00012129974269507664
0.0002275360109883372
0.00017894038705890125
5.936499347853896e-05
0.00024897211031316147
0.000324163948268255
0.00033632151900876906
0.00032062391483123065
4.991940284945975e-05
0.000333091362554788
0.00018029490470325982
9.701440033690492e-05
0.0008197305040826713
0.0002710901224893612
0.00027028457139977036
0.0004020679618406575
0.00032628278753590065
0.0002865653254873366
0.00034519971086410806
0.0002734158252337693
0.00011273805509185528
0.0004227288218291133
0.0001560088370281067
4.647276767855146e-05
0.00016987075953996586
0.0003260971516933101
0.00022978235111850153
0.00029082662611585875
5.867937341463403e-05
3.3798617760812005e-05
0.00014247557439261982
0.0002493277652504008
0.000279584749659989
0.0003096641268108403
0.0002019236916983876
0.0001119424116495793
0.0003273183343157088
7.32558018918488e-05
0.0005571912806772161
0.00027067522378380345
0.0001768663513454764
0.00025746519779539466
0.00014365576549361472
0.00017527211224643434
0.00019218444131183788
0.0001409793574245722
0.0004257271066055384
0.0001655036892695231
0.00025957459913895166
0.0002503283684291091
0.00021090597951456402
0.00014849109766146285
0.0001814588668291132
0.00013227761580590316
0.00024665263526912895
0.00016012874978158558
0.0002447289237276588
0.0003925247116036701
0.00035224376752012175
0.00023352306872463183
0.00028076639876936887
0.00014592848516545737
0.0009947201251634397
0.0004689196119519086
0.0002297766939503368
0.00022331366193137237
0.00014641047050239258
5.156647802095904e-05
0.00046899201844041727
0.00013034341183508028
0.0001847599797683335
0.0001346585709468075
0.0003295771410129029
9.738315697581533e-05
8.922960096399414e-05
0.00032251572123698224
0.0002203427524311617
0.00020616051921688966
0.0002094180931298642
5.235504592349116e-05
0.00023404757071683192
9.305088322196839e-05
0.00038236652346768096
0.0003762275833695082
0.00026092051107947815
0.00034142198308018123
0.0002785839804025326
0.00018344268256006066
0.0002765285438373313
0.00020722148027895552
0.00019839499481832945
0.00036313501990597227
0.00024133746602441724
0.00012168246038366787
0.0002875875966009624
0.00037480421815416776
0.0003473746176041459
0.0002434228096766284
0.00021352658853045017
0.000313899661066711
0.00037182850217050146
0.00023071621006403637
0.00016843683932699565
0.00011584021253360046
0.00011220208449021111
0.000260853910140213
0.00012682387409768126
0.00016039721443424546
0.0003831478201353396
0.0002736004206450492
0.00020553197039134828
0.00015324893279000662
0.00023325968180395884
0.00039723565492977286
0.0001568866385923684
0.0001615929075982152
0.0001032187414896018
0.00016043358566873524
0.0002817511102594359
0.000402279150654926
0.00022781804535725313
0.00031161245619336516
0.00036314180996751844
0.00033278499304450245
5.3155849307984226e-05
0.00028535602210656084
0.00033022321679748967
0.00010809270322004687
0.00021023194976212676
5.0570299102048466e-05
0.000394281747365154
0.000200749902262487
0.001262969623068704
0.00013039037191281915
0.00016158882090167026
0.00028315588072993185
0.00021184116918959717
0.00016587138754857732
0.00039656042804381286
8.829808095264701e-05
3.173747708227465e-05
8.669475486760134e-05
0.00033234711034077827
0.00013510772305696465
0.00035076196994244434
0.00017859551542187546
7.822981539328837e-05
0.0001644032350690673
0.00015713985720822166
0.0004022541725703377
0.00041968854162778606
3.0325020361487987e-05
0.00011335936247073383
0.00025050874782769266
9.510395370633037e-05
0.00012093146409263093
0.0001320784879854536
0.0001601226755650821
0.000212117816117825
0.00022409689788105425
0.00019566634129356922
0.00030031941344077495
0.00012948136200789534
0.0002933391691433491
0.00014292449115297737
4.736930893993e-05
3.82278634560862e-05
0.00010132445636589269
0.00017278737857395754
9.64131698505558e-05
0.0001756956372751326
9.986732312780886e-05
8.234237191118154e-05
6.655461752735903e-05
0.00017968108367885752
0.0002005943646565811
0.0001245786424343957
0.0001703332384716271
0.0002552326562809391
0.00013562450098545705
0.0003017119081884327
0.0003392093663559795
0.00014357866467225205
0.0001841000859874311
0.00032287748284103983
0.00015235150231092122
0.00014709197975290326
0.00015266554606164896
0.00036007123768350927
0.00013502704973689106
7.665700080437863e-05
0.00030317493791699236
0.00019606047606778113
0.00015039401046124112
0.0001341781704506199
6.90746773519398e-05
0.00030407672334654815
0.00023887546301971775
0.000320437155570679
Solved 199/200 episodes
0.0002270135249591564
Evaluated model in 45.8 seconds
Saved model for run
62a05ac8c1af4a4592bf481e0f7d4d63 with name round-19
Completed training in 65206.6 seconds
Solved 36/200 episodes
Solved 38/200 episodes
Solved 40/200 episodes
Solved 38/200 episodes
Solved 57/200 episodes
Solved 98/200 episodes
Solved 153/200 episodes
Solved 169/200 episodes
Solved 177/200 episodes
Solved 182/200 episodes
Solved 192/200 episodes
Solved 194/200 episodes
Solved 196/200 episodes
Solved 192/200 episodes
Solved 199/200 episodes
Solved 196/200 episodes
Solved 200/200 episodes
Solved 198/200 episodes
Solved 200/200 episodes
Solved 199/200 episodes
