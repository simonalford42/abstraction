Using torch device NVIDIA TITAN Xp
net: causal
cc_weight: 1.0
abstract_pen: 0.01
params: {'epochs': 100, 'lr': 0.0008, 'n': 5000}
Net has 207442 parameters
Round 0
Generated trajectories in 104.1 seconds
epoch: 0	train loss: 146540.09375	(34.1s)
epoch: 1	train loss: 139892.703125	(30.9s)
epoch: 2	train loss: 117632.890625	(31.1s)
epoch: 3	train loss: 102913.515625	(30.9s)
epoch: 4	train loss: 95130.3359375	(31.0s)
epoch: 5	train loss: 89764.5078125	(31.0s)
epoch: 6	train loss: 85920.859375	(31.2s)
epoch: 7	train loss: 81817.765625	(31.0s)
epoch: 8	train loss: 76988.484375	(30.8s)
epoch: 9	train loss: 72559.3125	(30.9s)
epoch: 10	train loss: 69456.6640625	(31.0s)
epoch: 11	train loss: 65892.8671875	(30.8s)
epoch: 12	train loss: 63549.08203125	(30.8s)
epoch: 13	train loss: 61998.71875	(30.8s)
epoch: 14	train loss: 59728.73828125	(30.9s)
epoch: 15	train loss: 57852.1015625	(30.9s)
epoch: 16	train loss: 55089.00390625	(30.8s)
epoch: 17	train loss: 52608.5234375	(30.8s)
epoch: 18	train loss: 50091.6953125	(30.9s)
epoch: 19	train loss: 47757.84765625	(30.8s)
epoch: 20	train loss: 46278.14453125	(30.8s)
epoch: 21	train loss: 44989.94921875	(31.1s)
epoch: 22	train loss: 43665.98046875	(31.0s)
epoch: 23	train loss: 43363.1484375	(31.0s)
epoch: 24	train loss: 41867.02734375	(31.1s)
epoch: 25	train loss: 41807.3203125	(31.0s)
epoch: 26	train loss: 41165.48828125	(31.1s)
epoch: 27	train loss: 40534.37890625	(31.1s)
epoch: 28	train loss: 39690.671875	(30.9s)
epoch: 29	train loss: 39500.640625	(30.9s)
epoch: 30	train loss: 39423.0078125	(31.0s)
epoch: 31	train loss: 38929.6796875	(30.9s)
epoch: 32	train loss: 39071.1953125	(30.8s)
epoch: 33	train loss: 39064.8984375	(30.8s)
epoch: 34	train loss: 38666.14453125	(30.9s)
epoch: 35	train loss: 37370.64453125	(30.9s)
epoch: 36	train loss: 38878.22265625	(30.8s)
epoch: 37	train loss: 37561.76171875	(30.9s)
epoch: 38	train loss: 37123.73046875	(31.0s)
epoch: 39	train loss: 37072.92578125	(31.0s)
epoch: 40	train loss: 35840.25390625	(30.8s)
epoch: 41	train loss: 36773.625	(30.9s)
epoch: 42	train loss: 37431.5	(30.9s)
epoch: 43	train loss: 36877.36328125	(30.9s)
epoch: 44	train loss: 35724.58203125	(30.9s)
epoch: 45	train loss: 35216.69921875	(30.9s)
epoch: 46	train loss: 36231.56640625	(30.9s)
epoch: 47	train loss: 35341.67578125	(31.3s)
epoch: 48	train loss: 35639.47265625	(30.9s)
epoch: 49	train loss: 35327.1640625	(30.9s)
epoch: 50	train loss: 37653.453125	(31.1s)
epoch: 51	train loss: 35410.19921875	(31.4s)
epoch: 52	train loss: 36161.234375	(31.1s)
epoch: 53	train loss: 35541.58984375	(30.8s)
epoch: 54	train loss: 36502.984375	(31.1s)
epoch: 55	train loss: 35573.6328125	(30.9s)
epoch: 56	train loss: 41854.640625	(30.8s)
epoch: 57	train loss: 37554.6328125	(30.9s)
epoch: 58	train loss: 35953.7734375	(30.9s)
epoch: 59	train loss: 35446.78515625	(30.9s)
epoch: 60	train loss: 33732.32421875	(30.9s)
epoch: 61	train loss: 33975.36328125	(30.9s)
epoch: 62	train loss: 33594.56640625	(31.0s)
epoch: 63	train loss: 34135.71875	(31.0s)
epoch: 64	train loss: 34735.265625	(31.0s)
epoch: 65	train loss: 33135.79296875	(31.1s)
epoch: 66	train loss: 31569.46875	(31.2s)
epoch: 67	train loss: 31185.791015625	(31.1s)
epoch: 68	train loss: 31325.36328125	(31.0s)
epoch: 69	train loss: 30207.013671875	(30.9s)
epoch: 70	train loss: 31504.6796875	(30.9s)
epoch: 71	train loss: 31380.58984375	(30.9s)
epoch: 72	train loss: 30524.431640625	(30.9s)
epoch: 73	train loss: 32689.986328125	(30.9s)
epoch: 74	train loss: 31309.80078125	(30.8s)
epoch: 75	train loss: 30406.623046875	(31.0s)
epoch: 76	train loss: 29521.865234375	(30.9s)
epoch: 77	train loss: 31931.3671875	(30.9s)
epoch: 78	train loss: 29981.375	(30.9s)
epoch: 79	train loss: 29138.83984375	(30.9s)
epoch: 80	train loss: 29023.224609375	(30.9s)
epoch: 81	train loss: 30154.25390625	(30.9s)
epoch: 82	train loss: 28530.185546875	(31.0s)
epoch: 83	train loss: 28295.490234375	(30.9s)
epoch: 84	train loss: 28318.46484375	(30.9s)
epoch: 85	train loss: 36243.73046875	(31.1s)
epoch: 86	train loss: 29820.53515625	(31.0s)
epoch: 87	train loss: 29454.857421875	(31.7s)
epoch: 88	train loss: 28104.0	(31.1s)
epoch: 89	train loss: 28186.59765625	(31.1s)
epoch: 90	train loss: 27765.39453125	(30.7s)
epoch: 91	train loss: 27462.609375	(30.8s)
epoch: 92	train loss: 27342.51953125	(30.8s)
epoch: 93	train loss: 26397.015625	(31.0s)
epoch: 94	train loss: 25606.31640625	(30.9s)
epoch: 95	train loss: 25482.76171875	(30.9s)
epoch: 96	train loss: 25509.720703125	(30.8s)
epoch: 97	train loss: 25420.099609375	(31.1s)
epoch: 98	train loss: 24598.927734375	(30.9s)
epoch: 99	train loss: 24185.98046875	(30.8s)
Evaluating model on 200 episodes
0.0054340705119102495
0.0008070211086305789
0.0033416167663579636
0.00106708206658368
0.0017944136796662253
0.0012774484566762112
0.0017282134349443368
0.0016457436911878176
0.0011927043793548363
0.0068891631273895896
0.0016210707095028324
0.0024551995695219373
0.003664499615427173
0.0014299084915754584
0.002710061518882867
0.0015907130291452631
0.0010236622159330484
0.0017117704701377079
0.006793901502013406
0.002229780417716635
0.0017652904420034387
0.0014547861329364505
0.0016744186232244829
0.0021277901891153307
0.007597908700214854
0.0013729082325321192
0.0014443120698243465
0.0006118401839298063
0.0010889077218182916
0.0015025229497821005
0.004448745685901183
0.005259436023022447
0.002086945250145315
0.001130320262745954
Solved 34/200 episodes
0.0004198708861487698
Evaluated model in 26.0 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-0
Round 1
Generated trajectories in 103.4 seconds
epoch: 0	train loss: 35999.6015625	(30.9s)
epoch: 1	train loss: 33723.50390625	(30.8s)
epoch: 2	train loss: 32789.64453125	(31.0s)
epoch: 3	train loss: 32299.939453125	(31.0s)
epoch: 4	train loss: 29795.146484375	(30.9s)
epoch: 5	train loss: 29265.08984375	(31.0s)
epoch: 6	train loss: 28878.228515625	(30.9s)
epoch: 7	train loss: 28064.23828125	(30.8s)
epoch: 8	train loss: 27665.994140625	(30.7s)
epoch: 9	train loss: 27554.3828125	(30.9s)
epoch: 10	train loss: 28476.650390625	(30.7s)
epoch: 11	train loss: 27271.123046875	(30.9s)
epoch: 12	train loss: 26473.662109375	(30.9s)
epoch: 13	train loss: 26431.203125	(30.9s)
epoch: 14	train loss: 26112.712890625	(30.7s)
epoch: 15	train loss: 25903.166015625	(30.7s)
epoch: 16	train loss: 26094.970703125	(30.8s)
epoch: 17	train loss: 28078.0	(30.9s)
epoch: 18	train loss: 26225.87890625	(30.8s)
epoch: 19	train loss: 25803.171875	(30.8s)
epoch: 20	train loss: 25272.095703125	(30.8s)
epoch: 21	train loss: 25159.640625	(30.8s)
epoch: 22	train loss: 24991.4921875	(30.8s)
epoch: 23	train loss: 24620.134765625	(31.0s)
epoch: 24	train loss: 24173.76171875	(30.8s)
epoch: 25	train loss: 24008.365234375	(30.8s)
epoch: 26	train loss: 23881.84375	(30.9s)
epoch: 27	train loss: 24139.90234375	(30.7s)
epoch: 28	train loss: 24059.4921875	(30.9s)
epoch: 29	train loss: 24142.998046875	(30.8s)
epoch: 30	train loss: 26070.744140625	(30.9s)
epoch: 31	train loss: 23929.154296875	(30.8s)
epoch: 32	train loss: 23935.39453125	(30.9s)
epoch: 33	train loss: 23849.666015625	(30.7s)
epoch: 34	train loss: 22963.97265625	(30.7s)
epoch: 35	train loss: 23288.48828125	(30.8s)
epoch: 36	train loss: 22778.78515625	(31.0s)
epoch: 37	train loss: 22538.79296875	(30.8s)
epoch: 38	train loss: 22272.8515625	(30.9s)
epoch: 39	train loss: 22016.439453125	(30.8s)
epoch: 40	train loss: 21903.169921875	(30.8s)
epoch: 41	train loss: 21617.3359375	(30.7s)
epoch: 42	train loss: 21287.1953125	(31.0s)
epoch: 43	train loss: 21045.947265625	(30.9s)
epoch: 44	train loss: 21079.009765625	(31.0s)
epoch: 45	train loss: 21108.58203125	(30.8s)
epoch: 46	train loss: 21400.548828125	(30.9s)
epoch: 47	train loss: 21142.23828125	(30.8s)
epoch: 48	train loss: 21623.140625	(30.8s)
epoch: 49	train loss: 21063.59375	(30.8s)
epoch: 50	train loss: 21341.947265625	(30.9s)
epoch: 51	train loss: 21381.515625	(30.8s)
epoch: 52	train loss: 21081.822265625	(30.8s)
epoch: 53	train loss: 20874.029296875	(30.7s)
epoch: 54	train loss: 20987.8046875	(31.0s)
epoch: 55	train loss: 20075.447265625	(30.7s)
epoch: 56	train loss: 19751.302734375	(30.9s)
epoch: 57	train loss: 20363.2890625	(30.8s)
epoch: 58	train loss: 20077.41796875	(30.8s)
epoch: 59	train loss: 19579.6640625	(30.8s)
epoch: 60	train loss: 19834.89453125	(30.8s)
epoch: 61	train loss: 20186.7421875	(31.1s)
epoch: 62	train loss: 19887.53515625	(30.8s)
epoch: 63	train loss: 19576.701171875	(30.8s)
epoch: 64	train loss: 19531.212890625	(30.9s)
epoch: 65	train loss: 19587.5390625	(30.8s)
epoch: 66	train loss: 19469.40625	(30.8s)
epoch: 67	train loss: 19237.087890625	(30.8s)
epoch: 68	train loss: 19302.2421875	(31.0s)
epoch: 69	train loss: 20364.705078125	(30.8s)
epoch: 70	train loss: 19642.751953125	(30.9s)
epoch: 71	train loss: 18889.06640625	(30.8s)
epoch: 72	train loss: 18545.609375	(31.0s)
epoch: 73	train loss: 18628.591796875	(30.9s)
epoch: 74	train loss: 18163.634765625	(31.0s)
epoch: 75	train loss: 17965.189453125	(30.8s)
epoch: 76	train loss: 18108.662109375	(30.9s)
epoch: 77	train loss: 18555.671875	(30.8s)
epoch: 78	train loss: 18788.763671875	(30.8s)
epoch: 79	train loss: 18192.240234375	(30.8s)
epoch: 80	train loss: 18887.09765625	(31.0s)
epoch: 81	train loss: 18188.42578125	(30.7s)
epoch: 82	train loss: 17856.955078125	(31.0s)
epoch: 83	train loss: 17970.501953125	(31.0s)
epoch: 84	train loss: 18210.31640625	(30.8s)
epoch: 85	train loss: 18544.080078125	(30.7s)
epoch: 86	train loss: 18842.974609375	(30.9s)
epoch: 87	train loss: 18888.748046875	(31.2s)
epoch: 88	train loss: 19583.7421875	(30.8s)
epoch: 89	train loss: 18858.62109375	(30.8s)
epoch: 90	train loss: 19033.08984375	(30.9s)
epoch: 91	train loss: 19118.228515625	(31.0s)
epoch: 92	train loss: 19249.24609375	(30.8s)
epoch: 93	train loss: 19076.8125	(30.8s)
epoch: 94	train loss: 18909.5390625	(30.8s)
epoch: 95	train loss: 18387.154296875	(30.9s)
epoch: 96	train loss: 18052.921875	(30.8s)
epoch: 97	train loss: 18009.310546875	(30.6s)
epoch: 98	train loss: 18367.2578125	(30.6s)
epoch: 99	train loss: 17930.197265625	(30.8s)
Evaluating model on 200 episodes
0.0021998540391602243
0.0008427368358332807
0.00046894384431652727
0.0016708414249088882
0.0009825858255175666
0.0005453420938768735
0.000739190960302949
0.0011459410125098657
0.0015896104159764946
0.0022508976897890015
0.0034217126230942085
0.002760966442292556
0.0008535754722498285
0.0021032829285104527
0.0018005206734718134
0.00138499948264904
0.001064503687606096
0.0012649232496187324
0.0015100408830524732
0.0010474881882228974
0.002002681139128981
0.0027190819078400223
0.0023885768189627144
0.00228486641208292
0.004877339059021324
0.0010742233949713408
0.0012900163371038313
0.001149458162087415
0.001821419841144234
0.00287961235808325
0.0004197256806946825
0.001064468891127035
Solved 32/200 episodes
0.0002680971388760376
Evaluated model in 24.7 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-1
Round 2
Generated trajectories in 103.4 seconds
epoch: 0	train loss: 37733.609375	(31.0s)
epoch: 1	train loss: 32640.171875	(30.7s)
epoch: 2	train loss: 30316.544921875	(30.9s)
epoch: 3	train loss: 29197.576171875	(31.1s)
epoch: 4	train loss: 27883.212890625	(30.9s)
epoch: 5	train loss: 27022.939453125	(30.9s)
epoch: 6	train loss: 26354.01171875	(30.9s)
epoch: 7	train loss: 25926.37109375	(31.0s)
epoch: 8	train loss: 25300.552734375	(30.9s)
epoch: 9	train loss: 24981.87890625	(30.9s)
epoch: 10	train loss: 24544.443359375	(30.9s)
epoch: 11	train loss: 24166.759765625	(31.0s)
epoch: 12	train loss: 23897.74609375	(31.0s)
epoch: 13	train loss: 23647.123046875	(31.0s)
epoch: 14	train loss: 23271.8359375	(31.0s)
epoch: 15	train loss: 23180.056640625	(31.1s)
epoch: 16	train loss: 22693.107421875	(31.0s)
epoch: 17	train loss: 22613.40625	(31.0s)
epoch: 18	train loss: 22315.43359375	(31.1s)
epoch: 19	train loss: 22305.685546875	(31.0s)
epoch: 20	train loss: 22321.224609375	(31.0s)
epoch: 21	train loss: 22323.96484375	(30.8s)
epoch: 22	train loss: 22114.939453125	(30.9s)
epoch: 23	train loss: 21995.505859375	(31.0s)
epoch: 24	train loss: 21747.115234375	(30.9s)
epoch: 25	train loss: 21259.53515625	(30.9s)
epoch: 26	train loss: 21141.7734375	(30.8s)
epoch: 27	train loss: 21058.53125	(31.0s)
epoch: 28	train loss: 20610.650390625	(30.9s)
epoch: 29	train loss: 20505.4453125	(30.9s)
epoch: 30	train loss: 20244.015625	(30.9s)
epoch: 31	train loss: 20037.4765625	(30.9s)
epoch: 32	train loss: 19887.01953125	(30.9s)
epoch: 33	train loss: 20388.62109375	(30.9s)
epoch: 34	train loss: 20132.658203125	(30.9s)
epoch: 35	train loss: 19845.359375	(31.0s)
epoch: 36	train loss: 19784.501953125	(30.9s)
epoch: 37	train loss: 19988.650390625	(30.9s)
epoch: 38	train loss: 20128.583984375	(30.9s)
epoch: 39	train loss: 19632.267578125	(30.9s)
epoch: 40	train loss: 19714.83984375	(30.8s)
epoch: 41	train loss: 19359.197265625	(30.7s)
epoch: 42	train loss: 18889.462890625	(30.8s)
epoch: 43	train loss: 18753.890625	(30.7s)
epoch: 44	train loss: 19021.0625	(30.8s)
epoch: 45	train loss: 18094.185546875	(30.9s)
epoch: 46	train loss: 17785.234375	(30.8s)
epoch: 47	train loss: 17652.185546875	(30.9s)
epoch: 48	train loss: 17897.64453125	(30.8s)
epoch: 49	train loss: 17386.720703125	(30.9s)
epoch: 50	train loss: 17433.64453125	(30.9s)
epoch: 51	train loss: 17449.00390625	(30.8s)
epoch: 52	train loss: 17131.361328125	(30.9s)
epoch: 53	train loss: 16997.140625	(30.8s)
epoch: 54	train loss: 16773.171875	(31.0s)
epoch: 55	train loss: 16678.45703125	(31.0s)
epoch: 56	train loss: 17952.404296875	(30.9s)
epoch: 57	train loss: 17218.642578125	(30.9s)
epoch: 58	train loss: 16979.92578125	(30.8s)
epoch: 59	train loss: 16546.8671875	(31.0s)
epoch: 60	train loss: 16643.701171875	(31.0s)
epoch: 61	train loss: 16721.341796875	(30.8s)
epoch: 62	train loss: 16838.8125	(30.8s)
epoch: 63	train loss: 16526.056640625	(30.7s)
epoch: 64	train loss: 16500.76171875	(30.8s)
epoch: 65	train loss: 16426.84765625	(30.8s)
epoch: 66	train loss: 16443.6875	(30.8s)
epoch: 67	train loss: 16332.69140625	(30.8s)
epoch: 68	train loss: 16289.4765625	(30.7s)
epoch: 69	train loss: 16387.66015625	(30.8s)
epoch: 70	train loss: 15899.234375	(30.8s)
epoch: 71	train loss: 16011.2177734375	(30.8s)
epoch: 72	train loss: 15932.05859375	(30.8s)
epoch: 73	train loss: 15686.8388671875	(30.7s)
epoch: 74	train loss: 15416.0458984375	(30.8s)
epoch: 75	train loss: 15624.59765625	(30.9s)
epoch: 76	train loss: 15860.546875	(30.9s)
epoch: 77	train loss: 15806.6328125	(30.9s)
epoch: 78	train loss: 16054.7724609375	(30.7s)
epoch: 79	train loss: 15845.080078125	(30.8s)
epoch: 80	train loss: 15773.3818359375	(30.8s)
epoch: 81	train loss: 15797.5986328125	(30.7s)
epoch: 82	train loss: 15938.1513671875	(30.7s)
epoch: 83	train loss: 16418.20703125	(30.7s)
epoch: 84	train loss: 15852.9599609375	(30.8s)
epoch: 85	train loss: 15742.7998046875	(30.8s)
epoch: 86	train loss: 15440.640625	(30.8s)
epoch: 87	train loss: 15488.798828125	(30.9s)
epoch: 88	train loss: 16068.0390625	(30.6s)
epoch: 89	train loss: 15512.810546875	(30.7s)
epoch: 90	train loss: 14986.2978515625	(30.7s)
epoch: 91	train loss: 14829.44140625	(30.7s)
epoch: 92	train loss: 14571.828125	(30.7s)
epoch: 93	train loss: 14528.7978515625	(30.7s)
epoch: 94	train loss: 14774.984375	(30.7s)
epoch: 95	train loss: 14583.197265625	(30.7s)
epoch: 96	train loss: 15014.916015625	(30.6s)
epoch: 97	train loss: 14440.7626953125	(30.5s)
epoch: 98	train loss: 14341.27734375	(30.5s)
epoch: 99	train loss: 14308.0654296875	(30.6s)
Evaluating model on 200 episodes
0.002146867400733754
0.0011021796833827263
0.0023717128176940607
0.0013948656865977681
0.0035108829203333394
0.0015221791245494387
0.003807740405318327
0.0016493930597789586
0.0008632555522622583
0.0018776033568883577
0.0009447022572809752
0.0014576810333892353
0.006450268602444946
0.0009681225479157133
0.0010238893947745156
0.001693628710199846
0.003036913934920449
0.0009269367772503756
0.0005266395382932387
0.0007464402617188171
0.0008409025871919261
0.0010202430950647051
0.001510817336384207
0.000964806425534854
0.0020656398971498546
0.006047026003943756
0.0038636307592521636
0.0009661135416536126
0.004184230752538757
0.0029497609484678833
0.0014968688647059025
0.00029872473554432336
0.003404236415268055
0.0009487397884103385
0.0007567317152279429
0.0007009687018580735
0.0004828455372868727
Solved 37/200 episodes
0.0003526209508560517
Evaluated model in 28.1 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-2
Round 3
Generated trajectories in 102.2 seconds
epoch: 0	train loss: 40476.08203125	(30.7s)
epoch: 1	train loss: 33950.27734375	(30.8s)
epoch: 2	train loss: 31194.84765625	(30.8s)
epoch: 3	train loss: 29802.4375	(31.0s)
epoch: 4	train loss: 28363.888671875	(30.9s)
epoch: 5	train loss: 27167.814453125	(31.0s)
epoch: 6	train loss: 26309.501953125	(30.9s)
epoch: 7	train loss: 25652.92578125	(30.9s)
epoch: 8	train loss: 25927.33203125	(31.1s)
epoch: 9	train loss: 24767.734375	(31.0s)
epoch: 10	train loss: 24168.44921875	(31.0s)
epoch: 11	train loss: 23826.845703125	(31.0s)
epoch: 12	train loss: 23318.69140625	(30.9s)
epoch: 13	train loss: 23063.29296875	(30.9s)
epoch: 14	train loss: 22471.24609375	(30.9s)
epoch: 15	train loss: 22044.7578125	(31.0s)
epoch: 16	train loss: 21813.64453125	(31.0s)
epoch: 17	train loss: 22198.794921875	(30.9s)
epoch: 18	train loss: 21514.28515625	(31.0s)
epoch: 19	train loss: 21376.3203125	(30.9s)
epoch: 20	train loss: 21222.001953125	(31.0s)
epoch: 21	train loss: 20998.6015625	(31.0s)
epoch: 22	train loss: 20660.935546875	(30.9s)
epoch: 23	train loss: 20983.5546875	(31.0s)
epoch: 24	train loss: 20063.47265625	(30.9s)
epoch: 25	train loss: 19746.060546875	(30.9s)
epoch: 26	train loss: 19537.1328125	(31.0s)
epoch: 27	train loss: 19064.55859375	(30.9s)
epoch: 28	train loss: 20469.970703125	(31.0s)
epoch: 29	train loss: 18966.2109375	(30.9s)
epoch: 30	train loss: 18292.04296875	(31.0s)
epoch: 31	train loss: 18144.904296875	(30.9s)
epoch: 32	train loss: 18052.55078125	(31.0s)
epoch: 33	train loss: 18138.12890625	(31.0s)
epoch: 34	train loss: 17679.703125	(30.9s)
epoch: 35	train loss: 17643.484375	(31.0s)
epoch: 36	train loss: 17645.62109375	(30.9s)
epoch: 37	train loss: 17762.119140625	(31.0s)
epoch: 38	train loss: 17414.546875	(31.0s)
epoch: 39	train loss: 17424.794921875	(31.0s)
epoch: 40	train loss: 18046.11328125	(31.0s)
epoch: 41	train loss: 17129.873046875	(30.9s)
epoch: 42	train loss: 17041.689453125	(31.0s)
epoch: 43	train loss: 17387.341796875	(31.0s)
epoch: 44	train loss: 17550.0390625	(31.0s)
epoch: 45	train loss: 16662.927734375	(31.0s)
epoch: 46	train loss: 16708.13671875	(30.9s)
epoch: 47	train loss: 16747.49609375	(31.0s)
epoch: 48	train loss: 16763.513671875	(30.9s)
epoch: 49	train loss: 16938.12109375	(31.0s)
epoch: 50	train loss: 17194.435546875	(31.0s)
epoch: 51	train loss: 16390.068359375	(30.9s)
epoch: 52	train loss: 16442.9921875	(30.9s)
epoch: 53	train loss: 16454.017578125	(30.8s)
epoch: 54	train loss: 15917.7158203125	(31.0s)
epoch: 55	train loss: 15588.6748046875	(31.0s)
epoch: 56	train loss: 15656.359375	(31.0s)
epoch: 57	train loss: 15085.3916015625	(30.9s)
epoch: 58	train loss: 15020.7861328125	(30.9s)
epoch: 59	train loss: 14876.3017578125	(31.0s)
epoch: 60	train loss: 14860.7626953125	(30.9s)
epoch: 61	train loss: 14857.478515625	(30.9s)
epoch: 62	train loss: 14572.1337890625	(31.0s)
epoch: 63	train loss: 14874.6611328125	(30.9s)
epoch: 64	train loss: 15516.62109375	(31.0s)
epoch: 65	train loss: 14596.8642578125	(31.0s)
epoch: 66	train loss: 14657.658203125	(31.0s)
epoch: 67	train loss: 14293.3447265625	(31.0s)
epoch: 68	train loss: 14042.6005859375	(30.9s)
epoch: 69	train loss: 13898.6142578125	(30.9s)
epoch: 70	train loss: 13830.54296875	(30.9s)
epoch: 71	train loss: 13806.9677734375	(31.0s)
epoch: 72	train loss: 13801.1455078125	(31.0s)
epoch: 73	train loss: 13851.8037109375	(30.9s)
epoch: 74	train loss: 13295.7666015625	(30.9s)
epoch: 75	train loss: 13779.57421875	(30.9s)
epoch: 76	train loss: 13850.9423828125	(31.0s)
epoch: 77	train loss: 13462.654296875	(31.0s)
epoch: 78	train loss: 13530.2197265625	(31.0s)
epoch: 79	train loss: 13633.3525390625	(31.0s)
epoch: 80	train loss: 13447.8359375	(30.9s)
epoch: 81	train loss: 13616.0	(31.0s)
epoch: 82	train loss: 13272.328125	(31.0s)
epoch: 83	train loss: 13255.3779296875	(30.9s)
epoch: 84	train loss: 13285.0830078125	(31.0s)
epoch: 85	train loss: 13075.5322265625	(30.9s)
epoch: 86	train loss: 13595.283203125	(31.0s)
epoch: 87	train loss: 12861.2177734375	(30.9s)
epoch: 88	train loss: 12978.189453125	(31.0s)
epoch: 89	train loss: 12822.966796875	(31.0s)
epoch: 90	train loss: 12547.015625	(30.9s)
epoch: 91	train loss: 12736.6552734375	(31.0s)
epoch: 92	train loss: 12726.904296875	(30.9s)
epoch: 93	train loss: 12519.5556640625	(31.1s)
epoch: 94	train loss: 12560.6240234375	(31.0s)
epoch: 95	train loss: 13725.9990234375	(30.8s)
epoch: 96	train loss: 12671.2138671875	(30.8s)
epoch: 97	train loss: 12718.1396484375	(30.7s)
epoch: 98	train loss: 12482.3564453125	(30.9s)
epoch: 99	train loss: 12799.580078125	(31.0s)
Evaluating model on 200 episodes
0.0019678352065966463
0.0008607546889314628
0.0032458375014054277
0.0015591747396683786
0.0013765712097996254
0.0025286219876891535
0.001718529402569402
0.0025456934909016127
0.0024370121653191747
0.0038630025228485465
0.0027423398423707113
0.004085217350317786
0.001705964236559036
0.0009726601850107888
0.0011434280677349307
0.0016852397238835692
0.0009250887136052673
0.0018188750773333595
0.0031962696384118102
0.005047216555794876
0.002534875020501204
0.002910019510939795
0.0003462646554908133
0.0005524470307136653
0.0011669704953318513
0.0022932070694098783
0.0015524162008659914
0.000890694915142376
0.002225646503315407
0.006689445706017848
0.0012355463222775143
0.0021428944576958506
0.0013181789032387313
0.002937507573127126
0.0012437605546438135
0.002330093673663214
0.0015363398881163449
0.0015894653300744142
0.001112477157508501
0.0015927821426885203
0.001402884155443947
0.0007851445199256497
0.0013994601809827144
0.0013152360957714596
0.00044146942474815825
0.0014006408397108316
Solved 46/200 episodes
0.0004518560031704859
Evaluated model in 31.5 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-3
Round 4
Generated trajectories in 101.9 seconds
epoch: 0	train loss: 38229.88671875	(30.7s)
epoch: 1	train loss: 31767.166015625	(30.6s)
epoch: 2	train loss: 29011.2734375	(30.7s)
epoch: 3	train loss: 27104.46875	(30.6s)
epoch: 4	train loss: 25985.486328125	(30.7s)
epoch: 5	train loss: 24737.486328125	(30.7s)
epoch: 6	train loss: 23962.2421875	(30.7s)
epoch: 7	train loss: 23182.986328125	(30.7s)
epoch: 8	train loss: 23389.9140625	(30.6s)
epoch: 9	train loss: 22301.79296875	(30.8s)
epoch: 10	train loss: 22101.205078125	(30.7s)
epoch: 11	train loss: 22428.921875	(30.7s)
epoch: 12	train loss: 21426.45703125	(30.7s)
epoch: 13	train loss: 20818.0703125	(30.7s)
epoch: 14	train loss: 20411.224609375	(30.7s)
epoch: 15	train loss: 19829.162109375	(30.7s)
epoch: 16	train loss: 19611.359375	(30.7s)
epoch: 17	train loss: 19217.109375	(30.7s)
epoch: 18	train loss: 18904.14453125	(30.6s)
epoch: 19	train loss: 18825.080078125	(30.7s)
epoch: 20	train loss: 18265.02734375	(30.7s)
epoch: 21	train loss: 17979.05859375	(30.7s)
epoch: 22	train loss: 17914.958984375	(30.7s)
epoch: 23	train loss: 18441.044921875	(30.6s)
epoch: 24	train loss: 17956.45703125	(30.7s)
epoch: 25	train loss: 17648.306640625	(30.7s)
epoch: 26	train loss: 17524.630859375	(30.7s)
epoch: 27	train loss: 17433.3671875	(30.7s)
epoch: 28	train loss: 17149.310546875	(30.6s)
epoch: 29	train loss: 17041.279296875	(30.7s)
epoch: 30	train loss: 17160.658203125	(30.8s)
epoch: 31	train loss: 16756.056640625	(30.7s)
epoch: 32	train loss: 16498.849609375	(30.7s)
epoch: 33	train loss: 16254.38671875	(30.7s)
epoch: 34	train loss: 16096.876953125	(30.7s)
epoch: 35	train loss: 16094.53125	(30.8s)
epoch: 36	train loss: 15949.3876953125	(30.7s)
epoch: 37	train loss: 15738.1953125	(30.7s)
epoch: 38	train loss: 15169.412109375	(30.7s)
epoch: 39	train loss: 15095.8232421875	(30.7s)
epoch: 40	train loss: 15031.716796875	(30.7s)
epoch: 41	train loss: 15131.640625	(30.7s)
epoch: 42	train loss: 14897.2158203125	(30.7s)
epoch: 43	train loss: 14641.2392578125	(30.6s)
epoch: 44	train loss: 14151.1865234375	(30.7s)
epoch: 45	train loss: 14145.44140625	(30.8s)
epoch: 46	train loss: 13994.640625	(30.7s)
epoch: 47	train loss: 13864.8369140625	(30.8s)
epoch: 48	train loss: 13996.0068359375	(30.6s)
epoch: 49	train loss: 13815.0625	(30.7s)
epoch: 50	train loss: 13807.4970703125	(30.7s)
epoch: 51	train loss: 13910.0732421875	(30.7s)
epoch: 52	train loss: 13590.361328125	(30.8s)
epoch: 53	train loss: 13861.658203125	(30.6s)
epoch: 54	train loss: 13851.9443359375	(30.7s)
epoch: 55	train loss: 13780.5419921875	(30.7s)
epoch: 56	train loss: 13090.173828125	(30.7s)
epoch: 57	train loss: 13221.9755859375	(30.7s)
epoch: 58	train loss: 13096.5771484375	(30.6s)
epoch: 59	train loss: 12988.044921875	(30.7s)
epoch: 60	train loss: 12877.5634765625	(30.8s)
epoch: 61	train loss: 12847.037109375	(30.7s)
epoch: 62	train loss: 13323.21484375	(30.8s)
epoch: 63	train loss: 12708.19140625	(30.6s)
epoch: 64	train loss: 13133.5390625	(30.7s)
epoch: 65	train loss: 12545.140625	(30.7s)
epoch: 66	train loss: 12442.58984375	(30.7s)
epoch: 67	train loss: 12298.349609375	(30.7s)
epoch: 68	train loss: 13194.6865234375	(30.7s)
epoch: 69	train loss: 12951.677734375	(30.7s)
epoch: 70	train loss: 12616.482421875	(30.7s)
epoch: 71	train loss: 12313.5693359375	(30.7s)
epoch: 72	train loss: 12328.177734375	(30.8s)
epoch: 73	train loss: 12291.2265625	(30.6s)
epoch: 74	train loss: 12121.4619140625	(30.7s)
epoch: 75	train loss: 12246.2392578125	(30.7s)
epoch: 76	train loss: 12427.296875	(30.7s)
epoch: 77	train loss: 12233.0673828125	(30.8s)
epoch: 78	train loss: 11817.591796875	(30.6s)
epoch: 79	train loss: 11803.3369140625	(30.7s)
epoch: 80	train loss: 12218.4912109375	(30.7s)
epoch: 81	train loss: 12013.69921875	(30.7s)
epoch: 82	train loss: 11880.3564453125	(30.7s)
epoch: 83	train loss: 11755.4296875	(30.6s)
epoch: 84	train loss: 12165.7001953125	(30.8s)
epoch: 85	train loss: 11859.4990234375	(30.7s)
epoch: 86	train loss: 11521.310546875	(30.7s)
epoch: 87	train loss: 12071.181640625	(30.8s)
epoch: 88	train loss: 11491.8427734375	(30.6s)
epoch: 89	train loss: 11440.525390625	(30.7s)
epoch: 90	train loss: 11434.0908203125	(30.8s)
epoch: 91	train loss: 11247.505859375	(30.7s)
epoch: 92	train loss: 11317.583984375	(30.7s)
epoch: 93	train loss: 12992.52734375	(30.5s)
epoch: 94	train loss: 11328.501953125	(30.6s)
epoch: 95	train loss: 12067.5234375	(30.6s)
epoch: 96	train loss: 11417.9453125	(30.6s)
epoch: 97	train loss: 11332.5517578125	(30.7s)
epoch: 98	train loss: 11303.5	(30.6s)
epoch: 99	train loss: 11595.3779296875	(30.7s)
Evaluating model on 200 episodes
0.0014358156358040023
0.00043225754325249
0.001050360969851941
0.0017445117482566275
0.0011313647275008926
0.0012252106909207733
0.0011417356945457868
0.000843803834868595
0.0005796928605832363
0.0007964401946602655
0.001070882067324419
0.0019611480664545006
0.0010993755697979526
0.001349219153780723
0.0021769775355803883
0.0007116977907240984
0.000684402593227181
0.0022606862421525875
0.0027842431004501123
0.0020826264192103684
0.001748380026583618
0.0019306707420452898
0.0026973842335185814
0.0005719428099837387
0.0018613194057252257
0.0007591736306494567
0.0006826882866638092
0.0008563668650042798
0.000629402237245813
0.0017861469939816743
0.0024718179016157854
0.0009909068321576342
0.00124060925683058
0.0005593216737906914
0.0009170262674160767
0.0016234895156230778
0.0011578567400647444
0.0036307410079773816
0.0011042296367425781
0.0017906573080431877
0.0018884023738792166
0.002537546008170466
0.0020050570435289827
0.0007381116977610093
0.0014659539596965153
0.0015748962953997154
0.0012788814159908465
Solved 47/200 episodes
0.0003353071630251846
Evaluated model in 29.3 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-4
Round 5
Generated trajectories in 101.1 seconds
epoch: 0	train loss: 35992.2109375	(30.5s)
epoch: 1	train loss: 30166.111328125	(30.6s)
epoch: 2	train loss: 27138.7109375	(30.6s)
epoch: 3	train loss: 26051.494140625	(30.8s)
epoch: 4	train loss: 24490.953125	(30.7s)
epoch: 5	train loss: 23189.748046875	(30.8s)
epoch: 6	train loss: 22428.1640625	(30.7s)
epoch: 7	train loss: 21886.29296875	(30.7s)
epoch: 8	train loss: 21640.62109375	(30.7s)
epoch: 9	train loss: 21410.23828125	(30.7s)
epoch: 10	train loss: 20464.5703125	(31.5s)
epoch: 11	train loss: 20026.82421875	(30.8s)
epoch: 12	train loss: 19578.66015625	(30.7s)
epoch: 13	train loss: 19887.0078125	(30.7s)
epoch: 14	train loss: 19020.76171875	(30.7s)
epoch: 15	train loss: 18536.392578125	(30.8s)
epoch: 16	train loss: 18273.884765625	(30.8s)
epoch: 17	train loss: 17591.888671875	(30.7s)
epoch: 18	train loss: 17409.98828125	(30.7s)
epoch: 19	train loss: 17250.3671875	(30.7s)
epoch: 20	train loss: 16727.978515625	(30.8s)
epoch: 21	train loss: 16941.51171875	(30.7s)
epoch: 22	train loss: 16927.8984375	(30.8s)
epoch: 23	train loss: 16227.8583984375	(31.1s)
epoch: 24	train loss: 16528.6953125	(30.7s)
epoch: 25	train loss: 16208.77734375	(30.9s)
epoch: 26	train loss: 16951.333984375	(30.7s)
epoch: 27	train loss: 16620.38671875	(30.7s)
epoch: 28	train loss: 16255.1630859375	(30.8s)
epoch: 29	train loss: 15780.8798828125	(30.7s)
epoch: 30	train loss: 15266.169921875	(30.8s)
epoch: 31	train loss: 14870.83984375	(30.7s)
epoch: 32	train loss: 14917.974609375	(30.7s)
epoch: 33	train loss: 14538.43359375	(30.8s)
epoch: 34	train loss: 14946.513671875	(30.6s)
epoch: 35	train loss: 14448.3681640625	(30.7s)
epoch: 36	train loss: 14544.919921875	(30.7s)
epoch: 37	train loss: 14407.85546875	(30.8s)
epoch: 38	train loss: 14528.185546875	(30.8s)
epoch: 39	train loss: 14385.7021484375	(30.7s)
epoch: 40	train loss: 13644.0908203125	(30.8s)
epoch: 41	train loss: 13394.646484375	(30.7s)
epoch: 42	train loss: 14418.087890625	(30.8s)
epoch: 43	train loss: 13283.525390625	(30.7s)
epoch: 44	train loss: 13804.0576171875	(30.7s)
epoch: 45	train loss: 13382.6083984375	(30.7s)
epoch: 46	train loss: 13577.3173828125	(30.7s)
epoch: 47	train loss: 13240.7919921875	(30.8s)
epoch: 48	train loss: 12931.5751953125	(30.7s)
epoch: 49	train loss: 12994.6376953125	(30.8s)
epoch: 50	train loss: 13011.3994140625	(30.7s)
epoch: 51	train loss: 13071.2021484375	(30.6s)
epoch: 52	train loss: 12930.0107421875	(30.8s)
epoch: 53	train loss: 12828.5712890625	(30.7s)
epoch: 54	train loss: 12561.3017578125	(30.8s)
epoch: 55	train loss: 13178.8466796875	(30.7s)
epoch: 56	train loss: 13060.40625	(30.7s)
epoch: 57	train loss: 13321.8994140625	(30.7s)
epoch: 58	train loss: 12460.30078125	(30.7s)
epoch: 59	train loss: 12226.9560546875	(30.7s)
epoch: 60	train loss: 12388.931640625	(30.8s)
epoch: 61	train loss: 12343.650390625	(30.7s)
epoch: 62	train loss: 12145.55859375	(30.7s)
epoch: 63	train loss: 12965.9892578125	(30.7s)
epoch: 64	train loss: 12385.4921875	(30.8s)
epoch: 65	train loss: 12177.845703125	(30.7s)
epoch: 66	train loss: 11800.537109375	(30.7s)
epoch: 67	train loss: 12093.3134765625	(30.7s)
epoch: 68	train loss: 11707.5576171875	(30.7s)
epoch: 69	train loss: 12031.640625	(30.8s)
epoch: 70	train loss: 12928.8125	(30.8s)
epoch: 71	train loss: 11715.732421875	(30.7s)
epoch: 72	train loss: 12158.9033203125	(30.8s)
epoch: 73	train loss: 11953.412109375	(30.7s)
epoch: 74	train loss: 11483.88671875	(30.8s)
epoch: 75	train loss: 11767.3330078125	(30.8s)
epoch: 76	train loss: 11497.46875	(30.7s)
epoch: 77	train loss: 11053.6982421875	(30.7s)
epoch: 78	train loss: 11098.5166015625	(30.7s)
epoch: 79	train loss: 11032.5224609375	(30.8s)
epoch: 80	train loss: 11186.193359375	(30.7s)
epoch: 81	train loss: 11039.599609375	(30.8s)
epoch: 82	train loss: 11192.9658203125	(30.7s)
epoch: 83	train loss: 10846.78515625	(30.7s)
epoch: 84	train loss: 10893.904296875	(30.8s)
epoch: 85	train loss: 11233.453125	(30.7s)
epoch: 86	train loss: 10829.1396484375	(30.8s)
epoch: 87	train loss: 10341.41796875	(30.8s)
epoch: 88	train loss: 10308.65625	(30.7s)
epoch: 89	train loss: 10429.5859375	(30.7s)
epoch: 90	train loss: 10612.90234375	(30.7s)
epoch: 91	train loss: 10286.5283203125	(30.7s)
epoch: 92	train loss: 11722.763671875	(30.6s)
epoch: 93	train loss: 10542.537109375	(30.5s)
epoch: 94	train loss: 10223.9755859375	(30.6s)
epoch: 95	train loss: 12330.9736328125	(30.5s)
epoch: 96	train loss: 10538.451171875	(30.8s)
epoch: 97	train loss: 10324.3662109375	(30.7s)
epoch: 98	train loss: 10542.1953125	(30.7s)
epoch: 99	train loss: 9980.5546875	(30.8s)
Evaluating model on 200 episodes
0.0008904644934003814
0.0006943223556845624
0.0025940549230047813
0.0006869646174035391
0.0010077863189508207
0.0014599066809751094
0.0009173716139533401
0.001322739481111057
0.0006579269181450152
0.002700595797380499
0.0017539898346347566
0.000969302662789622
0.001351885686441771
0.0021568150129626573
0.0009732323784798306
0.0009850635797192808
0.0015165428428897737
0.0024699222849449143
0.0021882699089474044
0.0010417810441979852
0.0008073671063660489
0.0012252853775862604
0.0010318285328269536
0.001580280607763248
0.0010581449232631712
0.0023149208864197135
0.0009602083339884232
0.0015668951631856284
0.0008416149353757242
0.0014260521035924154
0.0005689179088221863
0.0023774445289745927
0.0028037006656328836
0.0018035537123068934
0.0017246122952201404
0.0017216819964232855
0.002158514881254329
0.0008195020269149131
0.0008277100198690525
0.0017042400625844796
0.0007078388721724464
0.0005761359815161475
0.0024016410929367077
0.0030948675208492204
0.0006112524201853375
0.0014307902739447956
0.0017072261155893405
0.0013859251317066641
0.002376785962951544
0.0018426115265659368
0.0033559430703462567
0.0021991583134877146
Solved 52/200 episodes
0.00039675795393319786
Evaluated model in 32.7 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-5
Round 6
Generated trajectories in 102.4 seconds
epoch: 0	train loss: 34768.4921875	(30.5s)
epoch: 1	train loss: 28819.134765625	(30.5s)
epoch: 2	train loss: 25828.06640625	(30.7s)
epoch: 3	train loss: 24069.759765625	(30.7s)
epoch: 4	train loss: 22760.34375	(30.7s)
epoch: 5	train loss: 21781.533203125	(30.7s)
epoch: 6	train loss: 20880.513671875	(30.7s)
epoch: 7	train loss: 20221.484375	(30.7s)
epoch: 8	train loss: 19833.6015625	(30.8s)
epoch: 9	train loss: 19559.140625	(30.7s)
epoch: 10	train loss: 18896.6796875	(30.7s)
epoch: 11	train loss: 18683.849609375	(30.7s)
epoch: 12	train loss: 18401.9609375	(30.7s)
epoch: 13	train loss: 18164.673828125	(30.8s)
epoch: 14	train loss: 17629.220703125	(30.7s)
epoch: 15	train loss: 17132.38671875	(30.7s)
epoch: 16	train loss: 17875.5234375	(30.7s)
epoch: 17	train loss: 17368.341796875	(30.7s)
epoch: 18	train loss: 17256.330078125	(30.8s)
epoch: 19	train loss: 17085.87109375	(30.7s)
epoch: 20	train loss: 16521.95703125	(30.7s)
epoch: 21	train loss: 16165.3408203125	(30.7s)
epoch: 22	train loss: 15530.21875	(30.7s)
epoch: 23	train loss: 15331.3212890625	(30.8s)
epoch: 24	train loss: 15187.68359375	(30.7s)
epoch: 25	train loss: 15018.1640625	(30.7s)
epoch: 26	train loss: 15242.5087890625	(30.7s)
epoch: 27	train loss: 14747.3681640625	(30.8s)
epoch: 28	train loss: 14572.400390625	(30.8s)
epoch: 29	train loss: 14167.4609375	(30.8s)
epoch: 30	train loss: 15165.9755859375	(30.7s)
epoch: 31	train loss: 14024.01953125	(30.7s)
epoch: 32	train loss: 13766.3720703125	(30.8s)
epoch: 33	train loss: 13417.3408203125	(30.8s)
epoch: 34	train loss: 13216.302734375	(30.7s)
epoch: 35	train loss: 13541.00390625	(30.8s)
epoch: 36	train loss: 13460.0634765625	(30.7s)
epoch: 37	train loss: 13805.3515625	(30.8s)
epoch: 38	train loss: 13533.0615234375	(30.8s)
epoch: 39	train loss: 13083.681640625	(30.7s)
epoch: 40	train loss: 13534.884765625	(30.7s)
epoch: 41	train loss: 12926.5341796875	(30.7s)
epoch: 42	train loss: 12516.16796875	(30.7s)
epoch: 43	train loss: 12447.0986328125	(30.8s)
epoch: 44	train loss: 12902.5322265625	(30.7s)
epoch: 45	train loss: 12591.625	(30.8s)
epoch: 46	train loss: 12440.1826171875	(30.7s)
epoch: 47	train loss: 12273.1171875	(30.8s)
epoch: 48	train loss: 12783.4521484375	(30.7s)
epoch: 49	train loss: 11699.673828125	(30.7s)
epoch: 50	train loss: 12055.787109375	(30.7s)
epoch: 51	train loss: 11993.412109375	(30.7s)
epoch: 52	train loss: 11853.982421875	(30.7s)
epoch: 53	train loss: 11601.041015625	(30.8s)
epoch: 54	train loss: 13241.8056640625	(30.7s)
epoch: 55	train loss: 12087.841796875	(30.8s)
epoch: 56	train loss: 11765.7216796875	(30.7s)
epoch: 57	train loss: 11502.623046875	(30.8s)
epoch: 58	train loss: 11519.0625	(30.8s)
epoch: 59	train loss: 11420.333984375	(30.7s)
epoch: 60	train loss: 11022.0087890625	(30.7s)
epoch: 61	train loss: 11195.2021484375	(30.7s)
epoch: 62	train loss: 11431.1337890625	(30.8s)
epoch: 63	train loss: 10778.65625	(30.8s)
epoch: 64	train loss: 11181.2236328125	(30.7s)
epoch: 65	train loss: 10494.505859375	(30.8s)
epoch: 66	train loss: 10580.3974609375	(30.7s)
epoch: 67	train loss: 10413.24609375	(30.8s)
epoch: 68	train loss: 10201.65625	(30.8s)
epoch: 69	train loss: 10415.4150390625	(30.7s)
epoch: 70	train loss: 10720.76953125	(30.7s)
epoch: 71	train loss: 10452.4931640625	(30.7s)
epoch: 72	train loss: 10350.9169921875	(30.7s)
epoch: 73	train loss: 10331.7724609375	(30.8s)
epoch: 74	train loss: 10072.7724609375	(30.7s)
epoch: 75	train loss: 10364.8212890625	(30.7s)
epoch: 76	train loss: 11230.791015625	(30.7s)
epoch: 77	train loss: 9972.5830078125	(30.7s)
epoch: 78	train loss: 10055.0048828125	(30.8s)
epoch: 79	train loss: 10594.55859375	(30.6s)
epoch: 80	train loss: 9924.0576171875	(30.7s)
epoch: 81	train loss: 9911.90625	(30.7s)
epoch: 82	train loss: 9709.5263671875	(30.7s)
epoch: 83	train loss: 9631.05859375	(30.8s)
epoch: 84	train loss: 9763.5146484375	(30.7s)
epoch: 85	train loss: 9804.255859375	(30.8s)
epoch: 86	train loss: 9537.9521484375	(30.7s)
epoch: 87	train loss: 9594.7890625	(30.7s)
epoch: 88	train loss: 9727.90625	(30.8s)
epoch: 89	train loss: 9337.4541015625	(30.7s)
epoch: 90	train loss: 9479.58984375	(30.7s)
epoch: 91	train loss: 9481.037109375	(30.5s)
epoch: 92	train loss: 9052.416015625	(30.6s)
epoch: 93	train loss: 9396.21484375	(30.6s)
epoch: 94	train loss: 9133.263671875	(30.6s)
epoch: 95	train loss: 9574.373046875	(30.8s)
epoch: 96	train loss: 8916.25390625	(30.7s)
epoch: 97	train loss: 9317.3349609375	(30.8s)
epoch: 98	train loss: 8679.3408203125	(30.8s)
epoch: 99	train loss: 8922.0615234375	(30.8s)
Evaluating model on 200 episodes
0.002172328441095791
0.0007021919736871496
0.0009349080064566806
0.0022522160224980325
0.0013447516927184411
0.0015081167355674551
0.001697325671557337
0.0017531695352359252
0.0017537434857028227
0.001866991473694465
0.00170644520683429
0.001503145627234167
0.0008311910769407405
0.0032193604318308646
0.0017633524374105036
0.0014654397588553063
0.0009636486768080242
0.003748128139276934
0.0012578237626505804
0.004210222580392535
0.0019964845385402443
0.0011960823839427365
0.002158753072661865
0.0013221472509030718
0.0009723589209897909
0.0015098748214465256
0.002680214115470234
0.001722944764727193
0.002077923615514818
0.005510546839407955
0.0009066789686282087
0.0007465034286724404
0.0013557043403125135
0.0011513936677312647
0.0011324627292410964
0.002081756735980675
0.0009491177645811279
0.0029303680785233155
0.0012953367847027646
0.0028105740706310966
0.0009800180014281068
0.0009399520749866497
0.001628506145789288
0.0013042968375057999
0.002489194434019737
0.0011757138495643933
0.0015071466596881774
0.0013686263555428013
0.0025759254103832063
0.0012897423025256453
0.0010598071530694142
0.0029118719655040573
0.0018416461758673764
0.0008672904048580676
0.0023869482053366178
0.0013465948302837205
0.002630692378261301
0.0018230656504682782
0.0019500320298296476
0.001082830730592832
0.0016139612242113798
0.001902565557975322
0.002344172377119181
0.002513224462745711
0.0009442477512493497
0.00036363536924000073
0.0008021945104701445
0.0016845005585411047
0.0010102176531053492
0.002741333263905042
0.001074283238267526
0.0015555969848719542
0.0010936413976499655
Solved 73/200 episodes
0.0006298260178695707
Evaluated model in 29.5 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-6
Round 7
Generated trajectories in 102.7 seconds
epoch: 0	train loss: 34449.96875	(30.8s)
epoch: 1	train loss: 27302.716796875	(30.9s)
epoch: 2	train loss: 24415.34765625	(31.0s)
epoch: 3	train loss: 22605.857421875	(31.1s)
epoch: 4	train loss: 21292.990234375	(31.1s)
epoch: 5	train loss: 20530.923828125	(31.0s)
epoch: 6	train loss: 20068.560546875	(31.1s)
epoch: 7	train loss: 19458.42578125	(31.1s)
epoch: 8	train loss: 18796.453125	(31.1s)
epoch: 9	train loss: 18552.96875	(31.1s)
epoch: 10	train loss: 17747.23046875	(31.0s)
epoch: 11	train loss: 17745.42578125	(31.1s)
epoch: 12	train loss: 17242.5859375	(31.0s)
epoch: 13	train loss: 16631.287109375	(31.1s)
epoch: 14	train loss: 16375.921875	(31.1s)
epoch: 15	train loss: 16250.181640625	(31.0s)
epoch: 16	train loss: 16181.931640625	(31.1s)
epoch: 17	train loss: 15805.0205078125	(31.0s)
epoch: 18	train loss: 15372.240234375	(31.1s)
epoch: 19	train loss: 15576.6865234375	(31.1s)
epoch: 20	train loss: 15004.29296875	(31.1s)
epoch: 21	train loss: 15147.546875	(31.1s)
epoch: 22	train loss: 14808.48828125	(31.0s)
epoch: 23	train loss: 14761.6767578125	(31.2s)
epoch: 24	train loss: 15048.251953125	(31.1s)
epoch: 25	train loss: 14999.7705078125	(31.1s)
epoch: 26	train loss: 14241.6923828125	(31.1s)
epoch: 27	train loss: 14424.61328125	(31.0s)
epoch: 28	train loss: 14024.900390625	(31.2s)
epoch: 29	train loss: 13559.9365234375	(31.1s)
epoch: 30	train loss: 14291.443359375	(31.1s)
epoch: 31	train loss: 13779.8486328125	(31.1s)
epoch: 32	train loss: 13771.755859375	(31.0s)
epoch: 33	train loss: 13055.2705078125	(31.2s)
epoch: 34	train loss: 13027.18359375	(31.1s)
epoch: 35	train loss: 13706.9443359375	(31.1s)
epoch: 36	train loss: 12984.5859375	(31.1s)
epoch: 37	train loss: 12537.6982421875	(31.0s)
epoch: 38	train loss: 12686.2841796875	(31.2s)
epoch: 39	train loss: 12295.509765625	(31.1s)
epoch: 40	train loss: 12522.0205078125	(31.1s)
epoch: 41	train loss: 12116.6328125	(31.1s)
epoch: 42	train loss: 12395.3837890625	(31.0s)
epoch: 43	train loss: 11740.279296875	(31.2s)
epoch: 44	train loss: 11776.26171875	(31.1s)
epoch: 45	train loss: 12294.4765625	(31.1s)
epoch: 46	train loss: 11428.8935546875	(31.1s)
epoch: 47	train loss: 11251.4052734375	(31.0s)
epoch: 48	train loss: 12295.6435546875	(31.2s)
epoch: 49	train loss: 12208.3037109375	(31.1s)
epoch: 50	train loss: 11027.548828125	(31.1s)
epoch: 51	train loss: 10806.6552734375	(31.1s)
epoch: 52	train loss: 11312.6171875	(31.0s)
epoch: 53	train loss: 10682.8486328125	(31.1s)
epoch: 54	train loss: 11012.7314453125	(31.1s)
epoch: 55	train loss: 11082.72265625	(31.1s)
epoch: 56	train loss: 11306.78515625	(31.1s)
epoch: 57	train loss: 10922.35546875	(31.0s)
epoch: 58	train loss: 10939.8837890625	(31.1s)
epoch: 59	train loss: 10841.8486328125	(31.0s)
epoch: 60	train loss: 10493.9296875	(31.2s)
epoch: 61	train loss: 10586.44921875	(31.1s)
epoch: 62	train loss: 10402.5986328125	(31.0s)
epoch: 63	train loss: 10402.4912109375	(31.1s)
epoch: 64	train loss: 11547.82421875	(31.0s)
epoch: 65	train loss: 10789.681640625	(31.1s)
epoch: 66	train loss: 10723.623046875	(31.1s)
epoch: 67	train loss: 10468.23046875	(31.1s)
epoch: 68	train loss: 10371.658203125	(31.1s)
epoch: 69	train loss: 10351.564453125	(31.0s)
epoch: 70	train loss: 10321.6669921875	(31.1s)
epoch: 71	train loss: 10913.34765625	(31.1s)
epoch: 72	train loss: 10645.642578125	(31.1s)
epoch: 73	train loss: 10200.52734375	(31.1s)
epoch: 74	train loss: 10428.19921875	(31.0s)
epoch: 75	train loss: 10209.3291015625	(31.2s)
epoch: 76	train loss: 9841.6396484375	(31.1s)
epoch: 77	train loss: 9741.0615234375	(31.1s)
epoch: 78	train loss: 9722.6650390625	(31.1s)
epoch: 79	train loss: 10348.087890625	(31.0s)
epoch: 80	train loss: 11036.2177734375	(31.1s)
epoch: 81	train loss: 10993.791015625	(31.1s)
epoch: 82	train loss: 10662.748046875	(31.1s)
epoch: 83	train loss: 9445.658203125	(31.1s)
epoch: 84	train loss: 9377.0048828125	(31.0s)
epoch: 85	train loss: 10011.1025390625	(31.2s)
epoch: 86	train loss: 9756.0283203125	(31.1s)
epoch: 87	train loss: 9780.294921875	(31.1s)
epoch: 88	train loss: 9443.890625	(31.1s)
epoch: 89	train loss: 9644.4931640625	(31.0s)
epoch: 90	train loss: 9193.53515625	(31.0s)
epoch: 91	train loss: 9207.6416015625	(30.9s)
epoch: 92	train loss: 8897.6064453125	(31.0s)
epoch: 93	train loss: 9440.6376953125	(31.0s)
epoch: 94	train loss: 9014.6982421875	(31.0s)
epoch: 95	train loss: 9403.1005859375	(31.1s)
epoch: 96	train loss: 9037.3251953125	(31.1s)
epoch: 97	train loss: 9126.0205078125	(31.1s)
epoch: 98	train loss: 9095.013671875	(31.1s)
epoch: 99	train loss: 9194.7021484375	(31.0s)
Evaluating model on 200 episodes
0.0014671332282887306
0.0030831547762015057
0.003858910349663347
0.001442364685620608
0.001085119701823906
0.0016996136352342244
0.0013631750383259107
0.0025498386472463607
0.001635946771582692
0.0020979323860956358
0.0007630991569991844
0.0013152181668879671
0.0019200625316252147
0.0011966556369522798
0.0018995946699457515
0.0006111554084782256
0.0035540215903893113
0.0019953436380352025
0.003432172832783015
0.0015423684872075683
0.0012564426815515617
0.002190118090116552
0.0008486301958328113
0.0024866448220564052
0.0027604475552733573
0.0015806020953907417
0.0022858239553897874
0.001580660367514664
0.0030953530222177505
0.003589121886761859
0.0020628626031490662
0.0016613243829591998
0.0011932046852182663
0.0018366250160397612
0.0019593478342964468
0.0019825464637092823
0.0011299294379164672
0.002511464864558851
0.001664181548403576
0.0008751516901005137
0.0024687380419992325
0.0016014874883694575
0.0011603894459907538
0.0028554745571454988
0.003626404847388833
0.0020150964824097173
0.0012533866157844807
0.0017487612359608257
0.002773286753532981
0.001233492128085345
0.001664232920544843
0.0017045974495886185
0.002103589377999443
0.0011809364696091507
0.0032200296331817905
0.0008890336393960752
0.002519710576702242
0.0013541851318829383
0.0014296459142185216
0.0005008925929562085
0.001509529572407094
0.0023594657832290977
Solved 62/200 episodes
0.0005911786559811335
Evaluated model in 31.4 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-7
Round 8
Generated trajectories in 102.6 seconds
epoch: 0	train loss: 32443.34375	(30.9s)
epoch: 1	train loss: 25781.83203125	(30.8s)
epoch: 2	train loss: 23054.982421875	(30.9s)
epoch: 3	train loss: 21061.705078125	(30.9s)
epoch: 4	train loss: 20107.142578125	(31.0s)
epoch: 5	train loss: 18992.23046875	(30.9s)
epoch: 6	train loss: 18400.193359375	(31.0s)
epoch: 7	train loss: 17598.96875	(31.0s)
epoch: 8	train loss: 16971.794921875	(31.0s)
epoch: 9	train loss: 16574.779296875	(31.0s)
epoch: 10	train loss: 16272.25390625	(30.9s)
epoch: 11	train loss: 15656.6767578125	(31.0s)
epoch: 12	train loss: 15527.4453125	(31.0s)
epoch: 13	train loss: 15224.7802734375	(31.0s)
epoch: 14	train loss: 14484.4091796875	(31.0s)
epoch: 15	train loss: 14432.36328125	(30.9s)
epoch: 16	train loss: 13946.625	(31.0s)
epoch: 17	train loss: 14002.208984375	(31.0s)
epoch: 18	train loss: 13577.3740234375	(31.0s)
epoch: 19	train loss: 14279.4658203125	(31.0s)
epoch: 20	train loss: 13294.8046875	(30.9s)
epoch: 21	train loss: 13546.2109375	(31.0s)
epoch: 22	train loss: 13055.1591796875	(31.0s)
epoch: 23	train loss: 12946.380859375	(31.0s)
epoch: 24	train loss: 12450.029296875	(31.0s)
epoch: 25	train loss: 13273.576171875	(30.9s)
epoch: 26	train loss: 12372.9033203125	(31.0s)
epoch: 27	train loss: 11994.779296875	(31.1s)
epoch: 28	train loss: 11924.150390625	(31.0s)
epoch: 29	train loss: 11600.431640625	(31.0s)
epoch: 30	train loss: 12365.4853515625	(30.9s)
epoch: 31	train loss: 11454.208984375	(31.0s)
epoch: 32	train loss: 11250.28125	(31.0s)
epoch: 33	train loss: 10965.5244140625	(31.0s)
epoch: 34	train loss: 11456.951171875	(31.0s)
epoch: 35	train loss: 11446.634765625	(30.9s)
epoch: 36	train loss: 11323.88671875	(31.0s)
epoch: 37	train loss: 10748.6748046875	(31.0s)
epoch: 38	train loss: 10410.513671875	(31.0s)
epoch: 39	train loss: 10801.8798828125	(31.0s)
epoch: 40	train loss: 10210.9677734375	(30.9s)
epoch: 41	train loss: 10627.9453125	(31.0s)
epoch: 42	train loss: 10722.7734375	(31.0s)
epoch: 43	train loss: 10195.6982421875	(31.0s)
epoch: 44	train loss: 9957.6796875	(31.0s)
epoch: 45	train loss: 10075.3330078125	(30.9s)
epoch: 46	train loss: 10257.84765625	(31.0s)
epoch: 47	train loss: 9833.4892578125	(31.0s)
epoch: 48	train loss: 10372.96484375	(31.0s)
epoch: 49	train loss: 10149.58203125	(31.0s)
epoch: 50	train loss: 10687.158203125	(31.0s)
epoch: 51	train loss: 9560.1689453125	(31.0s)
epoch: 52	train loss: 9407.1953125	(30.9s)
epoch: 53	train loss: 9366.814453125	(31.0s)
epoch: 54	train loss: 9431.6328125	(31.1s)
epoch: 55	train loss: 9795.6591796875	(31.0s)
epoch: 56	train loss: 10027.5048828125	(31.0s)
epoch: 57	train loss: 9196.3955078125	(30.9s)
epoch: 58	train loss: 9266.958984375	(31.0s)
epoch: 59	train loss: 9578.919921875	(31.1s)
epoch: 60	train loss: 9231.7529296875	(31.0s)
epoch: 61	train loss: 9326.1015625	(31.0s)
epoch: 62	train loss: 8892.6474609375	(30.9s)
epoch: 63	train loss: 8881.6044921875	(31.0s)
epoch: 64	train loss: 8920.6171875	(31.0s)
epoch: 65	train loss: 8747.3271484375	(30.9s)
epoch: 66	train loss: 8983.224609375	(31.0s)
epoch: 67	train loss: 8363.0546875	(30.9s)
epoch: 68	train loss: 8287.1162109375	(31.0s)
epoch: 69	train loss: 8560.5927734375	(31.0s)
epoch: 70	train loss: 8697.6904296875	(30.9s)
epoch: 71	train loss: 8890.2294921875	(31.0s)
epoch: 72	train loss: 8247.4599609375	(30.9s)
epoch: 73	train loss: 8909.1279296875	(31.0s)
epoch: 74	train loss: 8909.1298828125	(31.0s)
epoch: 75	train loss: 8142.60009765625	(31.0s)
epoch: 76	train loss: 8137.9208984375	(31.0s)
epoch: 77	train loss: 8697.4892578125	(30.9s)
epoch: 78	train loss: 8276.8642578125	(31.0s)
epoch: 79	train loss: 8434.3349609375	(31.0s)
epoch: 80	train loss: 8300.4765625	(31.0s)
epoch: 81	train loss: 8965.7060546875	(31.0s)
epoch: 82	train loss: 7788.09912109375	(30.9s)
epoch: 83	train loss: 7762.7333984375	(31.0s)
epoch: 84	train loss: 8107.60546875	(31.0s)
epoch: 85	train loss: 8069.13037109375	(31.0s)
epoch: 86	train loss: 7674.2958984375	(31.0s)
epoch: 87	train loss: 9102.2294921875	(30.9s)
epoch: 88	train loss: 8238.3115234375	(30.9s)
epoch: 89	train loss: 7540.0732421875	(30.9s)
epoch: 90	train loss: 7587.13525390625	(30.8s)
epoch: 91	train loss: 7595.30126953125	(30.9s)
epoch: 92	train loss: 7537.22216796875	(30.8s)
epoch: 93	train loss: 8031.986328125	(31.0s)
epoch: 94	train loss: 7480.34033203125	(31.1s)
epoch: 95	train loss: 7605.00048828125	(31.0s)
epoch: 96	train loss: 8188.55224609375	(31.0s)
epoch: 97	train loss: 7234.8701171875	(31.0s)
epoch: 98	train loss: 7386.736328125	(31.1s)
epoch: 99	train loss: 7867.1279296875	(31.0s)
Evaluating model on 200 episodes
0.000760008856146173
0.0012787719305893204
0.0009655512314768404
0.0013787935048722747
0.0012857316544492884
0.0006946169305592775
0.0008696463832166046
0.003007443172724119
0.0014803495553981823
0.0012856797426744986
0.0021658838377334177
0.0026028604013845326
0.0012961932570760837
0.003169676478137262
0.001562322420795681
0.0023644175053050276
0.0010673244880763377
0.001620924987946637
0.0008632989946428486
0.0012307600591157097
0.002020106669078814
0.0030397281863350565
0.0011448800478798436
0.0005977467051707208
0.0015864270224648697
0.0005368671677737999
0.0010695972481092515
0.0009344088768037702
0.0008880579619223929
0.0009148792223641067
0.0007177940315159503
0.0011699214344844222
0.0008620921384135727
0.0011219723034758186
0.0005803039777674713
0.0008038310278117514
0.0011143170540890424
0.0011356975457881792
0.001078416834039112
0.0022349145375691088
0.0020318565628258513
0.0019444859779189127
0.0016189710539557772
0.001043238158308668
0.0015684711409752101
0.001521502758675654
0.0010484786938225927
0.0015864020689540276
0.0018122715584468097
0.001112877357324275
0.0011097353602123733
0.0017293072954859012
0.0013798123296250456
0.0010171486352666525
0.0006479919341342923
0.000764109134391989
0.0027045643219025806
0.0016324704346890725
0.0009480719199927989
0.002882822995161405
0.0010633814759785309
0.0014779585409477086
0.0018154184096298802
0.0017966096638701856
0.0010511675097993301
0.004180024259767379
0.0020083567778783618
0.0009622960233173217
0.0009964615043524343
0.0008475475074697493
0.0010836486821062864
0.001135888978751609
0.0021486649568128327
0.0011675571440719068
0.0011721149000406608
0.005449343807413243
0.0032876975237741135
0.0012443301384337246
0.0021270997224039296
0.0008312445744296485
0.0011131956946580127
0.0013661436423717532
0.0017317692604592594
Solved 83/200 episodes
0.0006233236288799248
Evaluated model in 31.9 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-8
Round 9
Generated trajectories in 103.0 seconds
epoch: 0	train loss: 29337.923828125	(30.9s)
epoch: 1	train loss: 23487.693359375	(30.8s)
epoch: 2	train loss: 20507.197265625	(30.9s)
epoch: 3	train loss: 19032.20703125	(30.8s)
epoch: 4	train loss: 17806.193359375	(31.0s)
epoch: 5	train loss: 17049.837890625	(30.9s)
epoch: 6	train loss: 16354.013671875	(30.9s)
epoch: 7	train loss: 15688.5888671875	(30.9s)
epoch: 8	train loss: 14612.5419921875	(30.9s)
epoch: 9	train loss: 14452.974609375	(31.0s)
epoch: 10	train loss: 14187.33984375	(31.0s)
epoch: 11	train loss: 13932.7919921875	(30.9s)
epoch: 12	train loss: 13991.05859375	(31.0s)
epoch: 13	train loss: 12850.1572265625	(30.9s)
epoch: 14	train loss: 12988.2275390625	(31.0s)
epoch: 15	train loss: 12722.7783203125	(31.0s)
epoch: 16	train loss: 12436.7138671875	(30.9s)
epoch: 17	train loss: 13205.89453125	(31.1s)
epoch: 18	train loss: 11952.21875	(30.9s)
epoch: 19	train loss: 11662.056640625	(31.0s)
epoch: 20	train loss: 12268.390625	(31.0s)
epoch: 21	train loss: 12364.8779296875	(31.0s)
epoch: 22	train loss: 11580.001953125	(31.0s)
epoch: 23	train loss: 10653.5185546875	(30.9s)
epoch: 24	train loss: 11355.4208984375	(31.0s)
epoch: 25	train loss: 10922.998046875	(31.0s)
epoch: 26	train loss: 11110.220703125	(31.0s)
epoch: 27	train loss: 10626.7685546875	(31.0s)
epoch: 28	train loss: 10422.060546875	(30.9s)
epoch: 29	train loss: 9931.728515625	(31.0s)
epoch: 30	train loss: 10096.9658203125	(30.9s)
epoch: 31	train loss: 9580.9287109375	(31.0s)
epoch: 32	train loss: 9912.5771484375	(31.0s)
epoch: 33	train loss: 9944.203125	(30.9s)
epoch: 34	train loss: 9970.3369140625	(31.0s)
epoch: 35	train loss: 9480.33984375	(30.9s)
epoch: 36	train loss: 9703.9794921875	(31.0s)
epoch: 37	train loss: 8832.3544921875	(31.0s)
epoch: 38	train loss: 9679.2177734375	(31.2s)
epoch: 39	train loss: 9495.6025390625	(31.4s)
epoch: 40	train loss: 8882.771484375	(31.8s)
epoch: 41	train loss: 8621.2421875	(31.7s)
epoch: 42	train loss: 8882.884765625	(31.5s)
epoch: 43	train loss: 8419.494140625	(31.9s)
epoch: 44	train loss: 8277.1279296875	(32.1s)
epoch: 45	train loss: 9699.72265625	(32.0s)
epoch: 46	train loss: 9308.5986328125	(32.1s)
epoch: 47	train loss: 8494.42578125	(32.0s)
epoch: 48	train loss: 8435.103515625	(32.0s)
epoch: 49	train loss: 8441.0751953125	(32.1s)
epoch: 50	train loss: 7812.1943359375	(32.0s)
epoch: 51	train loss: 7857.09912109375	(32.1s)
epoch: 52	train loss: 8577.607421875	(32.1s)
epoch: 53	train loss: 8283.720703125	(32.1s)
epoch: 54	train loss: 8269.892578125	(32.0s)
epoch: 55	train loss: 8080.509765625	(32.0s)
epoch: 56	train loss: 7763.80517578125	(32.1s)
epoch: 57	train loss: 7640.1943359375	(31.4s)
epoch: 58	train loss: 7741.3271484375	(31.8s)
epoch: 59	train loss: 9903.212890625	(32.1s)
epoch: 60	train loss: 9055.533203125	(31.9s)
epoch: 61	train loss: 7312.44384765625	(32.1s)
epoch: 62	train loss: 7481.94921875	(32.0s)
epoch: 63	train loss: 7654.92919921875	(32.1s)
epoch: 64	train loss: 7582.83935546875	(31.9s)
epoch: 65	train loss: 7196.58251953125	(32.2s)
epoch: 66	train loss: 7051.35107421875	(32.0s)
epoch: 67	train loss: 7051.63427734375	(31.9s)
epoch: 68	train loss: 7937.54736328125	(32.2s)
epoch: 69	train loss: 7149.16650390625	(31.9s)
epoch: 70	train loss: 6639.25927734375	(32.1s)
epoch: 71	train loss: 6782.8974609375	(32.1s)
epoch: 72	train loss: 7064.8837890625	(32.0s)
epoch: 73	train loss: 6874.58056640625	(32.0s)
epoch: 74	train loss: 7190.1953125	(32.1s)
epoch: 75	train loss: 6825.88623046875	(32.1s)
epoch: 76	train loss: 6977.98388671875	(32.0s)
epoch: 77	train loss: 7117.9755859375	(32.0s)
epoch: 78	train loss: 7764.5546875	(32.1s)
epoch: 79	train loss: 7059.56103515625	(32.0s)
epoch: 80	train loss: 6638.22705078125	(31.7s)
epoch: 81	train loss: 7296.21142578125	(31.1s)
epoch: 82	train loss: 7023.8447265625	(31.4s)
epoch: 83	train loss: 7415.81201171875	(32.2s)
epoch: 84	train loss: 6638.99072265625	(32.0s)
epoch: 85	train loss: 6571.23681640625	(32.2s)
epoch: 86	train loss: 6783.22412109375	(32.0s)
epoch: 87	train loss: 6231.3212890625	(32.1s)
epoch: 88	train loss: 6407.1748046875	(32.0s)
epoch: 89	train loss: 6272.1455078125	(31.9s)
epoch: 90	train loss: 6038.005859375	(32.0s)
epoch: 91	train loss: 6452.59423828125	(32.1s)
epoch: 92	train loss: 6573.7255859375	(32.0s)
epoch: 93	train loss: 7533.42138671875	(32.1s)
epoch: 94	train loss: 6966.0302734375	(31.9s)
epoch: 95	train loss: 5865.107421875	(32.0s)
epoch: 96	train loss: 5904.56396484375	(32.1s)
epoch: 97	train loss: 6451.44287109375	(32.0s)
epoch: 98	train loss: 6347.125	(32.1s)
epoch: 99	train loss: 6080.01904296875	(32.0s)
Evaluating model on 200 episodes
0.0013257239894301164
0.000691528425704746
0.0010331704761483707
0.0012550241326607647
0.0022230155468100244
0.0008440832229098305
0.002598087904819598
0.0017962583886784206
0.0008087868860457093
0.001378856401667387
0.0023990003392100334
0.0010751629569789483
0.002551041784629758
0.00136023368734944
0.0030031522193374777
0.0010728396353608166
0.0020093884629507858
0.0010164487233851106
0.0026043845078675075
0.00036757706111529843
0.0011987876029955012
0.0011920873895922507
0.001732917943627399
0.001941410163999535
0.001592618355061859
0.002131093951538787
0.0017742097786270148
0.0013880259100915282
0.0012667323996235307
0.0009096093824609852
0.00048422588588437065
0.002024352781518246
0.0010022391051231687
0.0016286140463697457
0.0051330842998140724
0.0013814014615491032
0.001076480645861011
0.002258596068713814
0.0008448822985743531
0.0014439222161724632
0.0014430888812057673
0.000913675003955307
0.0013335015876994778
0.001184731446200102
0.0010809647008919944
0.0005123693244968308
0.0009834319996746043
0.0027256888114117407
0.000992674989700188
0.0020269183670669527
0.0012926472452818416
0.0011175557755493735
0.0011503135025962435
0.00143836068141704
0.0014459037093298608
0.0008725955752500644
0.0009497575895358367
0.0013485114084853028
0.0009572284761816263
0.0019349639398186763
0.0012564517351815344
0.0009749559426887168
0.0025165710608502302
0.0013709216194304947
0.002477328263921663
0.0009294695785987036
0.0008330390448918479
0.0022671576120450888
0.00096861327228208
0.0006959986903893878
0.0011145287256416243
0.0011841283703688532
0.00152411751198662
0.0006712571213518878
0.0016841869523886999
0.0020293239731951247
0.0018540681659942493
0.0017067472279170314
0.0011388409038772807
0.0004680785083716425
0.0010390957595518557
0.001488439864400649
0.0024954990605527364
0.001519671690542824
0.0010389006092736963
0.0011532655476912622
0.0004980011124809442
Solved 87/200 episodes
0.0006321129869093719
Evaluated model in 36.7 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-9
Round 10
Generated trajectories in 109.2 seconds
epoch: 0	train loss: 24621.787109375	(31.7s)
epoch: 1	train loss: 19217.349609375	(31.4s)
epoch: 2	train loss: 16743.783203125	(30.9s)
epoch: 3	train loss: 15253.212890625	(31.0s)
epoch: 4	train loss: 14184.26953125	(31.6s)
epoch: 5	train loss: 13142.4287109375	(32.0s)
epoch: 6	train loss: 13549.2001953125	(32.0s)
epoch: 7	train loss: 12778.4111328125	(31.9s)
epoch: 8	train loss: 12566.8271484375	(32.0s)
epoch: 9	train loss: 11579.2626953125	(32.0s)
epoch: 10	train loss: 11110.5888671875	(32.0s)
epoch: 11	train loss: 10908.8046875	(31.9s)
epoch: 12	train loss: 12223.125	(31.9s)
epoch: 13	train loss: 10619.2177734375	(32.0s)
epoch: 14	train loss: 10297.3134765625	(31.8s)
epoch: 15	train loss: 10437.7451171875	(32.1s)
epoch: 16	train loss: 10593.5849609375	(32.0s)
epoch: 17	train loss: 10505.087890625	(31.8s)
epoch: 18	train loss: 9554.0810546875	(32.1s)
epoch: 19	train loss: 9770.697265625	(31.9s)
epoch: 20	train loss: 10089.546875	(32.0s)
epoch: 21	train loss: 9129.4599609375	(32.1s)
epoch: 22	train loss: 8965.986328125	(32.0s)
epoch: 23	train loss: 9916.193359375	(32.0s)
epoch: 24	train loss: 8822.533203125	(31.8s)
epoch: 25	train loss: 8802.697265625	(32.0s)
epoch: 26	train loss: 8583.353515625	(32.0s)
epoch: 27	train loss: 8855.626953125	(31.9s)
epoch: 28	train loss: 8611.6240234375	(32.1s)
epoch: 29	train loss: 9311.0732421875	(31.9s)
epoch: 30	train loss: 9087.126953125	(31.9s)
epoch: 31	train loss: 8340.9951171875	(31.2s)
epoch: 32	train loss: 8577.404296875	(31.0s)
epoch: 33	train loss: 8394.6455078125	(31.0s)
epoch: 34	train loss: 8056.92431640625	(31.5s)
epoch: 35	train loss: 7872.15185546875	(32.1s)
epoch: 36	train loss: 8585.236328125	(32.0s)
epoch: 37	train loss: 7730.62060546875	(31.9s)
epoch: 38	train loss: 8684.9404296875	(32.0s)
epoch: 39	train loss: 8087.11328125	(31.9s)
epoch: 40	train loss: 7574.35595703125	(31.9s)
epoch: 41	train loss: 7993.1162109375	(31.9s)
epoch: 42	train loss: 7685.7578125	(32.0s)
epoch: 43	train loss: 7525.2783203125	(31.9s)
epoch: 44	train loss: 7674.85302734375	(31.9s)
epoch: 45	train loss: 7376.08642578125	(32.0s)
epoch: 46	train loss: 7193.00439453125	(31.8s)
epoch: 47	train loss: 8532.4775390625	(31.9s)
epoch: 48	train loss: 7277.63134765625	(32.1s)
epoch: 49	train loss: 6997.2939453125	(31.9s)
epoch: 50	train loss: 7393.07373046875	(31.9s)
epoch: 51	train loss: 6931.51708984375	(31.9s)
epoch: 52	train loss: 6917.06396484375	(32.0s)
epoch: 53	train loss: 8697.48046875	(32.1s)
epoch: 54	train loss: 8765.494140625	(32.0s)
epoch: 55	train loss: 8405.0458984375	(32.0s)
epoch: 56	train loss: 6815.326171875	(32.0s)
epoch: 57	train loss: 6631.73388671875	(31.9s)
epoch: 58	train loss: 6649.099609375	(31.9s)
epoch: 59	train loss: 7924.90185546875	(32.0s)
epoch: 60	train loss: 6964.41845703125	(32.0s)
epoch: 61	train loss: 6612.7353515625	(31.9s)
epoch: 62	train loss: 7126.26220703125	(32.0s)
epoch: 63	train loss: 6414.228515625	(31.8s)
epoch: 64	train loss: 6584.1904296875	(31.7s)
epoch: 65	train loss: 6352.40966796875	(31.0s)
epoch: 66	train loss: 6697.42626953125	(30.9s)
epoch: 67	train loss: 6379.73974609375	(31.0s)
epoch: 68	train loss: 6269.296875	(31.6s)
epoch: 69	train loss: 6393.7119140625	(32.0s)
epoch: 70	train loss: 6364.83447265625	(32.0s)
epoch: 71	train loss: 5809.93505859375	(31.9s)
epoch: 72	train loss: 5748.908203125	(32.0s)
epoch: 73	train loss: 6532.0888671875	(31.9s)
epoch: 74	train loss: 7545.06494140625	(31.9s)
epoch: 75	train loss: 6173.83935546875	(32.0s)
epoch: 76	train loss: 6344.91943359375	(32.0s)
epoch: 77	train loss: 6301.5361328125	(32.0s)
epoch: 78	train loss: 5494.990234375	(32.0s)
epoch: 79	train loss: 5825.95263671875	(32.0s)
epoch: 80	train loss: 6471.66748046875	(31.8s)
epoch: 81	train loss: 6361.84716796875	(32.1s)
epoch: 82	train loss: 5976.64404296875	(32.0s)
epoch: 83	train loss: 5849.53076171875	(31.8s)
epoch: 84	train loss: 10039.1904296875	(32.0s)
epoch: 85	train loss: 6956.14111328125	(32.0s)
epoch: 86	train loss: 7634.22265625	(31.9s)
epoch: 87	train loss: 6207.01123046875	(31.9s)
epoch: 88	train loss: 5813.68212890625	(31.9s)
epoch: 89	train loss: 5907.853515625	(31.9s)
epoch: 90	train loss: 5375.720703125	(31.7s)
epoch: 91	train loss: 5959.31689453125	(32.0s)
epoch: 92	train loss: 6751.400390625	(32.0s)
epoch: 93	train loss: 5897.720703125	(32.0s)
epoch: 94	train loss: 4864.51416015625	(32.0s)
epoch: 95	train loss: 4767.89404296875	(31.7s)
epoch: 96	train loss: 6089.8408203125	(32.0s)
epoch: 97	train loss: 5266.8466796875	(32.1s)
epoch: 98	train loss: 5174.2529296875	(31.9s)
epoch: 99	train loss: 6555.36328125	(31.2s)
Evaluating model on 200 episodes
0.0009185334347421303
0.0009825832301950349
0.002148160911763791
0.0008287626919542056
0.0010080884353050755
0.0014861066471591282
0.0007904747503094508
0.0004514278225542512
0.0013008001049456652
0.0010018973306337696
0.0009187990908685606
0.001313639278830831
0.0025505955272819847
0.001883135725537696
0.0008450942881800552
0.0007172213723499547
0.0015941441553877667
0.0009914642802565747
0.0026193965422862675
0.00228242373402911
0.001039549033381403
0.0011716722718639566
0.0012396982766707274
0.0009202194772418201
0.0011024556256415963
0.0008942554417113901
0.0022571713323906385
0.0011002771627520109
0.0018172241771026165
0.0011317359590479596
0.001014164650629167
0.0013105801243165677
0.0011866834764460738
0.0012180324851406592
0.00175464229323552
0.0011804065490485466
0.0008667141454578894
0.0008573915827302978
0.0005874496911322543
0.0014623316539135495
0.00243951532632179
0.0010330055314019166
0.000997345011312159
0.0011079756695835385
0.0006959170357276889
0.0008588400066530864
0.0014137888154740115
0.001772641718162049
0.0017635064415606814
0.0007314831392639982
0.004197775077773258
0.0018378851491434034
0.0010668019537358002
0.0014403613968170248
0.0019114522244005154
0.0009345740949760915
0.000950126554016606
0.0020143497342360204
0.0004821031928128962
0.001326454732528267
0.002542438060118002
0.0017754061191226356
0.0016698367124428676
0.0007263228086458184
0.0011496710387291387
0.001290788886763039
0.0029465667948898044
0.00045601079794075554
0.002560473984340206
0.002094018685311312
0.0016791534520355829
0.001187709301787739
0.0014280739284004084
0.0008250652340806978
0.001853487712320533
0.0018854039020987008
0.001032589387385222
0.0016670118244292097
0.002121869209986471
0.0023744094426299193
0.0003516129683703184
0.0012396105274612118
0.0023241244948621897
0.0011131701607761594
0.001209548923821115
0.0015037072165903415
0.0017081105092074723
0.0012553351017621545
0.000408315219222762
0.0018293719747816794
0.0009531336048288116
0.0013249793257934718
0.0006606485117420864
0.0010394418674298902
0.0010689742072524193
0.0015158140574840218
0.0009511589267276576
0.0016736790071263385
0.001058622154446579
0.0024273248922187262
0.00229372507866091
0.0017082000239973976
0.001085687119888866
0.0018660916703083785
0.0015644650235117296
0.0013545003481946576
0.0007969882729715513
0.0011511998392961687
0.0024237280248438336
Solved 109/200 episodes
0.0007642444195368186
Evaluated model in 35.6 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-10
Round 11
Generated trajectories in 105.2 seconds
epoch: 0	train loss: 22054.275390625	(31.5s)
epoch: 1	train loss: 16684.71484375	(31.5s)
epoch: 2	train loss: 15418.0556640625	(32.1s)
epoch: 3	train loss: 13385.8681640625	(31.9s)
epoch: 4	train loss: 12303.2568359375	(32.1s)
epoch: 5	train loss: 11867.0517578125	(32.1s)
epoch: 6	train loss: 11697.263671875	(32.0s)
epoch: 7	train loss: 10875.32421875	(32.1s)
epoch: 8	train loss: 11249.064453125	(32.0s)
epoch: 9	train loss: 10597.12890625	(32.1s)
epoch: 10	train loss: 9752.3056640625	(32.1s)
epoch: 11	train loss: 10053.9306640625	(31.9s)
epoch: 12	train loss: 9326.013671875	(32.0s)
epoch: 13	train loss: 10459.70703125	(32.0s)
epoch: 14	train loss: 9346.265625	(31.4s)
epoch: 15	train loss: 9785.8837890625	(31.7s)
epoch: 16	train loss: 8877.908203125	(32.0s)
epoch: 17	train loss: 8904.7373046875	(32.1s)
epoch: 18	train loss: 9068.9248046875	(31.9s)
epoch: 19	train loss: 8420.0341796875	(32.1s)
epoch: 20	train loss: 8316.634765625	(31.9s)
epoch: 21	train loss: 7991.3056640625	(32.1s)
epoch: 22	train loss: 8313.0419921875	(32.0s)
epoch: 23	train loss: 7677.357421875	(31.9s)
epoch: 24	train loss: 8581.8134765625	(32.1s)
epoch: 25	train loss: 7778.634765625	(32.0s)
epoch: 26	train loss: 7717.22216796875	(32.0s)
epoch: 27	train loss: 7856.30322265625	(32.1s)
epoch: 28	train loss: 7246.759765625	(31.8s)
epoch: 29	train loss: 8935.9892578125	(32.1s)
epoch: 30	train loss: 7162.45556640625	(31.9s)
epoch: 31	train loss: 7428.5634765625	(32.1s)
epoch: 32	train loss: 8204.140625	(31.9s)
epoch: 33	train loss: 7421.283203125	(32.0s)
epoch: 34	train loss: 7281.9267578125	(31.8s)
epoch: 35	train loss: 7089.3974609375	(31.0s)
epoch: 36	train loss: 7165.94189453125	(31.7s)
epoch: 37	train loss: 6971.78076171875	(32.1s)
epoch: 38	train loss: 8284.263671875	(31.9s)
epoch: 39	train loss: 6786.61328125	(32.1s)
epoch: 40	train loss: 6457.943359375	(31.9s)
epoch: 41	train loss: 6854.0107421875	(32.1s)
epoch: 42	train loss: 6391.9794921875	(32.0s)
epoch: 43	train loss: 6732.44775390625	(31.9s)
epoch: 44	train loss: 7665.75	(32.0s)
epoch: 45	train loss: 6586.25390625	(32.0s)
epoch: 46	train loss: 6386.265625	(31.9s)
epoch: 47	train loss: 5736.5693359375	(32.0s)
epoch: 48	train loss: 6132.22802734375	(32.0s)
epoch: 49	train loss: 5983.39306640625	(32.1s)
epoch: 50	train loss: 6295.623046875	(31.8s)
epoch: 51	train loss: 6262.69140625	(32.1s)
epoch: 52	train loss: 6220.75244140625	(31.9s)
epoch: 53	train loss: 7512.36572265625	(32.1s)
epoch: 54	train loss: 6039.9423828125	(32.0s)
epoch: 55	train loss: 6091.9755859375	(31.8s)
epoch: 56	train loss: 5645.81494140625	(32.0s)
epoch: 57	train loss: 6286.154296875	(31.3s)
epoch: 58	train loss: 6085.28515625	(31.0s)
epoch: 59	train loss: 5883.22705078125	(31.2s)
epoch: 60	train loss: 6226.107421875	(31.8s)
epoch: 61	train loss: 6535.85693359375	(32.0s)
epoch: 62	train loss: 6291.265625	(31.9s)
epoch: 63	train loss: 5523.21826171875	(32.0s)
epoch: 64	train loss: 5122.33203125	(31.9s)
epoch: 65	train loss: 4956.30224609375	(32.0s)
epoch: 66	train loss: 5473.4677734375	(31.9s)
epoch: 67	train loss: 4824.1689453125	(32.0s)
epoch: 68	train loss: 7073.91845703125	(32.0s)
epoch: 69	train loss: 6473.33349609375	(32.0s)
epoch: 70	train loss: 5910.01025390625	(32.0s)
epoch: 71	train loss: 5210.4365234375	(31.9s)
epoch: 72	train loss: 5252.9287109375	(32.0s)
epoch: 73	train loss: 5438.54931640625	(32.0s)
epoch: 74	train loss: 4918.71728515625	(32.0s)
epoch: 75	train loss: 5470.005859375	(32.0s)
epoch: 76	train loss: 5320.1669921875	(31.9s)
epoch: 77	train loss: 4948.79345703125	(32.0s)
epoch: 78	train loss: 7440.1630859375	(31.9s)
epoch: 79	train loss: 5175.91796875	(31.9s)
epoch: 80	train loss: 4879.8701171875	(32.1s)
epoch: 81	train loss: 6341.76513671875	(32.0s)
epoch: 82	train loss: 5903.96044921875	(32.0s)
epoch: 83	train loss: 6264.4814453125	(31.6s)
epoch: 84	train loss: 5411.89599609375	(30.9s)
epoch: 85	train loss: 4619.04248046875	(30.9s)
epoch: 86	train loss: 4478.37109375	(31.3s)
epoch: 87	train loss: 4421.453125	(31.9s)
epoch: 88	train loss: 4384.97119140625	(31.9s)
epoch: 89	train loss: 4813.392578125	(31.8s)
epoch: 90	train loss: 6739.7822265625	(32.0s)
epoch: 91	train loss: 5891.931640625	(32.0s)
epoch: 92	train loss: 4854.07373046875	(32.0s)
epoch: 93	train loss: 4709.31689453125	(31.9s)
epoch: 94	train loss: 4716.15185546875	(32.0s)
epoch: 95	train loss: 5600.22998046875	(32.1s)
epoch: 96	train loss: 4981.54248046875	(32.0s)
epoch: 97	train loss: 4481.85888671875	(32.1s)
epoch: 98	train loss: 5034.8330078125	(31.9s)
epoch: 99	train loss: 4439.791015625	(31.9s)
Evaluating model on 200 episodes
0.0016134845283886535
0.002293497170155336
0.0015347467535320903
0.0021073609896120615
0.0027191587651032023
0.002791727538230286
0.0011636767068172477
0.001460570269046002
0.0007561675001246233
0.0007111947030352894
0.0011037662043236196
0.0007862933762226021
0.0008749812576362663
0.0008893299908033276
0.0014997179871443223
0.0017532716718216595
0.004035477836926778
0.0012865784001010268
0.0010795824018714484
0.0012987728041480295
0.000788850448381189
0.0024361932459710674
0.00101613747006013
0.0017610633803997189
0.000958177528809756
0.002193731243261771
0.001702554778220426
0.0016077624626362834
0.0018696975266720983
0.0012621962135502447
0.0016820427133227764
0.0016614517366438772
0.0009109143969302144
0.001444226697328434
0.0010806532241076638
0.0005316536997270304
0.0009564047459906971
0.0009362452055938775
0.0022810436261352152
0.0017067454355128575
0.0018859135219827295
0.0025487141348620704
0.0010210204195270005
0.0011744419162924816
0.0015643387578165857
0.000911541990111194
0.001995847804209916
0.0012338253531197553
0.002290248235415978
0.002098809607559815
0.0011519922732986742
0.0027461494318916342
0.001230480319615405
0.0010400748053923184
0.0016334451875081868
0.00130505229006101
0.0006583872836927185
0.0018278127390658483
0.0009447265321413349
0.0016684365647961386
0.002104024674432973
0.0014938578038709237
0.0009996993729146197
0.0017821656037995126
0.0008092594580375589
0.002207362310501695
0.001076322948475453
0.0016719733357604127
0.0011549062102555279
0.0016244133468717337
0.0010673433969714096
0.0008839654600941921
0.0012226783619553316
0.0006105398841620106
0.001978778124546145
0.00047963818416554626
0.0008523739366368814
0.0016658135735067641
0.0009955741883863488
0.0009778654692292382
0.0012984476806278426
0.0018493775090010947
0.0030261623032856733
0.0009244225044052915
0.0013891038861402194
0.0014852229841755401
0.003592631355526724
0.0016132853024128752
0.001030043036735151
0.00257226888788864
0.0006332863407199815
0.0011432951585101407
0.0028110080398619174
0.0007440515123912169
0.001331452873915516
0.001820593550711727
0.0007599324706365893
0.001081928681635284
0.002263770480931271
0.003572168846343023
0.0011361206386209233
0.0017156463216815609
0.0015740231258405402
0.0011644508663771881
0.0015720706160209375
0.0017184151696710615
0.0010564210067968815
0.0011531459203979466
0.0011713332973562905
0.0009660685696179003
0.00150917047864388
0.0007719602959696204
0.0010610645743630205
0.0023546971795440188
0.002390342840226367
0.001214340697818746
0.0010572688561296673
0.0025002485803270248
0.002201732741013984
0.002258737222607875
0.0020736360489132088
0.0015840280367618626
0.0006727018100920961
0.0014720606995979324
0.003716398889082484
0.000904296483440703
0.0025276791294505958
0.0014413983282044202
Solved 128/200 episodes
0.0009850941463882934
Evaluated model in 40.4 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-11
Round 12
Generated trajectories in 110.8 seconds
epoch: 0	train loss: 20119.7421875	(32.1s)
epoch: 1	train loss: 15406.9462890625	(32.0s)
epoch: 2	train loss: 13499.4423828125	(32.0s)
epoch: 3	train loss: 12631.9140625	(32.2s)
epoch: 4	train loss: 11730.1494140625	(32.0s)
epoch: 5	train loss: 10888.984375	(32.2s)
epoch: 6	train loss: 10625.546875	(32.3s)
epoch: 7	train loss: 10151.6630859375	(31.8s)
epoch: 8	train loss: 9816.6103515625	(31.2s)
epoch: 9	train loss: 9596.8369140625	(31.1s)
epoch: 10	train loss: 9833.4658203125	(31.5s)
epoch: 11	train loss: 8785.533203125	(32.0s)
epoch: 12	train loss: 8423.0947265625	(32.2s)
epoch: 13	train loss: 8323.9072265625	(32.2s)
epoch: 14	train loss: 8455.09375	(32.0s)
epoch: 15	train loss: 8519.3994140625	(32.3s)
epoch: 16	train loss: 7817.369140625	(32.2s)
epoch: 17	train loss: 8407.78125	(32.3s)
epoch: 18	train loss: 7662.775390625	(32.2s)
epoch: 19	train loss: 8272.0400390625	(32.1s)
epoch: 20	train loss: 8198.4912109375	(32.3s)
epoch: 21	train loss: 7362.27490234375	(32.1s)
epoch: 22	train loss: 7707.64306640625	(32.2s)
epoch: 23	train loss: 7537.8193359375	(32.3s)
epoch: 24	train loss: 7393.52001953125	(32.1s)
epoch: 25	train loss: 7417.9775390625	(32.3s)
epoch: 26	train loss: 8238.7998046875	(32.1s)
epoch: 27	train loss: 7529.65673828125	(32.3s)
epoch: 28	train loss: 6682.9140625	(32.2s)
epoch: 29	train loss: 6807.84521484375	(32.1s)
epoch: 30	train loss: 6650.74853515625	(32.3s)
epoch: 31	train loss: 7766.41845703125	(32.1s)
epoch: 32	train loss: 6510.11328125	(32.3s)
epoch: 33	train loss: 7441.43359375	(32.2s)
epoch: 34	train loss: 8399.8330078125	(32.3s)
epoch: 35	train loss: 6318.984375	(32.2s)
epoch: 36	train loss: 6682.55419921875	(31.5s)
epoch: 37	train loss: 6111.48583984375	(31.2s)
epoch: 38	train loss: 6312.27978515625	(31.2s)
epoch: 39	train loss: 6268.97119140625	(31.2s)
epoch: 40	train loss: 8894.9541015625	(31.9s)
epoch: 41	train loss: 6815.18701171875	(31.3s)
epoch: 42	train loss: 6661.6337890625	(31.3s)
epoch: 43	train loss: 5590.5546875	(31.2s)
epoch: 44	train loss: 6482.28125	(31.4s)
epoch: 45	train loss: 6326.73681640625	(31.4s)
epoch: 46	train loss: 6286.44091796875	(31.3s)
epoch: 47	train loss: 6565.1259765625	(31.9s)
epoch: 48	train loss: 6531.9599609375	(32.1s)
epoch: 49	train loss: 6229.658203125	(32.2s)
epoch: 50	train loss: 6793.7275390625	(32.2s)
epoch: 51	train loss: 6189.0869140625	(32.1s)
epoch: 52	train loss: 5210.2822265625	(32.2s)
epoch: 53	train loss: 5163.146484375	(32.1s)
epoch: 54	train loss: 5338.787109375	(32.2s)
epoch: 55	train loss: 5365.71630859375	(32.2s)
epoch: 56	train loss: 6885.5087890625	(32.1s)
epoch: 57	train loss: 5989.9970703125	(32.2s)
epoch: 58	train loss: 5612.92138671875	(32.1s)
epoch: 59	train loss: 5209.7939453125	(32.2s)
epoch: 60	train loss: 5006.58154296875	(32.1s)
epoch: 61	train loss: 5485.64404296875	(31.6s)
epoch: 62	train loss: 6257.484375	(31.6s)
epoch: 63	train loss: 5569.2841796875	(32.1s)
epoch: 64	train loss: 5671.20068359375	(32.1s)
epoch: 65	train loss: 5754.2080078125	(32.0s)
epoch: 66	train loss: 5810.1201171875	(32.2s)
epoch: 67	train loss: 5417.84423828125	(32.1s)
epoch: 68	train loss: 4940.4521484375	(32.1s)
epoch: 69	train loss: 4356.119140625	(32.2s)
epoch: 70	train loss: 4534.13427734375	(32.0s)
epoch: 71	train loss: 7374.1298828125	(32.2s)
epoch: 72	train loss: 5412.857421875	(32.1s)
epoch: 73	train loss: 4870.626953125	(32.1s)
epoch: 74	train loss: 4660.6884765625	(32.2s)
epoch: 75	train loss: 4893.5302734375	(31.9s)
epoch: 76	train loss: 5288.4072265625	(32.1s)
epoch: 77	train loss: 4773.57763671875	(32.2s)
epoch: 78	train loss: 4642.20849609375	(32.0s)
epoch: 79	train loss: 5347.9365234375	(32.3s)
epoch: 80	train loss: 8781.990234375	(32.0s)
epoch: 81	train loss: 5042.76953125	(32.1s)
epoch: 82	train loss: 4457.15576171875	(32.1s)
epoch: 83	train loss: 4464.14697265625	(32.0s)
epoch: 84	train loss: 3858.777099609375	(32.2s)
epoch: 85	train loss: 5035.41650390625	(32.0s)
epoch: 86	train loss: 4434.240234375	(32.1s)
epoch: 87	train loss: 5157.09326171875	(31.9s)
epoch: 88	train loss: 4744.69775390625	(31.8s)
epoch: 89	train loss: 4533.63623046875	(31.2s)
epoch: 90	train loss: 5178.78173828125	(31.5s)
epoch: 91	train loss: 4569.99169921875	(32.2s)
epoch: 92	train loss: 4499.27294921875	(32.1s)
epoch: 93	train loss: 5063.86572265625	(32.1s)
epoch: 94	train loss: 4551.9912109375	(32.1s)
epoch: 95	train loss: 4772.9443359375	(32.1s)
epoch: 96	train loss: 3994.055908203125	(32.1s)
epoch: 97	train loss: 4887.23095703125	(32.0s)
epoch: 98	train loss: 3864.838623046875	(32.2s)
epoch: 99	train loss: 3940.915771484375	(32.0s)
Evaluating model on 200 episodes
0.0013948567914970529
0.001440928023449877
0.0015302443061955273
0.0007923193885896277
0.001532838259828912
0.0007831689995327906
0.001272911539854249
0.0013760683199389831
0.0006929114045176123
0.000711210074950941
0.0012839416982127919
0.0010797303849459342
0.001553370926558273
0.0007320437345277661
0.00024322532772202976
0.0005836186566739344
0.00288603263652476
0.00217794769196189
0.0011676102731144056
0.0009639919547172373
0.0017751222508700772
0.0013000347704898256
0.0012901333393529058
0.0007624769357335026
0.0006570708016321684
0.0010096615072013395
0.0006395862180094506
0.0013589500822490663
0.0019900293425760334
0.0014637991189374588
0.000913317545564496
0.0013463775994750904
0.0013378210818960465
0.0006118901013299668
0.0012869578919586881
0.0007283991427458306
0.00052549224299008
0.0012036399469004457
0.0012587668898049742
0.0014465133717749268
0.0011802115208970813
0.0014319693813873032
0.001736264513705724
0.0004950258058110754
0.0009172089753519685
0.001156398608476262
0.002307948337693233
0.001759916718583554
0.0009819336550468247
0.000938436144289361
0.0012358206203013347
0.0007646070870881279
0.0005131714178787661
0.0012088349597304063
0.001287685239471362
0.001256513660616922
0.0009859586619131732
0.002415834002343164
0.0005704650104841372
0.0019756809973235554
0.0013042043764681483
0.0010816906860175854
0.0017567712857271545
0.000876116077536911
0.0013144069337158726
0.001237198744352749
0.0011491592808202508
0.0006934473913133843
0.001563650451619954
0.0026510320971474834
0.002520376530950531
0.001400720329084751
0.00175705616287208
0.0012804397731749552
0.0016955099478461004
0.0010527075858893415
0.0011504270284300826
0.0008542310970369726
0.0007689542850130238
0.0007154869337682613
0.0017614263499787987
0.001253866104282982
0.000769244036847441
0.0010120119055500253
0.0012684686301905788
0.0008480889597800948
0.0005101382172748951
0.0013312852252056473
0.0005025980763093685
0.0018108417241213222
0.0008112009636533912
0.002202401176483753
0.0005219578587067579
0.0010342304344703734
0.00057628054595885
0.0006587347990197097
0.002159692444062481
0.00048708249551054905
0.0014170858566617624
0.0009396330866366043
0.0010998038018442458
0.001104580737856767
0.0005478612648504269
0.0026797921365262784
0.0009288427499996033
0.0012290858318850112
0.0008673588123532351
0.001135354814109365
0.0018249284553081345
0.0011113487912966361
0.0010995199845638126
0.0013134674394347132
0.0009826796422203188
0.001772841002093628
0.0009872759837890044
0.0015362372757711758
0.0007426244569614937
0.0014590002016727037
0.0021575529703113717
Solved 119/200 episodes
0.0007275245406776962
Evaluated model in 36.0 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-12
Round 13
Generated trajectories in 109.5 seconds
epoch: 0	train loss: 18394.8671875	(31.6s)
epoch: 1	train loss: 13713.3466796875	(31.8s)
epoch: 2	train loss: 11940.9140625	(31.9s)
epoch: 3	train loss: 11579.0673828125	(31.9s)
epoch: 4	train loss: 10884.103515625	(32.0s)
epoch: 5	train loss: 10228.814453125	(31.9s)
epoch: 6	train loss: 9749.93359375	(31.9s)
epoch: 7	train loss: 9482.3896484375	(31.9s)
epoch: 8	train loss: 9147.189453125	(32.0s)
epoch: 9	train loss: 8855.6259765625	(31.9s)
epoch: 10	train loss: 8307.2783203125	(32.0s)
epoch: 11	train loss: 7905.5537109375	(32.0s)
epoch: 12	train loss: 8146.2158203125	(31.8s)
epoch: 13	train loss: 10716.7080078125	(32.0s)
epoch: 14	train loss: 8609.94921875	(31.9s)
epoch: 15	train loss: 7551.55712890625	(31.9s)
epoch: 16	train loss: 7253.388671875	(31.4s)
epoch: 17	train loss: 7385.25537109375	(31.0s)
epoch: 18	train loss: 7154.88525390625	(31.3s)
epoch: 19	train loss: 8703.4833984375	(31.9s)
epoch: 20	train loss: 7704.251953125	(32.1s)
epoch: 21	train loss: 6817.9912109375	(32.0s)
epoch: 22	train loss: 7081.69580078125	(32.1s)
epoch: 23	train loss: 7400.06494140625	(31.9s)
epoch: 24	train loss: 6923.158203125	(32.0s)
epoch: 25	train loss: 6671.99560546875	(32.1s)
epoch: 26	train loss: 5954.904296875	(32.1s)
epoch: 27	train loss: 6051.84228515625	(32.1s)
epoch: 28	train loss: 6399.77783203125	(32.1s)
epoch: 29	train loss: 6237.38623046875	(31.9s)
epoch: 30	train loss: 6477.7177734375	(32.1s)
epoch: 31	train loss: 7006.30322265625	(31.8s)
epoch: 32	train loss: 5688.09423828125	(32.2s)
epoch: 33	train loss: 5969.27685546875	(32.1s)
epoch: 34	train loss: 5870.8310546875	(32.0s)
epoch: 35	train loss: 5617.67236328125	(32.1s)
epoch: 36	train loss: 7090.95263671875	(31.9s)
epoch: 37	train loss: 6172.11328125	(32.1s)
epoch: 38	train loss: 6184.37548828125	(32.0s)
epoch: 39	train loss: 6900.90625	(32.2s)
epoch: 40	train loss: 6800.28515625	(31.9s)
epoch: 41	train loss: 6902.208984375	(31.2s)
epoch: 42	train loss: 5586.18017578125	(31.1s)
epoch: 43	train loss: 6218.03125	(31.2s)
epoch: 44	train loss: 5848.61962890625	(31.9s)
epoch: 45	train loss: 5583.58056640625	(32.1s)
epoch: 46	train loss: 5420.46240234375	(32.0s)
epoch: 47	train loss: 6633.53759765625	(32.0s)
epoch: 48	train loss: 5692.3134765625	(32.0s)
epoch: 49	train loss: 5129.3466796875	(31.9s)
epoch: 50	train loss: 6073.7412109375	(32.1s)
epoch: 51	train loss: 5216.0126953125	(32.1s)
epoch: 52	train loss: 4878.64501953125	(32.0s)
epoch: 53	train loss: 5449.62939453125	(32.1s)
epoch: 54	train loss: 5767.16357421875	(32.1s)
epoch: 55	train loss: 5543.56982421875	(32.0s)
epoch: 56	train loss: 4898.5693359375	(32.1s)
epoch: 57	train loss: 5933.05078125	(32.0s)
epoch: 58	train loss: 5467.02880859375	(32.1s)
epoch: 59	train loss: 4932.36572265625	(32.1s)
epoch: 60	train loss: 4753.7646484375	(32.0s)
epoch: 61	train loss: 4693.36279296875	(32.1s)
epoch: 62	train loss: 4228.73583984375	(31.9s)
epoch: 63	train loss: 5057.0	(32.2s)
epoch: 64	train loss: 6289.5146484375	(32.0s)
epoch: 65	train loss: 4756.728515625	(31.9s)
epoch: 66	train loss: 4576.14404296875	(32.1s)
epoch: 67	train loss: 5050.50341796875	(31.9s)
epoch: 68	train loss: 7100.80322265625	(32.1s)
epoch: 69	train loss: 5033.31494140625	(32.1s)
epoch: 70	train loss: 4628.00537109375	(32.0s)
epoch: 71	train loss: 5156.74658203125	(31.8s)
epoch: 72	train loss: 6026.15185546875	(30.9s)
epoch: 73	train loss: 4526.96630859375	(31.0s)
epoch: 74	train loss: 3994.169921875	(30.9s)
epoch: 75	train loss: 4176.298828125	(31.7s)
epoch: 76	train loss: 3589.335205078125	(32.1s)
epoch: 77	train loss: 5332.36962890625	(32.1s)
epoch: 78	train loss: 4624.5849609375	(31.9s)
epoch: 79	train loss: 4875.41162109375	(31.9s)
epoch: 80	train loss: 4084.655517578125	(32.1s)
epoch: 81	train loss: 5876.2705078125	(32.0s)
epoch: 82	train loss: 5367.12744140625	(32.1s)
epoch: 83	train loss: 5045.98828125	(31.9s)
epoch: 84	train loss: 4870.912109375	(31.9s)
epoch: 85	train loss: 5481.41357421875	(32.0s)
epoch: 86	train loss: 4807.06103515625	(31.8s)
epoch: 87	train loss: 4039.7626953125	(31.9s)
epoch: 88	train loss: 4020.343994140625	(32.0s)
epoch: 89	train loss: 3914.0390625	(31.9s)
epoch: 90	train loss: 5514.71630859375	(32.0s)
epoch: 91	train loss: 4228.697265625	(32.0s)
epoch: 92	train loss: 5252.921875	(32.0s)
epoch: 93	train loss: 4248.9150390625	(32.0s)
epoch: 94	train loss: 4301.6708984375	(32.1s)
epoch: 95	train loss: 4029.00537109375	(32.0s)
epoch: 96	train loss: 4482.59423828125	(31.9s)
epoch: 97	train loss: 6416.90673828125	(32.1s)
epoch: 98	train loss: 5754.896484375	(31.9s)
epoch: 99	train loss: 4343.66796875	(32.0s)
Evaluating model on 200 episodes
0.0006760495712764428
0.0017673612528597005
0.001007100917323602
0.0012124126492141698
0.0011555650884187535
0.0013184466180973687
0.0007231309771834111
0.0007853057510286677
0.0009635102596803335
0.0010353082375331724
0.00044514419537985565
0.0011649548774585127
0.0014235489603985723
0.0006934575180316137
0.0013910479168631495
0.0008912423867773864
0.00199708142794076
0.0011137993650562647
0.0012801005258762206
0.0013558304909868943
0.001842820034653414
0.0024277487391373143
0.0015298288853955455
0.0005461652344820322
0.0009111168334735945
0.0006353116385879628
0.0008692323390278034
0.0010231218957414967
0.0004800311351293483
0.001061225446755998
0.0011675072710204404
0.0007328960986342281
0.001427364860016193
0.0010041634831438949
0.0004528251738520339
0.0012218129783529246
0.001182612442147836
0.0018976111461799722
0.0017277394610573538
0.0015856089920210554
0.0010388969500997084
0.0010109643920562423
0.0011050661851186305
0.0007364969278569333
0.0011634671747742687
0.0020981717059518866
0.0007316087994695408
0.000678138285625859
0.0008766778358091999
0.0012069594113199855
0.0010154662195418496
0.0007811374385450368
0.0011002764026910115
0.0014359019696712494
0.0006501472044664507
0.0011863683418844382
0.0019707275544836498
0.001471810331471109
0.0019551133271306755
0.0009711090252884398
0.001606023946964263
0.0007827231474948348
0.001839055711874001
0.0006365660467256223
0.0013194485845815507
0.0013170292384327492
0.000727389422536362
0.0010699224021664122
0.0012378623541735578
0.0014753764415218029
0.0007697377150179818
0.0003934059982734303
0.0007492707201208759
0.0011254217378639927
0.00197316327066801
0.0010282742732670158
0.0010648245629757487
0.0006233193713082333
0.002075255824214158
0.0015645099615953354
0.0007990095121688986
0.0005053856836942335
0.001242444141462329
0.0008624591939224047
0.001349692017196075
0.0014339777579152723
0.0009312997577321117
0.001022920682728346
0.0008614086073066574
0.0007932869119940733
0.0007166290345897627
0.0016735651330711098
0.0006551606925818811
0.0009161233707952002
0.0012716963924503943
0.001010248973761918
0.0005217223006184213
0.002036874872121391
0.001760156212791723
0.0016423922328006786
0.0019832779622187707
0.0011383475881302728
0.0006638923965510912
0.0010447629166366016
0.0009021212720724109
0.001682392349071701
0.0007339844347110816
0.0008607815907453187
0.001394965054714703
0.001269829455850413
0.000674218861377085
0.0012244270576903706
0.0005863080789115176
0.0008029514428926632
0.0013451371439889325
0.0016336140735802474
0.0009025745293911314
0.0010396068550603973
0.0012796526809159498
0.0019431870362798993
0.0013774873304310648
0.0012217910700807503
0.0009848003071109654
0.0005277183046928258
0.0012941220823753004
0.0018189928748390892
0.0017262222207998427
0.0016572833382168836
0.0008667202209835523
0.0014127102366056415
0.0008108566936344788
0.0026900655906906146
0.0005964917195692229
0.0008597887084333171
0.0007174769864202692
0.0010690128401620314
0.0014837260015257117
0.0009383490832988173
0.0011652891450163831
0.0010002769049606286
0.0007859906349040102
Solved 141/200 episodes
0.0008140394392423616
Evaluated model in 39.1 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-13
Round 14
Generated trajectories in 111.3 seconds
epoch: 0	train loss: 15452.044921875	(31.7s)
epoch: 1	train loss: 12158.2861328125	(31.0s)
epoch: 2	train loss: 10555.4306640625	(31.0s)
epoch: 3	train loss: 9704.1640625	(31.2s)
epoch: 4	train loss: 9295.5	(31.5s)
epoch: 5	train loss: 10418.255859375	(31.7s)
epoch: 6	train loss: 8409.431640625	(31.5s)
epoch: 7	train loss: 7802.26318359375	(31.6s)
epoch: 8	train loss: 7802.49609375	(31.8s)
epoch: 9	train loss: 7980.8310546875	(32.2s)
epoch: 10	train loss: 7831.8642578125	(32.1s)
epoch: 11	train loss: 7324.85302734375	(31.9s)
epoch: 12	train loss: 8234.78515625	(32.1s)
epoch: 13	train loss: 7342.7802734375	(32.1s)
epoch: 14	train loss: 6136.18212890625	(32.1s)
epoch: 15	train loss: 6512.8271484375	(32.2s)
epoch: 16	train loss: 6490.59619140625	(32.1s)
epoch: 17	train loss: 6299.44775390625	(32.2s)
epoch: 18	train loss: 6298.50634765625	(32.0s)
epoch: 19	train loss: 6763.07568359375	(32.2s)
epoch: 20	train loss: 6290.59814453125	(32.1s)
epoch: 21	train loss: 11013.8095703125	(32.2s)
epoch: 22	train loss: 7297.0546875	(32.2s)
epoch: 23	train loss: 5710.37939453125	(32.0s)
epoch: 24	train loss: 5474.83251953125	(32.1s)
epoch: 25	train loss: 5316.34716796875	(31.8s)
epoch: 26	train loss: 5129.04052734375	(31.4s)
epoch: 27	train loss: 5882.4296875	(32.2s)
epoch: 28	train loss: 7631.18798828125	(32.1s)
epoch: 29	train loss: 5608.85498046875	(32.3s)
epoch: 30	train loss: 6413.50048828125	(32.0s)
epoch: 31	train loss: 5407.61376953125	(32.2s)
epoch: 32	train loss: 5414.6279296875	(32.2s)
epoch: 33	train loss: 4937.6953125	(32.1s)
epoch: 34	train loss: 4660.51171875	(32.2s)
epoch: 35	train loss: 9985.5068359375	(32.1s)
epoch: 36	train loss: 5435.4228515625	(32.1s)
epoch: 37	train loss: 5087.703125	(32.1s)
epoch: 38	train loss: 5269.12841796875	(32.1s)
epoch: 39	train loss: 5792.89697265625	(32.1s)
epoch: 40	train loss: 4850.22314453125	(32.2s)
epoch: 41	train loss: 4098.5078125	(32.2s)
epoch: 42	train loss: 4190.201171875	(32.1s)
epoch: 43	train loss: 4453.767578125	(32.1s)
epoch: 44	train loss: 5173.123046875	(32.2s)
epoch: 45	train loss: 6296.7705078125	(31.6s)
epoch: 46	train loss: 5063.49462890625	(31.2s)
epoch: 47	train loss: 5003.15576171875	(31.9s)
epoch: 48	train loss: 4324.6328125	(32.2s)
epoch: 49	train loss: 4098.1689453125	(32.1s)
epoch: 50	train loss: 4650.11181640625	(32.2s)
epoch: 51	train loss: 5239.755859375	(32.2s)
epoch: 52	train loss: 4042.7333984375	(32.1s)
epoch: 53	train loss: 3875.77978515625	(32.3s)
epoch: 54	train loss: 6273.39013671875	(32.1s)
epoch: 55	train loss: 5243.72607421875	(32.1s)
epoch: 56	train loss: 4514.56298828125	(32.1s)
epoch: 57	train loss: 3699.182373046875	(32.2s)
epoch: 58	train loss: 3919.13818359375	(32.3s)
epoch: 59	train loss: 4498.07666015625	(32.2s)
epoch: 60	train loss: 5164.533203125	(32.2s)
epoch: 61	train loss: 4906.9296875	(32.1s)
epoch: 62	train loss: 5768.6142578125	(32.3s)
epoch: 63	train loss: 4814.1845703125	(32.2s)
epoch: 64	train loss: 3647.890380859375	(31.7s)
epoch: 65	train loss: 4785.841796875	(31.3s)
epoch: 66	train loss: 5499.4521484375	(31.3s)
epoch: 67	train loss: 3837.784912109375	(32.0s)
epoch: 68	train loss: 4565.09228515625	(32.1s)
epoch: 69	train loss: 4942.1396484375	(32.3s)
epoch: 70	train loss: 4044.8203125	(32.2s)
epoch: 71	train loss: 4142.12890625	(32.2s)
epoch: 72	train loss: 4104.84619140625	(32.3s)
epoch: 73	train loss: 3521.02587890625	(32.2s)
epoch: 74	train loss: 6758.01953125	(32.1s)
epoch: 75	train loss: 4765.05322265625	(32.2s)
epoch: 76	train loss: 4363.0302734375	(32.1s)
epoch: 77	train loss: 4022.174072265625	(32.3s)
epoch: 78	train loss: 3542.158447265625	(32.2s)
epoch: 79	train loss: 3736.00927734375	(32.2s)
epoch: 80	train loss: 3444.75341796875	(32.1s)
epoch: 81	train loss: 4747.1484375	(32.2s)
epoch: 82	train loss: 3405.328125	(32.3s)
epoch: 83	train loss: 5354.314453125	(32.1s)
epoch: 84	train loss: 4158.11767578125	(32.1s)
epoch: 85	train loss: 4593.21826171875	(32.1s)
epoch: 86	train loss: 3618.31982421875	(31.9s)
epoch: 87	train loss: 3168.51806640625	(31.0s)
epoch: 88	train loss: 3827.639892578125	(31.2s)
epoch: 89	train loss: 4296.021484375	(31.5s)
epoch: 90	train loss: 4529.45703125	(31.8s)
epoch: 91	train loss: 6216.07666015625	(32.3s)
epoch: 92	train loss: 4935.1826171875	(32.1s)
epoch: 93	train loss: 3386.3388671875	(32.3s)
epoch: 94	train loss: 3829.914306640625	(32.2s)
epoch: 95	train loss: 3159.886474609375	(32.2s)
epoch: 96	train loss: 3180.957763671875	(32.2s)
epoch: 97	train loss: 3409.9853515625	(32.0s)
epoch: 98	train loss: 3537.84375	(32.3s)
epoch: 99	train loss: 3912.77783203125	(32.0s)
Evaluating model on 200 episodes
0.0005311632868445789
0.0006387839421222452
0.0007476685805158922
0.0004181464192745472
0.0013062711999130745
0.001213683353171291
0.0007878283715096283
0.0017281576808520847
0.0004873985797695985
0.0006812808461836539
0.001464672514967705
0.00029875742135724675
0.00031191778089123545
0.0011062246260280517
0.000888568055905545
0.0015095456401468255
0.0012628938287749547
0.0006942434320080793
0.0009992952642884727
0.0018907185881289479
0.0007281414277713338
0.0005694949223569976
0.0010430613680606864
0.0008724923621533284
0.0010755161542874703
0.0014342193768243305
0.0013063810009852103
0.0010153751318284776
0.0009913816436892375
0.0011926846104457086
0.0014386991865143904
0.0006178858680609015
0.0027643501719770334
0.0005146657899280803
0.0017249039378334095
0.0008832268515681406
0.0011827869899207143
0.0007453955440050104
0.0013719748535550025
0.00046353447828551
0.0012822223174225656
0.001059596275323808
0.0009185271379205265
0.0014228329046090948
0.0012693450456329931
0.0014433605505665152
0.00041647876302401227
0.0009043954772399351
0.0007373846153010769
0.0009533822046180901
0.0015535026392599854
0.0012030073456116952
0.0007343903974591134
0.0017199458645033399
0.0005555240783327132
0.001065944387411709
0.0011966117913938693
0.0016244492382446135
0.0007342560453583825
0.0010894610243970722
0.002466192039719317
0.002008844123338349
0.0011783611198552535
0.002002578563685977
0.0009424781123276002
0.0015012343210401013
0.00047471665727373747
0.0006400451231456828
0.0008938414451904464
0.0006227582634892315
0.0020313301855432136
0.0008395071372433449
0.0006771985727027641
0.00038509506967701867
0.0009575496886100154
0.0015939381721961712
0.0006552708082381287
0.001225792502615756
0.0006309930361437312
0.0014892515252237597
0.000997851176428164
0.0007468421511778919
0.0013132258518453455
0.001352162767034315
0.0016476725447175436
0.0009498214560153429
0.0008112073483061977
0.0016015467453353966
0.004156759755543052
0.0007310516898128712
0.0008939145565299052
0.002038015725338482
0.000841282241001156
0.0026665332861739444
0.0013567824595763038
0.0011754595971069648
0.0018961539265001193
0.0014994716402725317
0.0014371846835794194
0.0021649185817715568
0.001428527658780305
0.0011979482005699538
0.0012686891772318632
0.0007033746877217103
0.0010458969802130014
0.0027569970429445114
0.001113337948040818
0.001140979582487489
0.0020378192214258505
0.0014804843940510182
0.0013125926695800828
0.001265507067728322
0.0010251164421788417
0.0015380032431990417
0.0010968239647419459
0.0007301580606674014
0.001545911441748363
0.0024155051611999693
0.0016117015866257134
0.0010946325380375907
0.0017150823821793892
0.0004985534889985704
0.00045633501576958224
0.0014041983494179375
0.0010223009074772044
0.0009549886952299857
0.0020254834505197194
0.0011941457128159527
0.0016668513029192884
0.0005535491629202358
0.0007812000708327105
0.0012481368934719025
0.0006286537410280863
0.0010627085945695096
0.0004988391595058298
0.0012665061694860924
0.0011342272498748368
0.001405070725013502
0.001241641763651084
0.0006657582318225383
0.001085569189550976
0.0009152102232162308
0.0009915102714980353
0.0009290284391075451
0.0018729905015406028
0.0018207674885551178
Solved 146/200 episodes
0.0008656607904390405
Evaluated model in 41.6 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-14
Round 15
Generated trajectories in 109.0 seconds
epoch: 0	train loss: 14101.388671875	(31.6s)
epoch: 1	train loss: 10300.8740234375	(31.7s)
epoch: 2	train loss: 8908.8427734375	(31.7s)
epoch: 3	train loss: 8168.30810546875	(31.9s)
epoch: 4	train loss: 7461.5791015625	(31.8s)
epoch: 5	train loss: 7183.7607421875	(31.9s)
epoch: 6	train loss: 7051.91064453125	(31.7s)
epoch: 7	train loss: 7194.43701171875	(31.8s)
epoch: 8	train loss: 7347.03857421875	(31.8s)
epoch: 9	train loss: 7804.38671875	(31.7s)
epoch: 10	train loss: 8010.10009765625	(31.1s)
epoch: 11	train loss: 6320.9365234375	(30.7s)
epoch: 12	train loss: 5566.84130859375	(30.8s)
epoch: 13	train loss: 5144.5908203125	(31.4s)
epoch: 14	train loss: 5541.0166015625	(31.8s)
epoch: 15	train loss: 5419.07275390625	(31.9s)
epoch: 16	train loss: 6014.814453125	(31.9s)
epoch: 17	train loss: 6465.78369140625	(31.7s)
epoch: 18	train loss: 5452.6875	(31.8s)
epoch: 19	train loss: 5354.72705078125	(32.0s)
epoch: 20	train loss: 5560.0556640625	(31.8s)
epoch: 21	train loss: 4798.02392578125	(31.7s)
epoch: 22	train loss: 5744.33154296875	(31.8s)
epoch: 23	train loss: 5325.46728515625	(31.7s)
epoch: 24	train loss: 4766.41552734375	(31.9s)
epoch: 25	train loss: 6078.31298828125	(31.9s)
epoch: 26	train loss: 5451.59912109375	(32.0s)
epoch: 27	train loss: 4361.873046875	(31.7s)
epoch: 28	train loss: 5386.07861328125	(31.9s)
epoch: 29	train loss: 4488.95458984375	(31.8s)
epoch: 30	train loss: 4349.86328125	(31.8s)
epoch: 31	train loss: 4316.94091796875	(31.9s)
epoch: 32	train loss: 3704.30517578125	(31.8s)
epoch: 33	train loss: 3722.18359375	(31.8s)
epoch: 34	train loss: 3442.2080078125	(31.9s)
epoch: 35	train loss: 7626.60693359375	(31.9s)
epoch: 36	train loss: 4655.22021484375	(31.8s)
epoch: 37	train loss: 4595.578125	(31.9s)
epoch: 38	train loss: 4279.39990234375	(31.4s)
epoch: 39	train loss: 6391.07421875	(30.8s)
epoch: 40	train loss: 6457.8955078125	(30.9s)
epoch: 41	train loss: 3923.95556640625	(30.8s)
epoch: 42	train loss: 4075.46875	(31.5s)
epoch: 43	train loss: 3717.636474609375	(31.4s)
epoch: 44	train loss: 3614.586181640625	(31.0s)
epoch: 45	train loss: 3502.754150390625	(31.1s)
epoch: 46	train loss: 3738.698974609375	(30.9s)
epoch: 47	train loss: 4291.982421875	(30.9s)
epoch: 48	train loss: 4114.5986328125	(31.4s)
epoch: 49	train loss: 4297.11962890625	(31.3s)
epoch: 50	train loss: 4040.450927734375	(31.7s)
epoch: 51	train loss: 9901.890625	(31.8s)
epoch: 52	train loss: 5878.5703125	(31.8s)
epoch: 53	train loss: 3923.332275390625	(31.9s)
epoch: 54	train loss: 5409.5634765625	(31.8s)
epoch: 55	train loss: 3777.941162109375	(31.8s)
epoch: 56	train loss: 3120.906982421875	(31.8s)
epoch: 57	train loss: 3311.3916015625	(31.8s)
epoch: 58	train loss: 2958.9716796875	(31.6s)
epoch: 59	train loss: 3547.141845703125	(30.9s)
epoch: 60	train loss: 3357.2080078125	(32.0s)
epoch: 61	train loss: 3321.47802734375	(31.9s)
epoch: 62	train loss: 3381.380126953125	(31.8s)
epoch: 63	train loss: 3585.23291015625	(31.9s)
epoch: 64	train loss: 4502.390625	(31.7s)
epoch: 65	train loss: 4852.00439453125	(31.8s)
epoch: 66	train loss: 4383.740234375	(31.8s)
epoch: 67	train loss: 4599.77685546875	(31.9s)
epoch: 68	train loss: 3229.955810546875	(31.5s)
epoch: 69	train loss: 2907.470703125	(30.8s)
epoch: 70	train loss: 2663.268310546875	(31.5s)
epoch: 71	train loss: 2991.8857421875	(31.7s)
epoch: 72	train loss: 2885.73486328125	(31.9s)
epoch: 73	train loss: 4492.34130859375	(31.7s)
epoch: 74	train loss: 4942.75830078125	(31.8s)
epoch: 75	train loss: 3720.20654296875	(31.9s)
epoch: 76	train loss: 2480.26220703125	(31.7s)
epoch: 77	train loss: 3539.668212890625	(31.9s)
epoch: 78	train loss: 2635.032958984375	(31.8s)
epoch: 79	train loss: 2943.245849609375	(31.8s)
epoch: 80	train loss: 3075.685546875	(31.2s)
epoch: 81	train loss: 4299.07275390625	(30.7s)
epoch: 82	train loss: 4100.33203125	(31.0s)
epoch: 83	train loss: 3221.602294921875	(31.8s)
epoch: 84	train loss: 2914.24072265625	(31.7s)
epoch: 85	train loss: 2604.4228515625	(31.8s)
epoch: 86	train loss: 3314.89453125	(31.6s)
epoch: 87	train loss: 4275.89013671875	(31.9s)
epoch: 88	train loss: 3182.574951171875	(31.7s)
epoch: 89	train loss: 2889.137451171875	(31.9s)
epoch: 90	train loss: 3079.435302734375	(31.9s)
epoch: 91	train loss: 4111.72705078125	(31.6s)
epoch: 92	train loss: 3721.894775390625	(31.9s)
epoch: 93	train loss: 4186.19921875	(31.8s)
epoch: 94	train loss: 3308.57666015625	(31.6s)
epoch: 95	train loss: 2736.169189453125	(30.9s)
epoch: 96	train loss: 2618.358642578125	(30.8s)
epoch: 97	train loss: 3524.2158203125	(31.1s)
epoch: 98	train loss: 3352.890869140625	(31.7s)
epoch: 99	train loss: 2741.642578125	(31.9s)
Evaluating model on 200 episodes
0.0009602263971828506
0.0028320358676585155
0.0005190818349850209
0.0005514399580144007
0.0005418186358708356
0.0006577686726814136
0.0003964018324040808
0.0009807111573931963
0.0005074350283393869
0.0007351588222566837
0.0007481727006961592
0.0012654607108624591
0.0007881656100558441
0.00045435415268002544
0.0002402804760398188
0.000655124935292406
0.0005259453027974814
0.00044399557494519587
0.0007200044744119319
0.000634108731901506
0.0006263579411461251
0.0020022528357609795
0.0011518001796907305
0.0008180602160470752
0.0018559803785664325
0.0013786574501974621
0.0009124056212183657
0.0006611961905340043
0.0010272963263560086
0.00132396508979582
0.000833995471607936
0.0006887948147777934
0.0007593262915699077
0.0014961857232265174
0.0012918076116714954
0.0011157093479141622
0.001466526763331124
0.001389977060171077
0.0020052083197627394
0.0007546575717343432
0.001328800478404446
0.0005666335988886809
0.00047617597605990755
0.001847747596912086
0.000685502772640852
0.0010691963425415452
0.001908609327316905
0.0010719650472310605
0.0013923270626736877
0.0006349489966461468
0.0010915618117905979
0.001489324110336479
0.0008350990519829793
0.0012973354739523003
0.00044235318648588873
0.0011339732966287276
0.0007866091852974932
0.0015308693931729068
0.0011030753826162254
0.0016135309801589197
0.0007942166446030439
0.0005479427898535505
0.00037331052735680716
0.0008415110956279518
0.0003685143571849701
0.0006655917938835707
0.0022074442204029765
0.0007922710226141914
0.0009186637164475542
0.0015992237628476385
0.0012073974976374302
0.0008722464034163798
0.0006527364756421823
0.0011478407989973978
0.001340725540285348
0.0009867918970485334
0.0011234162572209777
0.0005327010603650706
0.001509476483624894
0.001320061551682143
0.0015179123633970448
0.0005226123747706879
0.000720376854587812
0.0008252220018870762
0.0011314490971430628
0.0009158591819868889
0.0008132438143708414
0.0008677874237556742
0.000619060959881888
0.000541445661311675
0.0008564525162780358
0.0023090963038287426
0.0008012152155226266
0.0005042761060229333
0.0013667511415810117
0.0008886909503913062
0.0009639189988471723
0.0010571403067193127
0.0011202310309575598
0.0007921709814885011
0.0007476576606264148
0.0013893153606789252
0.0008021557110954415
0.0005887364881346002
0.0006398197840419872
0.00036886392296186057
0.0008648444429407112
0.0005384122487157583
0.0010712023538265688
0.000993078570900252
0.0010886024736009858
0.0005534850871689352
0.0011210418810862946
0.0018408612442726735
0.0006386053688402171
0.000926534557947889
0.001170226686129657
0.0007280705193317352
0.0027130275623428383
0.0010926302117695741
0.0007906967539383913
0.0009576774287519843
0.0008571393981665923
0.0014760577061679215
0.0010033602202383918
0.000914243425844082
0.0004390328203953686
0.001622492467504344
0.0007807370163979666
0.0011031908370569
0.0005974541173701356
0.0006442437179430271
0.0011203933685869237
0.0009080666234131925
0.0008215323231035235
0.0005432261774937311
0.000751882716545167
0.0008616033805689464
0.0009221659405739046
0.0012220560828508395
0.0008961658984389942
0.0007847506978058456
0.0013766545455907292
0.0005470657762045795
0.0008467989719065372
0.004198630547570677
0.0006466665048113404
0.0007921062334879445
0.000877077174855482
0.0008261952649003693
0.000738996006839443
0.001758717011398403
0.0006006161556191121
0.0006238508378208445
0.00038899719826683093
0.0015992100067835832
Solved 156/200 episodes
0.0007783870591319249
Evaluated model in 40.1 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-15
Round 16
Generated trajectories in 110.1 seconds
epoch: 0	train loss: 11408.0078125	(31.8s)
epoch: 1	train loss: 8991.1591796875	(31.7s)
epoch: 2	train loss: 7386.59423828125	(31.8s)
epoch: 3	train loss: 7124.71728515625	(32.0s)
epoch: 4	train loss: 6867.84765625	(31.5s)
epoch: 5	train loss: 5940.91259765625	(31.0s)
epoch: 6	train loss: 5705.9970703125	(30.9s)
epoch: 7	train loss: 5377.35546875	(31.2s)
epoch: 8	train loss: 5126.35205078125	(31.7s)
epoch: 9	train loss: 5793.52880859375	(31.9s)
epoch: 10	train loss: 4864.607421875	(31.8s)
epoch: 11	train loss: 4521.1611328125	(31.9s)
epoch: 12	train loss: 4384.12744140625	(31.8s)
epoch: 13	train loss: 5486.59814453125	(32.0s)
epoch: 14	train loss: 5590.8095703125	(31.9s)
epoch: 15	train loss: 5068.24365234375	(31.9s)
epoch: 16	train loss: 4412.53369140625	(31.9s)
epoch: 17	train loss: 4996.70654296875	(31.8s)
epoch: 18	train loss: 5086.75048828125	(32.0s)
epoch: 19	train loss: 5120.83837890625	(31.8s)
epoch: 20	train loss: 4307.76220703125	(32.0s)
epoch: 21	train loss: 4679.0419921875	(31.8s)
epoch: 22	train loss: 3863.817626953125	(31.7s)
epoch: 23	train loss: 4412.81396484375	(30.9s)
epoch: 24	train loss: 3901.007568359375	(31.0s)
epoch: 25	train loss: 5489.75439453125	(30.9s)
epoch: 26	train loss: 4236.9619140625	(31.4s)
epoch: 27	train loss: 4383.10595703125	(32.1s)
epoch: 28	train loss: 6038.42626953125	(31.3s)
epoch: 29	train loss: 4404.40673828125	(32.0s)
epoch: 30	train loss: 4108.37646484375	(31.9s)
epoch: 31	train loss: 3417.760498046875	(31.8s)
epoch: 32	train loss: 3168.42578125	(31.8s)
epoch: 33	train loss: 7330.91015625	(31.9s)
epoch: 34	train loss: 5631.78857421875	(31.8s)
epoch: 35	train loss: 4722.49365234375	(31.9s)
epoch: 36	train loss: 4151.50732421875	(31.7s)
epoch: 37	train loss: 4801.8525390625	(31.9s)
epoch: 38	train loss: 3207.76708984375	(31.7s)
epoch: 39	train loss: 3076.5986328125	(31.3s)
epoch: 40	train loss: 2910.75341796875	(31.5s)
epoch: 41	train loss: 2914.4736328125	(31.9s)
epoch: 42	train loss: 3428.175537109375	(31.8s)
epoch: 43	train loss: 4002.954345703125	(31.8s)
epoch: 44	train loss: 3442.6962890625	(31.8s)
epoch: 45	train loss: 3659.40869140625	(31.9s)
epoch: 46	train loss: 3057.7939453125	(31.8s)
epoch: 47	train loss: 2966.68115234375	(31.9s)
epoch: 48	train loss: 4250.4541015625	(31.7s)
epoch: 49	train loss: 3381.552978515625	(31.9s)
epoch: 50	train loss: 4167.83154296875	(31.9s)
epoch: 51	train loss: 4612.205078125	(31.9s)
epoch: 52	train loss: 3456.194580078125	(31.1s)
epoch: 53	train loss: 5060.08935546875	(31.0s)
epoch: 54	train loss: 3637.7490234375	(31.7s)
epoch: 55	train loss: 2791.173828125	(31.9s)
epoch: 56	train loss: 4033.272216796875	(31.8s)
epoch: 57	train loss: 3128.893798828125	(32.0s)
epoch: 58	train loss: 2947.88671875	(31.8s)
epoch: 59	train loss: 3214.231689453125	(31.8s)
epoch: 60	train loss: 3057.5302734375	(31.8s)
epoch: 61	train loss: 3255.6201171875	(31.9s)
epoch: 62	train loss: 2746.32666015625	(32.0s)
epoch: 63	train loss: 3137.819580078125	(31.7s)
epoch: 64	train loss: 3752.670654296875	(31.9s)
epoch: 65	train loss: 3801.547119140625	(31.1s)
epoch: 66	train loss: 3904.396240234375	(31.0s)
epoch: 67	train loss: 4793.64111328125	(31.2s)
epoch: 68	train loss: 3285.462158203125	(31.8s)
epoch: 69	train loss: 3194.862548828125	(31.8s)
epoch: 70	train loss: 2592.342041015625	(31.8s)
epoch: 71	train loss: 3908.218017578125	(31.8s)
epoch: 72	train loss: 3532.589599609375	(31.9s)
epoch: 73	train loss: 3388.12109375	(31.7s)
epoch: 74	train loss: 2819.213134765625	(32.0s)
epoch: 75	train loss: 2605.806884765625	(31.7s)
epoch: 76	train loss: 3400.6865234375	(31.9s)
epoch: 77	train loss: 4711.98486328125	(31.8s)
epoch: 78	train loss: 3460.86279296875	(31.9s)
epoch: 79	train loss: 3108.312255859375	(31.8s)
epoch: 80	train loss: 2717.0439453125	(31.8s)
epoch: 81	train loss: 2891.540283203125	(31.3s)
epoch: 82	train loss: 3991.688232421875	(30.7s)
epoch: 83	train loss: 3223.758056640625	(30.7s)
epoch: 84	train loss: 2380.505615234375	(31.3s)
epoch: 85	train loss: 2148.496826171875	(31.6s)
epoch: 86	train loss: 2420.042724609375	(31.9s)
epoch: 87	train loss: 5662.357421875	(31.7s)
epoch: 88	train loss: 3152.712646484375	(31.9s)
epoch: 89	train loss: 2404.040771484375	(31.8s)
epoch: 90	train loss: 3909.86376953125	(31.9s)
epoch: 91	train loss: 2972.421875	(31.8s)
epoch: 92	train loss: 3784.29833984375	(31.8s)
epoch: 93	train loss: 2444.85595703125	(31.8s)
epoch: 94	train loss: 3687.28369140625	(31.9s)
epoch: 95	train loss: 2487.5341796875	(31.8s)
epoch: 96	train loss: 2877.34521484375	(32.0s)
epoch: 97	train loss: 2532.477294921875	(31.7s)
epoch: 98	train loss: 2806.4462890625	(31.9s)
epoch: 99	train loss: 2239.253173828125	(31.8s)
Evaluating model on 200 episodes
0.0016789043780590872
0.001905395638459595
0.0006024998172506457
0.0007870746242891377
0.002402176732074521
0.0006359388442598919
0.0011731163787771948
0.0010365862126491265
0.0006007228425914946
0.0008169073917088099
0.0006730291418286718
0.0008983673130722692
0.0005410410328425184
0.0009648307393945287
0.0007183866000559647
0.0007403923759738973
0.0008699860982233076
0.0006290010841136488
0.0003928453021217138
0.0016035773835028522
0.0007995841355295852
0.001091101078250526
0.001216522327134347
0.0011160510017045613
0.0006343404970215892
0.0010488449002473822
0.0004814334724170084
0.0004996082716388628
0.0006988064960751217
0.000618167392531177
0.0021086452308130294
0.003712419573275838
0.001007341540181577
0.0006074419730187704
0.0007347374796933893
0.0011640145967248827
0.0009065125010238262
0.0009300317910588894
0.0004648693681831771
0.0012572072424946407
0.0006912042339940348
0.00037599648241762464
0.0006886771866687157
0.0006991350783209782
0.0036002798648648118
0.0010610299426704072
0.001181596533327441
0.0017787714969017543
0.000575957689761708
0.0013717417490884795
0.0011673419510316307
0.0015405227940601225
0.0012976352546892095
0.00044884562157676555
0.0012309901701519266
0.0009387346510361807
0.0010020704879934784
0.0007278089266643898
0.0008903201182281761
0.0013850654081579705
0.0007479203923139721
0.0006717576699075936
0.0014851807986815402
0.0011051484703784808
0.0005838423475276949
0.0004616807748334395
0.0007117161996361307
0.0010868137167776963
0.002081410497339675
0.0005298690460040234
0.0010549209030917457
0.0005137028946202398
0.0014986470022172991
0.0018112945668286153
0.0004883225022543533
0.0004072827919117117
0.0007706003837353949
0.000915686048574571
0.0010898181341569095
0.0008684453096066136
0.0006823940406320616
0.0008850193238079859
0.001055921722581843
0.0006863168886671442
0.0010437873486527959
0.0019909494260312334
0.000854986923513934
0.0006686648412141949
0.0006740806063383141
0.0015328752636130296
0.001043773729664584
0.0008532245074628223
0.001914683468890467
0.0008455793052105166
0.0007209581633408864
0.0006608503451085804
0.000788398725717343
0.0009996345892432147
0.000461116630255922
0.0009555841954186855
0.0012510537616671408
0.0012207460835766556
0.000629851703949195
0.0003143076215305233
0.0007182924536584109
0.0009358090877261324
0.0012243740645014138
0.0010175325170910748
0.0006067476642783731
0.0018612376879900694
0.0010629485654395207
0.0008337819396426008
0.0007195714685870123
0.0009660190145950765
0.0009040642335094162
0.0008542081632185727
0.0006001098451330714
0.001011760916629679
0.001000142095521865
0.0005857000858882469
0.0009967214583329305
0.0009686466995238637
0.0009460278474169014
0.0017285764681772612
0.00034624276668182574
0.0012883865492767654
0.0016626197393634357
0.0008624938467013029
0.0008382720543522737
0.0008712799637578428
0.0013476023170649288
0.0002665450568504942
0.0013232781927702793
0.001258510583890408
0.0008210866988520138
0.0008707877266141096
0.0006463102009774957
0.0005487342292326503
0.0008984961828640994
0.0008443595494706339
0.0015925564374284524
0.002148722240841986
0.0015895739362397428
0.0004283379310739595
0.0009734406226521565
0.0010424126347061246
0.0007046108342117143
0.000721705686136493
0.0009098200281531999
0.0009783514094372301
0.0008354450061839694
0.0009098741723970637
0.00077652697909798
0.001841857375620748
0.00046118528812907504
0.0007251234419527464
0.001591415073883528
0.0009527736527326687
0.0007807212714396883
0.001112761810873053
0.0009141169478728747
0.0014718091078975704
0.0012787390034645796
0.0007301600087278833
Solved 164/200 episodes
0.0008237516247254812
Evaluated model in 43.3 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-16
Round 17
Generated trajectories in 105.0 seconds
epoch: 0	train loss: 9543.7734375	(31.8s)
epoch: 1	train loss: 6493.1123046875	(32.0s)
epoch: 2	train loss: 6018.82470703125	(32.2s)
epoch: 3	train loss: 5692.533203125	(32.2s)
epoch: 4	train loss: 5025.18994140625	(32.2s)
epoch: 5	train loss: 4890.07958984375	(32.1s)
epoch: 6	train loss: 4529.919921875	(32.2s)
epoch: 7	train loss: 5300.87744140625	(32.2s)
epoch: 8	train loss: 4275.2734375	(32.3s)
epoch: 9	train loss: 5104.38330078125	(32.3s)
epoch: 10	train loss: 4978.26025390625	(32.2s)
epoch: 11	train loss: 3807.364013671875	(32.2s)
epoch: 12	train loss: 3580.978759765625	(32.1s)
epoch: 13	train loss: 6650.80712890625	(32.1s)
epoch: 14	train loss: 6052.4384765625	(31.3s)
epoch: 15	train loss: 4200.873046875	(31.2s)
epoch: 16	train loss: 4616.25	(31.2s)
epoch: 17	train loss: 4980.564453125	(31.4s)
epoch: 18	train loss: 3560.641845703125	(31.7s)
epoch: 19	train loss: 3330.60205078125	(31.2s)
epoch: 20	train loss: 3313.058349609375	(31.1s)
epoch: 21	train loss: 4695.74267578125	(31.2s)
epoch: 22	train loss: 2989.3681640625	(31.2s)
epoch: 23	train loss: 4614.3486328125	(31.3s)
epoch: 24	train loss: 4886.3916015625	(31.3s)
epoch: 25	train loss: 3552.93798828125	(31.2s)
epoch: 26	train loss: 3927.169677734375	(31.7s)
epoch: 27	train loss: 2954.321533203125	(32.1s)
epoch: 28	train loss: 3135.25732421875	(32.3s)
epoch: 29	train loss: 2979.9267578125	(32.2s)
epoch: 30	train loss: 3263.802734375	(32.2s)
epoch: 31	train loss: 3034.542724609375	(32.2s)
epoch: 32	train loss: 3295.96826171875	(32.1s)
epoch: 33	train loss: 3329.212646484375	(32.2s)
epoch: 34	train loss: 5592.73388671875	(31.5s)
epoch: 35	train loss: 3099.268798828125	(32.2s)
epoch: 36	train loss: 3292.450927734375	(32.3s)
epoch: 37	train loss: 3345.830322265625	(32.1s)
epoch: 38	train loss: 2729.666259765625	(32.3s)
epoch: 39	train loss: 2591.3154296875	(32.2s)
epoch: 40	train loss: 3255.50048828125	(32.2s)
epoch: 41	train loss: 3733.216552734375	(32.2s)
epoch: 42	train loss: 7139.31201171875	(32.0s)
epoch: 43	train loss: 3326.488037109375	(31.3s)
epoch: 44	train loss: 2971.19580078125	(31.5s)
epoch: 45	train loss: 5677.95751953125	(32.3s)
epoch: 46	train loss: 3226.53271484375	(32.3s)
epoch: 47	train loss: 2862.798583984375	(32.1s)
epoch: 48	train loss: 2994.09423828125	(32.2s)
epoch: 49	train loss: 2940.79638671875	(32.2s)
epoch: 50	train loss: 2563.77783203125	(32.2s)
epoch: 51	train loss: 3240.425537109375	(32.2s)
epoch: 52	train loss: 3669.415771484375	(32.1s)
epoch: 53	train loss: 3047.703125	(32.1s)
epoch: 54	train loss: 3019.193115234375	(31.2s)
epoch: 55	train loss: 3845.31103515625	(31.2s)
epoch: 56	train loss: 2874.774169921875	(31.9s)
epoch: 57	train loss: 2791.163330078125	(32.2s)
epoch: 58	train loss: 2141.279052734375	(32.2s)
epoch: 59	train loss: 3758.29150390625	(32.1s)
epoch: 60	train loss: 3439.388427734375	(32.2s)
epoch: 61	train loss: 2522.515380859375	(32.3s)
epoch: 62	train loss: 2626.41650390625	(32.2s)
epoch: 63	train loss: 3226.66845703125	(32.2s)
epoch: 64	train loss: 3350.136962890625	(31.9s)
epoch: 65	train loss: 2790.044921875	(31.2s)
epoch: 66	train loss: 3614.50146484375	(31.2s)
epoch: 67	train loss: 2452.65283203125	(31.7s)
epoch: 68	train loss: 3316.776611328125	(32.2s)
epoch: 69	train loss: 3575.989501953125	(32.1s)
epoch: 70	train loss: 5970.5673828125	(32.2s)
epoch: 71	train loss: 2735.834228515625	(32.2s)
epoch: 72	train loss: 2554.06689453125	(32.1s)
epoch: 73	train loss: 4256.61669921875	(32.1s)
epoch: 74	train loss: 3848.455322265625	(32.1s)
epoch: 75	train loss: 2938.422119140625	(32.2s)
epoch: 76	train loss: 2185.16650390625	(32.2s)
epoch: 77	train loss: 2877.581298828125	(32.1s)
epoch: 78	train loss: 4128.4150390625	(31.5s)
epoch: 79	train loss: 2684.116455078125	(31.1s)
epoch: 80	train loss: 3367.33251953125	(31.2s)
epoch: 81	train loss: 3616.87548828125	(31.4s)
epoch: 82	train loss: 2337.12353515625	(31.9s)
epoch: 83	train loss: 2150.83935546875	(32.1s)
epoch: 84	train loss: 2663.065185546875	(32.0s)
epoch: 85	train loss: 2625.670166015625	(32.2s)
epoch: 86	train loss: 2096.064208984375	(32.2s)
epoch: 87	train loss: 2451.78076171875	(32.2s)
epoch: 88	train loss: 3472.03271484375	(32.2s)
epoch: 89	train loss: 3445.861328125	(32.0s)
epoch: 90	train loss: 2536.651123046875	(32.2s)
epoch: 91	train loss: 2455.951171875	(32.1s)
epoch: 92	train loss: 2258.7431640625	(32.2s)
epoch: 93	train loss: 2062.710205078125	(32.2s)
epoch: 94	train loss: 3807.3173828125	(31.4s)
epoch: 95	train loss: 2094.1806640625	(31.2s)
epoch: 96	train loss: 1989.4193115234375	(31.2s)
epoch: 97	train loss: 1993.9971923828125	(31.4s)
epoch: 98	train loss: 2162.15185546875	(31.9s)
epoch: 99	train loss: 5375.45947265625	(31.4s)
Evaluating model on 200 episodes
0.0006274136203501257
0.0011130180388136067
0.0010339529639727867
0.0007998825081317177
0.0005678903164354981
0.0013124032579980849
0.0002488674282277417
0.00037547226978718897
0.0005629969740160353
0.0007139970492482042
0.0005700746552292912
0.0006031319978016351
0.000303069920391863
0.0005319914577161068
0.0009904942577453437
0.0012860437987414612
0.0006584422735613771
0.0003857626915515329
0.00038508317120431455
0.0007101149240043014
0.0007260858374138479
0.0003536034871811151
0.0004926027782361912
0.000295609692532
0.00041138082893407306
0.0003206071911942369
0.0007460420906681975
0.00027052657019844954
0.00039830714707601476
0.0004117274561155538
0.0012998696647021765
0.00036280184122006176
0.0009619570714892775
0.0005390205471485388
0.0003990816708210332
0.0006461818637843761
0.000728202952814172
0.00022828204445041983
0.0008767715990918651
0.0006729568322043633
0.0008686573551544411
0.0006887178185570519
0.000679782820498076
0.00024240128356420125
0.000606088587043511
0.0007157145055316505
0.0003936842054422919
0.0006527632000749387
0.001050752876229429
0.00035559713962191546
0.00035358950635082484
0.00043393529631430283
0.0015593662877411891
0.0005768003268357185
0.0008259793440198624
0.000500835141296689
0.0008664338430903359
0.0007001332218910997
0.0004335661278949401
0.0003053135296795517
0.0007830990208377545
0.0006783552671549842
0.0004054138705777448
0.0004474953835597262
0.0003950841850878946
0.00032017388101931744
0.0007698520666356974
0.00040635668246186867
0.000984030910139585
0.0015451718080791914
0.0007586519865546269
0.0010334807205557202
0.0005830381672309998
0.0005406201077372922
0.0006873672595247626
0.0007669818335082656
0.00036765878963785286
0.00032783231819683516
0.0007238707513696963
0.0005214640640098458
0.0006335239818740361
0.0006244291125767631
0.001112083783482376
0.0005270580227910873
0.0008422767686473283
0.00043830062941196957
0.0006035738465884192
0.0004722773380556398
0.0005772161096571765
0.000840443932563927
0.0011856091166464466
0.00034011670605751633
0.0003626893667387776
0.000770772941677933
0.00032940100321026804
0.0006195596290039941
0.0004729531727207359
0.0008405935281189158
0.0007602887952089076
0.001160764325565348
0.0005056118738698737
0.0005852892713197175
0.00048328082497887823
0.00025908001600114135
0.0010686138535115437
0.0005241708940957324
0.0013091780701264119
0.0006812514712995229
0.0005967611731042174
0.000338169372601745
0.0007627049055285863
0.0008441585617363575
0.000658473255559026
0.0008148630656958428
0.0007015592202151311
0.00038437687379138714
0.0004310403194668812
0.0009477538798819296
0.0004829446314715824
0.0002895974761155561
0.0004181129204100996
0.0005828642577398568
0.0004663037640663485
0.000619568360539583
0.0008519365232392115
0.0005017457781202841
0.0005286222620987136
0.0004212456343728783
0.0005705142426780764
0.0003350567349116318
0.00038096782782304217
0.0007529748759943208
0.0008919653385722389
0.0008592259578290395
0.0006953467411883387
0.0005908311562692419
0.0004331351283326512
0.0008989343076185227
0.00040481710410941474
0.0009450068891430205
0.00028729578771162777
0.0005316059293060486
0.0008316539693623781
0.00019773065851998246
0.000813809221761082
0.0004419927155301112
0.00039242334801551455
0.0005786861901080036
0.0002918125529605378
0.0007886836661782582
0.0002736323988167843
0.000652847493711306
0.0007140540610635071
0.00044716170505125773
0.0004328880915636546
0.0014173771578498418
0.0006964332751522306
0.0004155358503088917
0.00040891808637989806
0.0007063490179280052
Solved 160/200 episodes
0.0005021535616810996
Evaluated model in 41.1 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-17
Round 18
Generated trajectories in 108.3 seconds
epoch: 0	train loss: 8136.65869140625	(31.7s)
epoch: 1	train loss: 5361.9404296875	(31.8s)
epoch: 2	train loss: 5867.3828125	(31.8s)
epoch: 3	train loss: 4823.0673828125	(31.7s)
epoch: 4	train loss: 4287.47119140625	(31.2s)
epoch: 5	train loss: 3804.44189453125	(31.8s)
epoch: 6	train loss: 4677.1015625	(31.9s)
epoch: 7	train loss: 5132.5	(31.8s)
epoch: 8	train loss: 3845.5830078125	(31.9s)
epoch: 9	train loss: 5086.193359375	(32.0s)
epoch: 10	train loss: 3483.90869140625	(31.8s)
epoch: 11	train loss: 3443.36865234375	(31.4s)
epoch: 12	train loss: 3608.946533203125	(31.0s)
epoch: 13	train loss: 3333.588623046875	(31.7s)
epoch: 14	train loss: 3128.32177734375	(31.9s)
epoch: 15	train loss: 2828.02880859375	(31.8s)
epoch: 16	train loss: 2680.02001953125	(31.9s)
epoch: 17	train loss: 2840.13671875	(31.8s)
epoch: 18	train loss: 3073.614990234375	(31.8s)
epoch: 19	train loss: 4961.21533203125	(32.1s)
epoch: 20	train loss: 4831.59228515625	(31.7s)
epoch: 21	train loss: 2978.802001953125	(31.0s)
epoch: 22	train loss: 3053.968505859375	(30.8s)
epoch: 23	train loss: 4520.16845703125	(31.6s)
epoch: 24	train loss: 4591.93798828125	(32.1s)
epoch: 25	train loss: 2964.77294921875	(31.8s)
epoch: 26	train loss: 3157.380615234375	(31.9s)
epoch: 27	train loss: 3151.197021484375	(31.8s)
epoch: 28	train loss: 4356.9697265625	(32.1s)
epoch: 29	train loss: 2739.9013671875	(32.0s)
epoch: 30	train loss: 2461.060546875	(32.0s)
epoch: 31	train loss: 3950.58642578125	(31.9s)
epoch: 32	train loss: 3416.5615234375	(30.9s)
epoch: 33	train loss: 3519.0517578125	(30.9s)
epoch: 34	train loss: 3183.104736328125	(31.2s)
epoch: 35	train loss: 2905.171142578125	(31.8s)
epoch: 36	train loss: 5741.6845703125	(31.9s)
epoch: 37	train loss: 3042.13525390625	(31.9s)
epoch: 38	train loss: 3083.556396484375	(31.9s)
epoch: 39	train loss: 2393.912109375	(31.9s)
epoch: 40	train loss: 2308.57861328125	(31.8s)
epoch: 41	train loss: 2392.805908203125	(31.9s)
epoch: 42	train loss: 2248.443115234375	(32.0s)
epoch: 43	train loss: 4228.9921875	(32.1s)
epoch: 44	train loss: 2980.310791015625	(31.4s)
epoch: 45	train loss: 2602.8994140625	(30.9s)
epoch: 46	train loss: 2205.4921875	(31.0s)
epoch: 47	train loss: 2058.760986328125	(31.3s)
epoch: 48	train loss: 3974.43212890625	(31.7s)
epoch: 49	train loss: 2573.58349609375	(31.9s)
epoch: 50	train loss: 3455.056884765625	(31.8s)
epoch: 51	train loss: 3824.574462890625	(32.0s)
epoch: 52	train loss: 3215.7734375	(31.7s)
epoch: 53	train loss: 2347.1416015625	(32.1s)
epoch: 54	train loss: 2178.07421875	(31.9s)
epoch: 55	train loss: 2019.908447265625	(31.9s)
epoch: 56	train loss: 2204.433349609375	(31.9s)
epoch: 57	train loss: 4046.228515625	(31.8s)
epoch: 58	train loss: 4773.044921875	(31.1s)
epoch: 59	train loss: 3125.90625	(31.0s)
epoch: 60	train loss: 2721.67138671875	(30.9s)
epoch: 61	train loss: 3131.434814453125	(31.4s)
epoch: 62	train loss: 2277.046142578125	(31.5s)
epoch: 63	train loss: 2238.326416015625	(30.8s)
epoch: 64	train loss: 2861.901611328125	(31.0s)
epoch: 65	train loss: 3393.793701171875	(30.8s)
epoch: 66	train loss: 2246.000244140625	(31.0s)
epoch: 67	train loss: 1905.2408447265625	(30.8s)
epoch: 68	train loss: 3638.494140625	(30.9s)
epoch: 69	train loss: 2098.64501953125	(30.9s)
epoch: 70	train loss: 1797.3592529296875	(30.7s)
epoch: 71	train loss: 2576.1943359375	(30.8s)
epoch: 72	train loss: 2525.875	(30.9s)
epoch: 73	train loss: 2677.43994140625	(30.8s)
epoch: 74	train loss: 2009.775634765625	(30.8s)
epoch: 75	train loss: 2160.246826171875	(30.7s)
epoch: 76	train loss: 6132.1376953125	(30.9s)
epoch: 77	train loss: 3324.460693359375	(30.8s)
epoch: 78	train loss: 2232.83251953125	(30.6s)
epoch: 79	train loss: 2074.46533203125	(30.6s)
epoch: 80	train loss: 2417.548828125	(30.6s)
epoch: 81	train loss: 1950.5880126953125	(30.8s)
epoch: 82	train loss: 3924.934326171875	(30.7s)
epoch: 83	train loss: 3969.79052734375	(30.8s)
epoch: 84	train loss: 2776.649169921875	(30.9s)
epoch: 85	train loss: 1978.5113525390625	(31.0s)
epoch: 86	train loss: 1919.965576171875	(30.9s)
epoch: 87	train loss: 2209.156005859375	(30.9s)
epoch: 88	train loss: 1983.443359375	(30.7s)
epoch: 89	train loss: 1658.1224365234375	(30.8s)
epoch: 90	train loss: 1704.9384765625	(30.8s)
epoch: 91	train loss: 1658.93505859375	(30.8s)
epoch: 92	train loss: 2363.170166015625	(30.9s)
epoch: 93	train loss: 5114.94873046875	(30.8s)
epoch: 94	train loss: 2888.102294921875	(30.9s)
epoch: 95	train loss: 2314.9404296875	(30.9s)
epoch: 96	train loss: 2039.0704345703125	(30.8s)
epoch: 97	train loss: 1983.9632568359375	(30.8s)
epoch: 98	train loss: 1870.253173828125	(30.8s)
epoch: 99	train loss: 1722.23486328125	(30.8s)
Evaluating model on 200 episodes
0.0011699454433469595
0.0005294743649655073
0.0006417869327271867
0.001177049257811242
0.0006682524559033078
0.0007003673994264458
0.000581534380762605
0.0005965072770257913
0.00069499687362457
0.0007383570987258281
0.0005214730186708039
0.00067103362059296
0.0003565441443656296
0.0012628774245306638
0.0004794154068804346
0.0007231390251642248
0.0006497083340946119
0.0008806196437944891
0.000660168982655953
0.001363969389124329
0.0007781296649640849
0.0009581053673173301
0.0006701691953215535
0.0005659866771814202
0.0008495601142875295
0.0007909699208819523
0.0008750439049680819
0.001411502243093349
0.0011117124491978427
0.0011469542027736594
0.0006936779870805119
0.0009883057086881308
0.000803843369220359
0.0001933248533367243
0.0006214598894154693
0.0008702904202177056
0.001691516310809395
0.0011100819208170127
0.0007430846581839779
0.0014002416868276089
0.00046922058855771966
0.0004546943627329628
0.0006752215259439254
0.0005148168895728937
0.0009589223668033652
0.0008028850858055295
0.000555890964986045
0.0018646749580511824
0.001266419156775392
0.0008472460776829394
0.0011937967953296418
0.0016654969182733719
0.0010209292633685675
0.0005611474104003527
0.001252623732398206
0.0007127464238389317
0.0005858505529057071
0.0008300212096153039
0.001095141034446507
0.0007298613568959159
0.0015011936295195483
0.0005865131778674285
0.0014719777301996312
0.0007022757127591225
0.0007052014767850778
0.0011606377919088117
0.0006543412937389803
0.0008504895764807381
0.000761998289362964
0.0008491220531378412
0.0007359615351560933
0.0010266226573776294
0.0003384211617230903
0.0005368317962772077
0.0005861042915663953
0.0009360821845803002
0.0002760343530098908
0.0009597559998694219
0.0009223027427651687
0.0006358715998228743
0.001435744059158929
0.0024283000861327606
0.00045373896455203067
0.001151870526803617
0.0007734750502362765
0.000612844119023066
0.001081516185617407
0.0011780785504053408
0.0005185107346572776
0.0008996196086039109
0.0005427924864788485
0.0009593724369854802
0.000279870489716164
0.0008770985459705116
0.0006375474136306862
0.001148801584609568
0.0007834277475922136
0.0008458945067862563
0.0009244675645984696
0.001004820303478482
0.001198221112038785
0.001075506991535229
0.0004513112213317072
0.0017308012074036842
0.0005369648406485794
0.0005670287025623111
0.0008580960893596057
0.0009363614074324935
0.0002955465845713791
0.0007010091952300476
0.001464598321439957
0.0008972701677218234
0.0006100697943717629
0.0007069735640167588
0.002001441194519621
0.0009375794738843979
0.0010891672350226145
0.0018643369094206719
0.0011055675758353362
0.000890764511495945
0.0006625493457249832
0.00027877624688699143
0.0004904538271568107
0.0006985539021115983
0.0006800053452025168
0.0009380766753565694
0.001218372453747309
0.001020075015352025
0.001123546873507373
0.000669274127443821
0.001905585065590761
0.0006315153343621205
0.0008987520639493595
0.00078856144994331
0.0005506553604521157
0.0008065809034514646
0.0004310370233205114
0.0005644263065960331
0.001218285530870032
0.0007123395654438671
0.0007129233793724173
0.001125230554771406
0.00025577950542686847
0.00076546140085806
0.0010133851982296303
0.0010892312857322395
0.001296890718962955
0.0005243387845089623
0.0005578079036846854
0.0007332130169680599
0.00042481325321065157
0.001196960100060096
0.0005421096895588562
0.0008297014885881383
0.0016475993250943638
0.0008420982969710167
0.0005870593751069464
0.0011519495450556861
0.0008305377493473186
0.0009192863332758022
0.00042661702822702603
0.0008579441156446895
0.0005113496893803434
0.0007591616805783767
0.0003514860579040639
0.0006562168424958751
0.0008701004196717847
0.0007156709084133931
0.0006712035937281061
0.0005857989244759665
0.0006534696804010309
0.0009666925795430031
0.0008155048310274853
0.00117193255081314
0.0006636302570970858
0.0009692604162410134
0.0005643621455722799
0.0008942342341296829
0.00041635188939504917
0.0006148883281235208
Solved 180/200 episodes
0.0007676130465303326
Evaluated model in 41.0 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-18
Round 19
Generated trajectories in 103.2 seconds
epoch: 0	train loss: 7031.72802734375	(30.8s)
epoch: 1	train loss: 5199.0830078125	(30.9s)
epoch: 2	train loss: 4003.988525390625	(31.0s)
epoch: 3	train loss: 3831.20361328125	(31.1s)
epoch: 4	train loss: 4346.66650390625	(31.1s)
epoch: 5	train loss: 3587.72119140625	(31.0s)
epoch: 6	train loss: 5184.12158203125	(31.1s)
epoch: 7	train loss: 3961.645263671875	(31.1s)
epoch: 8	train loss: 4952.212890625	(31.1s)
epoch: 9	train loss: 3124.193115234375	(31.1s)
epoch: 10	train loss: 3071.080810546875	(31.0s)
epoch: 11	train loss: 3290.014404296875	(31.0s)
epoch: 12	train loss: 3894.82568359375	(31.1s)
epoch: 13	train loss: 3652.208984375	(31.1s)
epoch: 14	train loss: 2970.4462890625	(31.1s)
epoch: 15	train loss: 3266.107666015625	(31.0s)
epoch: 16	train loss: 2717.5166015625	(31.1s)
epoch: 17	train loss: 2545.092041015625	(31.1s)
epoch: 18	train loss: 3025.071533203125	(31.1s)
epoch: 19	train loss: 2973.62060546875	(31.1s)
epoch: 20	train loss: 2388.537109375	(31.1s)
epoch: 21	train loss: 2406.343505859375	(31.1s)
epoch: 22	train loss: 2277.977294921875	(31.1s)
epoch: 23	train loss: 2426.12744140625	(31.0s)
epoch: 24	train loss: 2258.337158203125	(31.1s)
epoch: 25	train loss: 4064.250244140625	(31.0s)
epoch: 26	train loss: 3854.4892578125	(31.2s)
epoch: 27	train loss: 3485.354248046875	(31.2s)
epoch: 28	train loss: 4281.5380859375	(31.0s)
epoch: 29	train loss: 2648.116943359375	(31.1s)
epoch: 30	train loss: 2229.98291015625	(31.1s)
epoch: 31	train loss: 2153.0693359375	(31.1s)
epoch: 32	train loss: 2693.20849609375	(31.1s)
epoch: 33	train loss: 3079.58154296875	(31.1s)
epoch: 34	train loss: 3065.6142578125	(31.1s)
epoch: 35	train loss: 4015.974365234375	(31.1s)
epoch: 36	train loss: 3185.383056640625	(31.1s)
epoch: 37	train loss: 2630.61279296875	(31.1s)
epoch: 38	train loss: 2566.259033203125	(31.0s)
epoch: 39	train loss: 3126.89697265625	(31.1s)
epoch: 40	train loss: 2477.58203125	(31.1s)
epoch: 41	train loss: 2517.951416015625	(31.1s)
epoch: 42	train loss: 5041.34765625	(31.1s)
epoch: 43	train loss: 3516.972412109375	(31.0s)
epoch: 44	train loss: 2210.097900390625	(31.1s)
epoch: 45	train loss: 2592.283935546875	(31.1s)
epoch: 46	train loss: 2456.454833984375	(31.0s)
epoch: 47	train loss: 2721.1474609375	(31.1s)
epoch: 48	train loss: 2667.526123046875	(31.0s)
epoch: 49	train loss: 2954.106689453125	(31.1s)
epoch: 50	train loss: 2417.681640625	(31.2s)
epoch: 51	train loss: 2971.130859375	(31.0s)
epoch: 52	train loss: 3826.86279296875	(31.1s)
epoch: 53	train loss: 2546.357421875	(31.1s)
epoch: 54	train loss: 2480.465576171875	(31.1s)
epoch: 55	train loss: 2378.021728515625	(31.1s)
epoch: 56	train loss: 3493.9404296875	(31.0s)
epoch: 57	train loss: 2631.697265625	(31.2s)
epoch: 58	train loss: 2871.903564453125	(31.1s)
epoch: 59	train loss: 3095.871826171875	(31.1s)
epoch: 60	train loss: 2264.610595703125	(31.1s)
epoch: 61	train loss: 2500.1181640625	(31.0s)
epoch: 62	train loss: 2030.444580078125	(31.2s)
epoch: 63	train loss: 1767.64794921875	(31.2s)
epoch: 64	train loss: 2459.4013671875	(31.1s)
epoch: 65	train loss: 2204.7880859375	(31.1s)
epoch: 66	train loss: 2182.11865234375	(31.0s)
epoch: 67	train loss: 3258.263916015625	(31.1s)
epoch: 68	train loss: 2304.509765625	(31.2s)
epoch: 69	train loss: 3001.9287109375	(31.1s)
epoch: 70	train loss: 3612.67578125	(31.1s)
epoch: 71	train loss: 2375.34130859375	(31.0s)
epoch: 72	train loss: 1888.4915771484375	(31.1s)
epoch: 73	train loss: 2180.245361328125	(31.1s)
epoch: 74	train loss: 2217.23681640625	(31.0s)
epoch: 75	train loss: 2834.385009765625	(31.1s)
epoch: 76	train loss: 2276.411376953125	(31.1s)
epoch: 77	train loss: 2026.783935546875	(31.0s)
epoch: 78	train loss: 2502.876708984375	(31.0s)
epoch: 79	train loss: 2187.377685546875	(30.6s)
epoch: 80	train loss: 2249.83154296875	(30.8s)
epoch: 81	train loss: 1808.042724609375	(30.7s)
epoch: 82	train loss: 1608.9212646484375	(30.7s)
epoch: 83	train loss: 1552.11474609375	(30.7s)
epoch: 84	train loss: 2779.729248046875	(30.6s)
epoch: 85	train loss: 3669.747802734375	(30.7s)
epoch: 86	train loss: 4962.1298828125	(30.7s)
epoch: 87	train loss: 3358.9912109375	(30.7s)
epoch: 88	train loss: 2674.14990234375	(30.7s)
epoch: 89	train loss: 2394.22265625	(30.7s)
epoch: 90	train loss: 3070.15380859375	(30.8s)
epoch: 91	train loss: 2600.598876953125	(30.7s)
epoch: 92	train loss: 2001.382568359375	(30.6s)
epoch: 93	train loss: 1689.880859375	(30.7s)
epoch: 94	train loss: 2763.92822265625	(30.7s)
epoch: 95	train loss: 2570.529541015625	(30.7s)
epoch: 96	train loss: 2181.239013671875	(30.8s)
epoch: 97	train loss: 3168.74951171875	(30.7s)
epoch: 98	train loss: 1963.6107177734375	(30.7s)
epoch: 99	train loss: 1873.2930908203125	(30.7s)
Evaluating model on 200 episodes
0.0010567701338732149
0.000871879044243542
0.0005799795857860267
0.00037830085520082236
0.0005863013086582214
0.0008921243970689829
0.0017599788440169175
0.00093190492667158
0.0003101253066688514
0.0011555184646567795
0.0007156691858004446
0.0009739111343757637
0.0008101608187384304
0.0010621582029671118
0.0012788976487439975
0.0007128476914658677
0.0007080893182622579
0.0005632127669358804
0.0010480732817086391
0.0007078426831867545
0.0011886695865541697
0.0014067348165553994
0.0004276090704396748
0.0003495826201217894
0.0003455161285172734
0.0005097401374639178
0.0005165820670451163
0.0004710251441792934
0.000722921330221156
0.0009977401878922651
0.0003098157924910083
0.0009199857872970271
0.0006735145319908042
0.0008898955438780831
0.0009137744871493071
0.0012890997073554899
0.0005929179925563706
0.0005353221226151187
0.0007633943280072546
0.0011377696432949354
0.0014550910909179038
0.0006241500109767499
0.0005549198508147542
0.0008452066940662917
0.0006317887887740249
0.0006052404264664801
0.0004842857610388559
0.0006290590374853557
0.000997462887880829
0.0006965886338245278
0.0018547292784205638
0.0009687251571276214
0.001127910177638114
0.0007797142441429767
0.0007094183424669609
0.0010547670428003096
0.001042685247512054
0.0011678665354490174
0.0005588529766966877
0.0007421272961544353
0.0004215407635846142
0.001223541068331822
0.000738812458581853
0.0009530464479515407
0.0007508521414112895
0.0007525242738969004
0.0005358731734584301
0.0007252224001734119
0.0009121883047858622
0.0012976352032716893
0.0009299585567153675
0.0008432826882860602
0.0005381593840259787
0.0010127192674644903
0.0006164964839519919
0.0018434811875825592
0.0013366915425814857
0.0005469226436451373
0.00036826141094934426
0.0015552402854458098
0.0013588730764561607
0.0007668534453841858
0.001408806013950241
0.0008926681791005345
0.000649892618563778
0.0005135264488368799
0.0006363915866813841
0.0012078433086344376
0.00023562600335935713
0.0009504634632652266
0.001138942658144515
0.001127713640547275
0.001112215925887641
0.0007123531399234448
0.0006606428731174673
0.0004099287885505224
0.0010809660180593314
0.0002087081676336311
0.0005906220832050741
0.0005560120163055318
0.0007756626273476286
0.0009788137257074642
0.0009660433064482277
0.0020765879582540945
0.00039313192226270784
0.0005696114116694338
0.0005287632572747368
0.001039119239206999
0.0008289679644803982
0.0008381319152148434
0.0006765611475808934
0.001086641140257901
0.0006411074637071579
0.0009374721480793703
0.0005690305425863092
0.0009345291064732919
0.0006032248675182927
0.0005193060350090188
0.0009510750720177644
0.0009818203279792215
0.0010797419917627634
0.0012365096719440772
0.0005541321975215396
0.000797369086285471
0.0012921365746828646
0.000657232244851581
0.0005417336033133324
0.0010293810582879814
0.0002985942771374539
0.0006319859690847807
0.0010089603619074777
0.0006055104402968517
0.0005718948671831944
0.0005733255355961672
0.0006723753285768908
0.001283876133660063
0.0013662987928980258
0.0007576244855348099
0.00030143488111207263
0.0002573440845232901
0.0013127131116258777
0.0003238782270166078
0.0007764964540911024
0.0009181970836531643
0.0007545290148603575
0.00041782008682073664
0.0010412625887283866
0.0008848406019435218
0.0019383647304493934
0.0010007964751821419
0.0014313294044162224
0.0005540479178307578
0.0005595850529971358
0.0005453179829702196
0.0014119873327823977
0.00031835589425099897
0.0008205832655221457
0.0006769894189933586
0.0003440573923398915
0.0011437847451816197
0.0005510306186088679
0.0006044902971552802
0.0006272542052422924
0.0004749075148873472
0.0010760691519635657
0.0005877775952017948
0.001138668437811753
0.0006335445772265302
0.0008979487502074335
0.000530013029856491
0.0008922103306152469
0.0008211678417637813
0.0007907017037983419
0.0007708166150324461
0.00038690317581891907
0.0003004557081242508
0.0005513802930831194
0.00035967854481506426
0.00041339462525987375
0.0011307733013078785
0.000693357924077039
0.0011582887194890646
Solved 182/200 episodes
0.0007434813887762558
Evaluated model in 39.2 seconds
Saved model for run
17b03a7bc7b3478ebcb1ad3e7c49ea34 with name round-19
Completed training in 65899.6 seconds
Solved 34/200 episodes
Solved 32/200 episodes
Solved 37/200 episodes
Solved 46/200 episodes
Solved 47/200 episodes
Solved 52/200 episodes
Solved 73/200 episodes
Solved 62/200 episodes
Solved 83/200 episodes
Solved 87/200 episodes
Solved 109/200 episodes
Solved 128/200 episodes
Solved 119/200 episodes
Solved 141/200 episodes
Solved 146/200 episodes
Solved 156/200 episodes
Solved 164/200 episodes
Solved 160/200 episodes
Solved 180/200 episodes
Solved 182/200 episodes
