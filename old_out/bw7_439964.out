Using torch device NVIDIA GeForce RTX 3090
net: causal
cc_weight: 1.0
abstract_pen: 0.1
params: {'epochs': 100, 'lr': 0.0008, 'n': 5000}
Net has 207442 parameters
Round 0
Generated trajectories in 72.0 seconds
epoch: 0	train loss: 150627.53125	(17.2s)
epoch: 1	train loss: 146169.46875	(16.1s)
epoch: 2	train loss: 135183.71875	(16.1s)
epoch: 3	train loss: 99193.6875	(16.1s)
epoch: 4	train loss: 86355.453125	(16.1s)
epoch: 5	train loss: 76039.359375	(16.1s)
epoch: 6	train loss: 69185.2421875	(16.1s)
epoch: 7	train loss: 65190.59375	(16.1s)
epoch: 8	train loss: 61423.20703125	(16.1s)
epoch: 9	train loss: 58051.7265625	(16.1s)
epoch: 10	train loss: 55549.3515625	(16.1s)
epoch: 11	train loss: 52856.609375	(16.1s)
epoch: 12	train loss: 51259.5703125	(16.1s)
epoch: 13	train loss: 50261.68359375	(16.1s)
epoch: 14	train loss: 49374.734375	(16.1s)
epoch: 15	train loss: 47665.91796875	(16.1s)
epoch: 16	train loss: 46921.203125	(16.1s)
epoch: 17	train loss: 46295.19140625	(16.1s)
epoch: 18	train loss: 45365.0703125	(16.1s)
epoch: 19	train loss: 44855.11328125	(16.2s)
epoch: 20	train loss: 44271.515625	(16.1s)
epoch: 21	train loss: 43952.75	(16.1s)
epoch: 22	train loss: 43268.94921875	(16.1s)
epoch: 23	train loss: 43430.95703125	(16.1s)
epoch: 24	train loss: 43049.1640625	(16.1s)
epoch: 25	train loss: 43027.13671875	(16.1s)
epoch: 26	train loss: 41679.94921875	(16.1s)
epoch: 27	train loss: 40959.6953125	(16.1s)
epoch: 28	train loss: 42719.71875	(16.1s)
epoch: 29	train loss: 41243.78515625	(16.1s)
epoch: 30	train loss: 39995.78125	(16.0s)
epoch: 31	train loss: 39635.140625	(16.0s)
epoch: 32	train loss: 39426.6328125	(16.0s)
epoch: 33	train loss: 38626.1484375	(16.0s)
epoch: 34	train loss: 38612.96875	(16.1s)
epoch: 35	train loss: 39408.05078125	(16.1s)
epoch: 36	train loss: 39645.046875	(16.1s)
epoch: 37	train loss: 38308.265625	(16.1s)
epoch: 38	train loss: 39249.87109375	(16.1s)
epoch: 39	train loss: 38281.5	(16.1s)
epoch: 40	train loss: 37817.03125	(16.1s)
epoch: 41	train loss: 37836.828125	(16.1s)
epoch: 42	train loss: 38407.1953125	(16.1s)
epoch: 43	train loss: 37229.65234375	(16.1s)
epoch: 44	train loss: 40969.2421875	(16.1s)
epoch: 45	train loss: 40231.5859375	(16.1s)
epoch: 46	train loss: 38225.75390625	(16.1s)
epoch: 47	train loss: 36821.65625	(16.1s)
epoch: 48	train loss: 39418.4921875	(16.1s)
epoch: 49	train loss: 36019.203125	(16.1s)
epoch: 50	train loss: 35372.62109375	(16.1s)
epoch: 51	train loss: 35169.0625	(16.1s)
epoch: 52	train loss: 36184.484375	(16.2s)
epoch: 53	train loss: 35135.7890625	(16.1s)
epoch: 54	train loss: 34640.55078125	(16.1s)
epoch: 55	train loss: 34223.24609375	(16.1s)
epoch: 56	train loss: 34801.671875	(16.1s)
epoch: 57	train loss: 37636.609375	(16.1s)
epoch: 58	train loss: 37322.0546875	(16.1s)
epoch: 59	train loss: 36850.578125	(16.1s)
epoch: 60	train loss: 35468.84765625	(16.1s)
epoch: 61	train loss: 36585.921875	(16.1s)
epoch: 62	train loss: 36177.84375	(16.2s)
epoch: 63	train loss: 35894.63671875	(16.1s)
epoch: 64	train loss: 37449.40234375	(16.1s)
epoch: 65	train loss: 34499.75	(16.1s)
epoch: 66	train loss: 34117.41796875	(16.1s)
epoch: 67	train loss: 33743.46484375	(16.1s)
epoch: 68	train loss: 33851.40234375	(16.1s)
epoch: 69	train loss: 35322.8359375	(16.1s)
epoch: 70	train loss: 34692.51953125	(16.1s)
epoch: 71	train loss: 34160.27734375	(16.2s)
epoch: 72	train loss: 33846.9453125	(16.1s)
epoch: 73	train loss: 37069.27734375	(16.1s)
epoch: 74	train loss: 34191.94140625	(16.1s)
epoch: 75	train loss: 33424.71484375	(16.1s)
epoch: 76	train loss: 35081.0078125	(16.1s)
epoch: 77	train loss: 37257.390625	(16.1s)
epoch: 78	train loss: 34893.0859375	(16.1s)
epoch: 79	train loss: 33254.71875	(16.2s)
epoch: 80	train loss: 32955.7109375	(16.1s)
epoch: 81	train loss: 32155.927734375	(16.1s)
epoch: 82	train loss: 32373.126953125	(16.1s)
epoch: 83	train loss: 31837.29296875	(16.1s)
epoch: 84	train loss: 31763.1640625	(16.1s)
epoch: 85	train loss: 30615.22265625	(16.1s)
epoch: 86	train loss: 30292.03515625	(16.1s)
epoch: 87	train loss: 30197.1484375	(16.1s)
epoch: 88	train loss: 34435.3359375	(16.1s)
epoch: 89	train loss: 30251.3359375	(16.1s)
epoch: 90	train loss: 31270.259765625	(16.1s)
epoch: 91	train loss: 32042.587890625	(16.1s)
epoch: 92	train loss: 34630.54296875	(16.2s)
epoch: 93	train loss: 32600.076171875	(16.1s)
epoch: 94	train loss: 32126.05078125	(16.1s)
epoch: 95	train loss: 32251.787109375	(16.1s)
epoch: 96	train loss: 30441.748046875	(16.1s)
epoch: 97	train loss: 29520.8046875	(16.1s)
epoch: 98	train loss: 30265.216796875	(16.1s)
epoch: 99	train loss: 29382.134765625	(16.1s)
Evaluating model on 200 episodes
0.0017564844783919398
0.0009196101418638136
0.002440830773723844
0.002159861624353322
0.0031505023077140018
0.000657327938824892
0.0015806536466698162
0.005666055018082261
0.0029445596888503394
0.001821301806921838
0.0009154402848383013
0.0009502698922005948
0.001353912716922423
0.00325737476341601
0.0034573602694893324
0.0004922607772641641
0.0025048647468793204
0.0013562090181646151
0.004658892397856107
0.00011928666450936969
0.0013884929459891282
0.0015690092768636532
0.0012090030710775561
0.0018337410013676465
0.0012788139766516078
0.004275621417892741
0.006881277052646813
0.0009257838764824555
0.0033185285865329206
0.002223999727513209
0.0009134702777373604
0.005058746662851061
0.0013132002597558313
Solved 33/200 episodes
0.0003717637354514913
Evaluated model in 21.2 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-0
Round 1
Generated trajectories in 71.9 seconds
epoch: 0	train loss: 41217.484375	(16.2s)
epoch: 1	train loss: 39162.8046875	(16.0s)
epoch: 2	train loss: 36906.6328125	(16.0s)
epoch: 3	train loss: 35346.41796875	(16.0s)
epoch: 4	train loss: 35388.75390625	(16.1s)
epoch: 5	train loss: 36245.15625	(16.0s)
epoch: 6	train loss: 34083.82421875	(16.1s)
epoch: 7	train loss: 32569.603515625	(16.1s)
epoch: 8	train loss: 34501.98828125	(16.1s)
epoch: 9	train loss: 32010.49609375	(16.1s)
epoch: 10	train loss: 30946.353515625	(16.0s)
epoch: 11	train loss: 31381.326171875	(16.1s)
epoch: 12	train loss: 31121.572265625	(16.1s)
epoch: 13	train loss: 32479.3828125	(16.1s)
epoch: 14	train loss: 30380.337890625	(16.1s)
epoch: 15	train loss: 31940.87109375	(16.0s)
epoch: 16	train loss: 30688.08203125	(16.1s)
epoch: 17	train loss: 29580.04296875	(16.1s)
epoch: 18	train loss: 29142.375	(16.1s)
epoch: 19	train loss: 29892.158203125	(16.1s)
epoch: 20	train loss: 29929.24609375	(16.0s)
epoch: 21	train loss: 29647.923828125	(16.1s)
epoch: 22	train loss: 29270.361328125	(16.1s)
epoch: 23	train loss: 28854.5625	(16.0s)
epoch: 24	train loss: 28689.67578125	(15.9s)
epoch: 25	train loss: 28558.890625	(15.8s)
epoch: 26	train loss: 28140.138671875	(15.9s)
epoch: 27	train loss: 28368.775390625	(15.9s)
epoch: 28	train loss: 27892.53515625	(15.9s)
epoch: 29	train loss: 28163.521484375	(15.9s)
epoch: 30	train loss: 29322.0	(15.9s)
epoch: 31	train loss: 29406.7578125	(15.9s)
epoch: 32	train loss: 31606.47265625	(15.9s)
epoch: 33	train loss: 30427.8828125	(15.9s)
epoch: 34	train loss: 28507.728515625	(15.9s)
epoch: 35	train loss: 27294.787109375	(15.9s)
epoch: 36	train loss: 26835.0234375	(15.9s)
epoch: 37	train loss: 26304.37890625	(15.9s)
epoch: 38	train loss: 28670.16015625	(15.9s)
epoch: 39	train loss: 27544.98046875	(15.9s)
epoch: 40	train loss: 26697.408203125	(15.9s)
epoch: 41	train loss: 25708.80859375	(15.9s)
epoch: 42	train loss: 25677.828125	(15.9s)
epoch: 43	train loss: 25793.12109375	(15.9s)
epoch: 44	train loss: 26406.794921875	(15.9s)
epoch: 45	train loss: 25590.033203125	(15.9s)
epoch: 46	train loss: 25443.1328125	(15.9s)
epoch: 47	train loss: 25624.921875	(15.9s)
epoch: 48	train loss: 25832.779296875	(15.9s)
epoch: 49	train loss: 25987.12109375	(15.9s)
epoch: 50	train loss: 25676.73046875	(15.9s)
epoch: 51	train loss: 25033.29296875	(15.9s)
epoch: 52	train loss: 24517.89453125	(15.9s)
epoch: 53	train loss: 24452.0546875	(15.9s)
epoch: 54	train loss: 24175.458984375	(15.9s)
epoch: 55	train loss: 25528.193359375	(15.9s)
epoch: 56	train loss: 26341.587890625	(15.9s)
epoch: 57	train loss: 24862.74609375	(15.9s)
epoch: 58	train loss: 25119.259765625	(15.9s)
epoch: 59	train loss: 25184.75	(15.9s)
epoch: 60	train loss: 25493.380859375	(15.9s)
epoch: 61	train loss: 24884.79296875	(15.9s)
epoch: 62	train loss: 24384.033203125	(15.9s)
epoch: 63	train loss: 24106.474609375	(15.9s)
epoch: 64	train loss: 23243.943359375	(15.9s)
epoch: 65	train loss: 23174.4453125	(15.9s)
epoch: 66	train loss: 22674.09765625	(15.9s)
epoch: 67	train loss: 22604.095703125	(15.9s)
epoch: 68	train loss: 22699.40234375	(15.9s)
epoch: 69	train loss: 22588.970703125	(15.9s)
epoch: 70	train loss: 22445.22265625	(15.9s)
epoch: 71	train loss: 22164.150390625	(15.9s)
epoch: 72	train loss: 22302.62109375	(15.9s)
epoch: 73	train loss: 22541.830078125	(15.9s)
epoch: 74	train loss: 23315.814453125	(15.9s)
epoch: 75	train loss: 22015.990234375	(15.9s)
epoch: 76	train loss: 21866.75	(15.9s)
epoch: 77	train loss: 21676.169921875	(15.9s)
epoch: 78	train loss: 21529.8515625	(15.9s)
epoch: 79	train loss: 21469.126953125	(15.9s)
epoch: 80	train loss: 23111.1015625	(15.9s)
epoch: 81	train loss: 21897.478515625	(15.9s)
epoch: 82	train loss: 21536.84375	(15.9s)
epoch: 83	train loss: 21764.369140625	(15.9s)
epoch: 84	train loss: 21564.84375	(15.9s)
epoch: 85	train loss: 21718.275390625	(15.9s)
epoch: 86	train loss: 22053.0546875	(15.9s)
epoch: 87	train loss: 22015.435546875	(15.9s)
epoch: 88	train loss: 22479.166015625	(15.9s)
epoch: 89	train loss: 22556.693359375	(15.9s)
epoch: 90	train loss: 22535.841796875	(15.9s)
epoch: 91	train loss: 23584.23828125	(15.9s)
epoch: 92	train loss: 22521.994140625	(15.9s)
epoch: 93	train loss: 22261.533203125	(15.9s)
epoch: 94	train loss: 21915.53515625	(15.9s)
epoch: 95	train loss: 21094.12109375	(15.9s)
epoch: 96	train loss: 21362.69921875	(15.9s)
epoch: 97	train loss: 21061.41796875	(15.9s)
epoch: 98	train loss: 20844.439453125	(15.9s)
epoch: 99	train loss: 22085.048828125	(15.9s)
Evaluating model on 200 episodes
0.002023805381274239
0.00042099366772971634
0.001390369641335888
0.0064778444793773815
0.0027137020661029965
0.0006170301243805119
0.0017911254246428144
0.0019805390760926195
0.003150581316125075
0.0015837653730462382
0.0006394839013713257
0.00126164518304803
0.00047878184098711546
0.0016735107656131731
0.008906690714770536
0.00038531858590431514
0.0037428166672018254
0.012164893289536849
0.0025510846047230493
0.0013935997703811154
0.0007302809804969002
0.000655529748958846
0.0011857259087264538
0.0014812705417820585
0.0023470455956460987
0.0011870334224113321
0.0004809775379147719
0.0003806868664303858
0.002262246237868177
0.0017010647382200907
0.0009645921953571295
0.0007941296341111107
0.0004549997211142909
0.0011102227621854076
0.005070890215898139
Solved 35/200 episodes
0.0003807713899038301
Evaluated model in 22.5 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-1
Round 2
Generated trajectories in 70.6 seconds
epoch: 0	train loss: 45029.2109375	(15.9s)
epoch: 1	train loss: 38823.30078125	(15.9s)
epoch: 2	train loss: 35883.32421875	(15.9s)
epoch: 3	train loss: 34044.125	(16.0s)
epoch: 4	train loss: 32964.3046875	(16.0s)
epoch: 5	train loss: 31938.216796875	(16.0s)
epoch: 6	train loss: 31092.544921875	(16.0s)
epoch: 7	train loss: 30754.78125	(16.0s)
epoch: 8	train loss: 29924.58984375	(16.0s)
epoch: 9	train loss: 29736.314453125	(16.0s)
epoch: 10	train loss: 28937.1875	(16.0s)
epoch: 11	train loss: 28547.580078125	(16.0s)
epoch: 12	train loss: 28432.9296875	(16.0s)
epoch: 13	train loss: 28767.2578125	(16.0s)
epoch: 14	train loss: 27981.685546875	(16.0s)
epoch: 15	train loss: 27427.013671875	(16.0s)
epoch: 16	train loss: 27620.73046875	(16.0s)
epoch: 17	train loss: 26827.806640625	(16.0s)
epoch: 18	train loss: 26485.052734375	(16.0s)
epoch: 19	train loss: 26309.80859375	(16.0s)
epoch: 20	train loss: 26233.08203125	(16.0s)
epoch: 21	train loss: 25950.62109375	(16.0s)
epoch: 22	train loss: 25692.38671875	(16.0s)
epoch: 23	train loss: 25477.46875	(16.0s)
epoch: 24	train loss: 25455.79296875	(16.0s)
epoch: 25	train loss: 25470.431640625	(16.0s)
epoch: 26	train loss: 25279.3515625	(15.9s)
epoch: 27	train loss: 24834.1015625	(16.0s)
epoch: 28	train loss: 24568.408203125	(16.0s)
epoch: 29	train loss: 24491.298828125	(16.0s)
epoch: 30	train loss: 24204.720703125	(16.0s)
epoch: 31	train loss: 23965.583984375	(16.1s)
epoch: 32	train loss: 23961.712890625	(16.0s)
epoch: 33	train loss: 23561.326171875	(16.0s)
epoch: 34	train loss: 23562.435546875	(16.1s)
epoch: 35	train loss: 23141.72265625	(16.1s)
epoch: 36	train loss: 23027.0703125	(16.1s)
epoch: 37	train loss: 23075.365234375	(16.0s)
epoch: 38	train loss: 23060.015625	(16.1s)
epoch: 39	train loss: 23032.359375	(16.1s)
epoch: 40	train loss: 22963.701171875	(16.1s)
epoch: 41	train loss: 23536.38671875	(16.1s)
epoch: 42	train loss: 22560.833984375	(16.0s)
epoch: 43	train loss: 22601.734375	(16.0s)
epoch: 44	train loss: 22243.060546875	(16.1s)
epoch: 45	train loss: 22372.220703125	(16.1s)
epoch: 46	train loss: 22323.857421875	(16.1s)
epoch: 47	train loss: 21785.4609375	(16.0s)
epoch: 48	train loss: 22096.28515625	(16.0s)
epoch: 49	train loss: 21897.376953125	(16.1s)
epoch: 50	train loss: 22151.603515625	(16.1s)
epoch: 51	train loss: 22033.716796875	(16.1s)
epoch: 52	train loss: 21918.548828125	(16.0s)
epoch: 53	train loss: 21864.029296875	(16.0s)
epoch: 54	train loss: 21550.1328125	(16.0s)
epoch: 55	train loss: 22015.712890625	(16.1s)
epoch: 56	train loss: 21363.916015625	(16.1s)
epoch: 57	train loss: 21175.80078125	(16.0s)
epoch: 58	train loss: 20926.099609375	(16.0s)
epoch: 59	train loss: 20846.962890625	(16.0s)
epoch: 60	train loss: 20837.91015625	(16.1s)
epoch: 61	train loss: 20650.421875	(16.1s)
epoch: 62	train loss: 20508.26953125	(16.0s)
epoch: 63	train loss: 20893.14453125	(16.1s)
epoch: 64	train loss: 20454.990234375	(16.0s)
epoch: 65	train loss: 20278.52734375	(16.1s)
epoch: 66	train loss: 20579.23828125	(16.1s)
epoch: 67	train loss: 20332.55859375	(16.1s)
epoch: 68	train loss: 20352.1171875	(16.1s)
epoch: 69	train loss: 20450.775390625	(16.0s)
epoch: 70	train loss: 21620.802734375	(16.1s)
epoch: 71	train loss: 20614.58203125	(16.1s)
epoch: 72	train loss: 20395.609375	(16.1s)
epoch: 73	train loss: 20241.400390625	(16.0s)
epoch: 74	train loss: 19787.25	(16.1s)
epoch: 75	train loss: 19688.15625	(16.1s)
epoch: 76	train loss: 19600.970703125	(16.1s)
epoch: 77	train loss: 19488.935546875	(16.1s)
epoch: 78	train loss: 19409.1796875	(16.1s)
epoch: 79	train loss: 19346.982421875	(16.1s)
epoch: 80	train loss: 19636.169921875	(16.1s)
epoch: 81	train loss: 20984.5859375	(16.1s)
epoch: 82	train loss: 19852.904296875	(16.1s)
epoch: 83	train loss: 19536.70703125	(16.0s)
epoch: 84	train loss: 19323.5625	(16.1s)
epoch: 85	train loss: 19698.98046875	(16.0s)
epoch: 86	train loss: 19544.75	(16.1s)
epoch: 87	train loss: 19518.734375	(16.1s)
epoch: 88	train loss: 19155.759765625	(16.1s)
epoch: 89	train loss: 18882.732421875	(16.1s)
epoch: 90	train loss: 18536.609375	(16.1s)
epoch: 91	train loss: 19625.302734375	(16.1s)
epoch: 92	train loss: 18585.70703125	(16.1s)
epoch: 93	train loss: 18339.701171875	(16.1s)
epoch: 94	train loss: 18375.634765625	(16.1s)
epoch: 95	train loss: 18466.787109375	(16.0s)
epoch: 96	train loss: 18295.533203125	(16.1s)
epoch: 97	train loss: 18276.23828125	(16.1s)
epoch: 98	train loss: 18235.462890625	(16.1s)
epoch: 99	train loss: 19169.12109375	(16.1s)
Evaluating model on 200 episodes
0.001547558468397862
0.0012276703491806984
0.0029963264518301004
0.0038903790546100936
0.0007295181650468814
0.00039774249162292106
0.000989159096207004
0.0009821512510825414
0.001768899275563084
0.001784865571244154
0.00487833070049722
0.0013276928584673442
0.001228992429241771
0.0017883770927558846
0.0016896876319757819
0.0011108122804823022
0.0023773307662590276
0.0024637079250169336
0.005301248606493962
0.0006829619916970842
0.0014381858927663416
0.001818746543274476
0.002467396074886589
0.000526706861169909
0.0020958366124735526
0.002020903493394144
0.0013312490831594915
0.0032852455811475272
0.0031725690224937773
0.0014818424512798498
0.003186804763390683
0.0019161579059852687
0.002406681666408466
0.0016843167092398896
0.0004727220240546068
0.0041840032848995175
0.0032633991246587934
Solved 37/200 episodes
0.0003795808977617776
Evaluated model in 23.5 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-2
Round 3
Generated trajectories in 71.9 seconds
epoch: 0	train loss: 47787.7890625	(16.1s)
epoch: 1	train loss: 40377.00390625	(16.2s)
epoch: 2	train loss: 37021.33984375	(16.2s)
epoch: 3	train loss: 34726.27734375	(16.2s)
epoch: 4	train loss: 33392.09765625	(16.2s)
epoch: 5	train loss: 31846.0546875	(16.2s)
epoch: 6	train loss: 30778.197265625	(16.2s)
epoch: 7	train loss: 30045.529296875	(16.2s)
epoch: 8	train loss: 29275.73828125	(16.2s)
epoch: 9	train loss: 28821.404296875	(16.2s)
epoch: 10	train loss: 28290.61328125	(16.2s)
epoch: 11	train loss: 28270.1953125	(16.2s)
epoch: 12	train loss: 28022.5234375	(16.2s)
epoch: 13	train loss: 27534.265625	(16.2s)
epoch: 14	train loss: 26996.990234375	(16.2s)
epoch: 15	train loss: 26504.57421875	(16.2s)
epoch: 16	train loss: 26264.734375	(16.2s)
epoch: 17	train loss: 26112.65234375	(16.2s)
epoch: 18	train loss: 25628.2578125	(16.2s)
epoch: 19	train loss: 25698.87109375	(16.2s)
epoch: 20	train loss: 25085.146484375	(16.2s)
epoch: 21	train loss: 25084.5390625	(16.2s)
epoch: 22	train loss: 24682.23828125	(16.2s)
epoch: 23	train loss: 24463.90234375	(16.2s)
epoch: 24	train loss: 24551.15234375	(16.2s)
epoch: 25	train loss: 24307.666015625	(16.2s)
epoch: 26	train loss: 24029.822265625	(16.2s)
epoch: 27	train loss: 23554.28125	(16.2s)
epoch: 28	train loss: 23578.9765625	(16.2s)
epoch: 29	train loss: 23488.541015625	(16.2s)
epoch: 30	train loss: 23744.044921875	(16.2s)
epoch: 31	train loss: 23142.34765625	(16.2s)
epoch: 32	train loss: 23240.755859375	(16.2s)
epoch: 33	train loss: 22973.171875	(16.2s)
epoch: 34	train loss: 22848.107421875	(16.2s)
epoch: 35	train loss: 22504.740234375	(16.2s)
epoch: 36	train loss: 22214.8125	(16.2s)
epoch: 37	train loss: 22029.646484375	(16.2s)
epoch: 38	train loss: 22254.294921875	(16.2s)
epoch: 39	train loss: 22608.537109375	(16.2s)
epoch: 40	train loss: 22105.369140625	(16.2s)
epoch: 41	train loss: 21967.416015625	(16.2s)
epoch: 42	train loss: 21543.826171875	(16.2s)
epoch: 43	train loss: 21868.228515625	(16.2s)
epoch: 44	train loss: 21469.681640625	(16.2s)
epoch: 45	train loss: 21285.66015625	(16.2s)
epoch: 46	train loss: 21197.70703125	(16.2s)
epoch: 47	train loss: 21216.849609375	(16.2s)
epoch: 48	train loss: 21337.279296875	(16.2s)
epoch: 49	train loss: 21264.525390625	(16.2s)
epoch: 50	train loss: 21045.107421875	(16.2s)
epoch: 51	train loss: 21190.595703125	(16.2s)
epoch: 52	train loss: 20626.0078125	(16.2s)
epoch: 53	train loss: 20391.48828125	(16.2s)
epoch: 54	train loss: 20624.01953125	(16.2s)
epoch: 55	train loss: 20306.171875	(16.2s)
epoch: 56	train loss: 20510.947265625	(16.2s)
epoch: 57	train loss: 20200.75390625	(16.1s)
epoch: 58	train loss: 19857.826171875	(16.1s)
epoch: 59	train loss: 19750.037109375	(16.2s)
epoch: 60	train loss: 19689.68359375	(16.2s)
epoch: 61	train loss: 19608.642578125	(16.2s)
epoch: 62	train loss: 19596.232421875	(16.2s)
epoch: 63	train loss: 19468.130859375	(16.1s)
epoch: 64	train loss: 19374.220703125	(16.2s)
epoch: 65	train loss: 19509.22265625	(16.2s)
epoch: 66	train loss: 19666.302734375	(16.2s)
epoch: 67	train loss: 19136.986328125	(16.2s)
epoch: 68	train loss: 19172.845703125	(16.2s)
epoch: 69	train loss: 19272.490234375	(16.2s)
epoch: 70	train loss: 19481.61328125	(16.2s)
epoch: 71	train loss: 19414.701171875	(16.2s)
epoch: 72	train loss: 19024.166015625	(16.2s)
epoch: 73	train loss: 19087.25	(16.1s)
epoch: 74	train loss: 19261.275390625	(16.2s)
epoch: 75	train loss: 18835.23046875	(16.2s)
epoch: 76	train loss: 18807.32421875	(16.2s)
epoch: 77	train loss: 18669.333984375	(16.2s)
epoch: 78	train loss: 18739.896484375	(16.1s)
epoch: 79	train loss: 18427.68359375	(16.1s)
epoch: 80	train loss: 18486.08203125	(16.2s)
epoch: 81	train loss: 18463.345703125	(16.2s)
epoch: 82	train loss: 18340.693359375	(16.2s)
epoch: 83	train loss: 18352.96484375	(16.1s)
epoch: 84	train loss: 19006.3359375	(16.1s)
epoch: 85	train loss: 18331.40234375	(16.2s)
epoch: 86	train loss: 18153.091796875	(16.2s)
epoch: 87	train loss: 18073.435546875	(16.2s)
epoch: 88	train loss: 18038.658203125	(16.2s)
epoch: 89	train loss: 18079.498046875	(16.1s)
epoch: 90	train loss: 17881.7578125	(16.2s)
epoch: 91	train loss: 17696.068359375	(16.2s)
epoch: 92	train loss: 17813.263671875	(16.2s)
epoch: 93	train loss: 17976.857421875	(16.2s)
epoch: 94	train loss: 17410.625	(16.1s)
epoch: 95	train loss: 17414.166015625	(16.2s)
epoch: 96	train loss: 17383.166015625	(16.2s)
epoch: 97	train loss: 17223.962890625	(16.2s)
epoch: 98	train loss: 17965.16796875	(16.2s)
epoch: 99	train loss: 17767.75390625	(16.1s)
Evaluating model on 200 episodes
0.003930877926904941
0.0026567200300632978
0.0012636839772526668
0.004746192083985079
0.001972681595816539
0.0010815347050083801
0.00048099118187868345
0.0014033282301776733
0.0014829318970441818
0.003486370487128928
0.0027309233038168815
0.0009059714193426771
0.006787603102566209
0.0005268233019575876
0.0018481841236482272
0.001706541410991728
0.0016203791601583362
0.0024126755859469997
0.0034221889764012303
0.0018574384801987825
0.0008972625675431283
0.001986744113310124
0.0018688916977473789
0.0022033349824986154
0.0014105360138880012
0.0018514959114587068
0.0024020616486571575
0.00269402130288654
0.002618697025657942
0.0014473062110482715
0.002038336975965649
0.0021749337174696848
0.0005838198611110619
0.0017830119941208977
0.003542828402714804
0.0069394777886125
0.0009952608088497072
0.0020535725857674454
0.004513540455263865
Solved 39/200 episodes
0.00045164587522430246
Evaluated model in 23.0 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-3
Round 4
Generated trajectories in 71.3 seconds
epoch: 0	train loss: 47019.44140625	(16.0s)
epoch: 1	train loss: 39176.51953125	(16.0s)
epoch: 2	train loss: 35497.8359375	(16.1s)
epoch: 3	train loss: 33192.390625	(16.2s)
epoch: 4	train loss: 31662.056640625	(16.2s)
epoch: 5	train loss: 30568.373046875	(16.2s)
epoch: 6	train loss: 29776.154296875	(16.1s)
epoch: 7	train loss: 28931.484375	(16.1s)
epoch: 8	train loss: 28581.283203125	(16.1s)
epoch: 9	train loss: 28026.84375	(16.2s)
epoch: 10	train loss: 27391.931640625	(16.2s)
epoch: 11	train loss: 27280.42578125	(16.1s)
epoch: 12	train loss: 26987.509765625	(16.1s)
epoch: 13	train loss: 26097.62109375	(16.1s)
epoch: 14	train loss: 25579.201171875	(16.2s)
epoch: 15	train loss: 25268.40625	(16.2s)
epoch: 16	train loss: 24830.328125	(16.2s)
epoch: 17	train loss: 24454.669921875	(16.1s)
epoch: 18	train loss: 24508.0625	(16.1s)
epoch: 19	train loss: 24815.734375	(16.2s)
epoch: 20	train loss: 24200.765625	(16.2s)
epoch: 21	train loss: 23603.875	(16.2s)
epoch: 22	train loss: 23179.34765625	(16.1s)
epoch: 23	train loss: 23262.818359375	(16.1s)
epoch: 24	train loss: 22627.744140625	(16.1s)
epoch: 25	train loss: 22423.017578125	(16.2s)
epoch: 26	train loss: 22655.8515625	(16.1s)
epoch: 27	train loss: 22991.857421875	(16.1s)
epoch: 28	train loss: 22124.765625	(16.0s)
epoch: 29	train loss: 21889.9375	(16.1s)
epoch: 30	train loss: 21587.923828125	(16.1s)
epoch: 31	train loss: 21522.115234375	(16.1s)
epoch: 32	train loss: 21651.2421875	(16.2s)
epoch: 33	train loss: 21284.611328125	(16.1s)
epoch: 34	train loss: 21271.509765625	(16.1s)
epoch: 35	train loss: 22508.08203125	(16.2s)
epoch: 36	train loss: 21432.330078125	(16.2s)
epoch: 37	train loss: 20815.49609375	(16.2s)
epoch: 38	train loss: 20571.61328125	(16.1s)
epoch: 39	train loss: 20611.75390625	(16.1s)
epoch: 40	train loss: 20757.515625	(16.1s)
epoch: 41	train loss: 20930.150390625	(16.2s)
epoch: 42	train loss: 20589.341796875	(16.2s)
epoch: 43	train loss: 20336.19921875	(16.1s)
epoch: 44	train loss: 20072.0625	(16.1s)
epoch: 45	train loss: 20402.0	(16.1s)
epoch: 46	train loss: 20480.41796875	(16.1s)
epoch: 47	train loss: 19929.60546875	(16.2s)
epoch: 48	train loss: 20427.396484375	(16.1s)
epoch: 49	train loss: 20273.603515625	(16.1s)
epoch: 50	train loss: 20095.150390625	(16.2s)
epoch: 51	train loss: 19969.3125	(16.1s)
epoch: 52	train loss: 19770.08203125	(16.1s)
epoch: 53	train loss: 19729.34765625	(16.2s)
epoch: 54	train loss: 19606.5625	(16.1s)
epoch: 55	train loss: 19523.41015625	(16.2s)
epoch: 56	train loss: 19350.99609375	(16.2s)
epoch: 57	train loss: 19149.083984375	(16.3s)
epoch: 58	train loss: 18755.263671875	(16.3s)
epoch: 59	train loss: 19286.298828125	(16.3s)
epoch: 60	train loss: 19018.791015625	(16.2s)
epoch: 61	train loss: 19559.451171875	(16.2s)
epoch: 62	train loss: 19181.220703125	(16.3s)
epoch: 63	train loss: 18720.96484375	(16.3s)
epoch: 64	train loss: 18475.974609375	(16.3s)
epoch: 65	train loss: 18400.978515625	(16.3s)
epoch: 66	train loss: 18443.275390625	(16.3s)
epoch: 67	train loss: 18046.638671875	(16.3s)
epoch: 68	train loss: 18272.908203125	(16.3s)
epoch: 69	train loss: 18139.646484375	(16.3s)
epoch: 70	train loss: 18455.419921875	(16.2s)
epoch: 71	train loss: 17813.38671875	(16.3s)
epoch: 72	train loss: 18199.951171875	(16.3s)
epoch: 73	train loss: 18123.23828125	(16.4s)
epoch: 74	train loss: 17664.341796875	(16.3s)
epoch: 75	train loss: 17544.62890625	(16.2s)
epoch: 76	train loss: 17570.283203125	(16.2s)
epoch: 77	train loss: 17363.98046875	(16.2s)
epoch: 78	train loss: 17381.306640625	(16.3s)
epoch: 79	train loss: 17617.03125	(16.3s)
epoch: 80	train loss: 17751.9140625	(16.2s)
epoch: 81	train loss: 17264.513671875	(16.2s)
epoch: 82	train loss: 17530.14453125	(16.3s)
epoch: 83	train loss: 17482.98828125	(16.2s)
epoch: 84	train loss: 17474.701171875	(16.3s)
epoch: 85	train loss: 17353.146484375	(16.2s)
epoch: 86	train loss: 17332.595703125	(16.3s)
epoch: 87	train loss: 17688.662109375	(16.3s)
epoch: 88	train loss: 17067.189453125	(16.3s)
epoch: 89	train loss: 16927.017578125	(16.3s)
epoch: 90	train loss: 17029.6171875	(16.3s)
epoch: 91	train loss: 16968.8671875	(16.2s)
epoch: 92	train loss: 16811.6015625	(16.2s)
epoch: 93	train loss: 16970.87890625	(16.3s)
epoch: 94	train loss: 16707.771484375	(16.3s)
epoch: 95	train loss: 16942.33984375	(16.3s)
epoch: 96	train loss: 16943.361328125	(16.2s)
epoch: 97	train loss: 16714.931640625	(16.2s)
epoch: 98	train loss: 16459.93359375	(16.3s)
epoch: 99	train loss: 16466.533203125	(16.3s)
Evaluating model on 200 episodes
0.0029607611982750575
0.0022916614793107976
0.0012206152264297998
0.0034756879200964854
0.001046703095198609
0.0011449150546328523
0.0013376523831818933
0.0019523039506263256
0.000874382943341819
0.002164649851196869
0.0009758397978397884
0.00051156625704607
0.0010405085995444097
0.005809012049576267
0.0013727843397646211
0.0023666320696520415
0.0017334510621367372
0.0009103487514948938
0.0004788685512791674
0.0008388357098738197
0.0020951075612174463
0.0007047914774981715
0.000870786510859034
0.0021431058354210108
0.0023952788590880423
0.0029151990253012626
0.0016786068308647373
0.001942532931934693
0.0010056995291961358
0.0025384974433109164
0.001207467420802762
0.0015722699705899383
0.0011332912872603629
0.001607278984010918
0.001112976279425008
0.0010692242306374413
0.0011845553177408873
0.000696841991157271
Solved 38/200 episodes
0.0003119034588840718
Evaluated model in 20.3 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-4
Round 5
Generated trajectories in 71.0 seconds
epoch: 0	train loss: 45145.86328125	(16.1s)
epoch: 1	train loss: 38067.0078125	(16.1s)
epoch: 2	train loss: 34460.92578125	(16.1s)
epoch: 3	train loss: 32461.10546875	(16.2s)
epoch: 4	train loss: 31004.216796875	(16.2s)
epoch: 5	train loss: 29738.052734375	(16.2s)
epoch: 6	train loss: 28840.20703125	(16.2s)
epoch: 7	train loss: 28154.083984375	(16.2s)
epoch: 8	train loss: 27641.478515625	(16.2s)
epoch: 9	train loss: 27578.814453125	(16.2s)
epoch: 10	train loss: 26931.51171875	(16.2s)
epoch: 11	train loss: 25917.900390625	(16.2s)
epoch: 12	train loss: 25516.6015625	(16.2s)
epoch: 13	train loss: 25117.05078125	(16.2s)
epoch: 14	train loss: 24919.064453125	(16.2s)
epoch: 15	train loss: 24495.919921875	(16.2s)
epoch: 16	train loss: 24440.80078125	(16.2s)
epoch: 17	train loss: 24099.833984375	(16.2s)
epoch: 18	train loss: 23702.2734375	(16.2s)
epoch: 19	train loss: 23093.322265625	(16.2s)
epoch: 20	train loss: 22773.517578125	(16.2s)
epoch: 21	train loss: 22541.427734375	(16.2s)
epoch: 22	train loss: 22075.470703125	(16.2s)
epoch: 23	train loss: 21993.310546875	(16.2s)
epoch: 24	train loss: 21596.84375	(16.1s)
epoch: 25	train loss: 21522.404296875	(16.2s)
epoch: 26	train loss: 21021.623046875	(16.1s)
epoch: 27	train loss: 20858.189453125	(16.2s)
epoch: 28	train loss: 20983.05859375	(16.1s)
epoch: 29	train loss: 21052.99609375	(16.1s)
epoch: 30	train loss: 20318.935546875	(16.1s)
epoch: 31	train loss: 20056.96875	(16.1s)
epoch: 32	train loss: 19955.13671875	(16.2s)
epoch: 33	train loss: 19984.12890625	(16.3s)
epoch: 34	train loss: 20476.1484375	(16.2s)
epoch: 35	train loss: 20507.212890625	(16.2s)
epoch: 36	train loss: 20124.263671875	(16.2s)
epoch: 37	train loss: 19872.40625	(16.2s)
epoch: 38	train loss: 19798.03515625	(16.3s)
epoch: 39	train loss: 20241.888671875	(16.2s)
epoch: 40	train loss: 19373.291015625	(16.2s)
epoch: 41	train loss: 18995.4921875	(16.2s)
epoch: 42	train loss: 18898.30859375	(16.2s)
epoch: 43	train loss: 18769.326171875	(16.2s)
epoch: 44	train loss: 18562.609375	(16.2s)
epoch: 45	train loss: 18728.14453125	(16.2s)
epoch: 46	train loss: 18890.896484375	(16.2s)
epoch: 47	train loss: 18327.0859375	(16.2s)
epoch: 48	train loss: 18249.69140625	(16.2s)
epoch: 49	train loss: 18487.4296875	(16.2s)
epoch: 50	train loss: 18124.181640625	(16.2s)
epoch: 51	train loss: 18383.40625	(16.2s)
epoch: 52	train loss: 18098.7265625	(16.1s)
epoch: 53	train loss: 18238.44921875	(16.2s)
epoch: 54	train loss: 17971.669921875	(16.2s)
epoch: 55	train loss: 17801.044921875	(16.2s)
epoch: 56	train loss: 17754.693359375	(16.2s)
epoch: 57	train loss: 18296.5234375	(16.1s)
epoch: 58	train loss: 17210.57421875	(16.2s)
epoch: 59	train loss: 17265.35546875	(16.2s)
epoch: 60	train loss: 17207.83984375	(16.1s)
epoch: 61	train loss: 17137.11328125	(16.1s)
epoch: 62	train loss: 17155.763671875	(16.1s)
epoch: 63	train loss: 17220.27734375	(16.2s)
epoch: 64	train loss: 17610.521484375	(16.2s)
epoch: 65	train loss: 17507.9375	(16.2s)
epoch: 66	train loss: 16908.64453125	(16.1s)
epoch: 67	train loss: 17064.30078125	(16.1s)
epoch: 68	train loss: 16757.904296875	(16.1s)
epoch: 69	train loss: 16539.228515625	(16.2s)
epoch: 70	train loss: 16599.03515625	(16.2s)
epoch: 71	train loss: 16312.728515625	(16.1s)
epoch: 72	train loss: 16536.294921875	(16.1s)
epoch: 73	train loss: 16641.62890625	(16.1s)
epoch: 74	train loss: 17122.388671875	(16.2s)
epoch: 75	train loss: 16821.75	(16.1s)
epoch: 76	train loss: 16417.73828125	(16.1s)
epoch: 77	train loss: 16876.814453125	(16.1s)
epoch: 78	train loss: 16435.982421875	(16.1s)
epoch: 79	train loss: 16538.349609375	(16.2s)
epoch: 80	train loss: 16987.537109375	(16.2s)
epoch: 81	train loss: 16464.314453125	(16.1s)
epoch: 82	train loss: 16694.6328125	(16.1s)
epoch: 83	train loss: 16396.5078125	(16.2s)
epoch: 84	train loss: 16425.89453125	(16.1s)
epoch: 85	train loss: 16338.611328125	(16.2s)
epoch: 86	train loss: 16250.1669921875	(16.2s)
epoch: 87	train loss: 16339.4140625	(16.1s)
epoch: 88	train loss: 15927.2373046875	(16.1s)
epoch: 89	train loss: 15724.58203125	(16.1s)
epoch: 90	train loss: 15675.4345703125	(16.2s)
epoch: 91	train loss: 15422.26953125	(16.1s)
epoch: 92	train loss: 15362.9833984375	(16.1s)
epoch: 93	train loss: 15767.9208984375	(16.1s)
epoch: 94	train loss: 15716.369140625	(16.1s)
epoch: 95	train loss: 17222.00390625	(16.2s)
epoch: 96	train loss: 15733.044921875	(16.1s)
epoch: 97	train loss: 15542.9521484375	(16.1s)
epoch: 98	train loss: 15315.328125	(16.1s)
epoch: 99	train loss: 15117.57421875	(16.1s)
Evaluating model on 200 episodes
0.002855352555798163
0.0033180566284240092
0.0016327951733338913
0.0031752600665034456
0.0005581734705109349
0.0027959153228746597
0.0009032266134454403
0.005644102724285663
0.0024322019417013507
0.0016901236776902806
0.0019362843202543444
0.0004568634260347328
0.002333689022634644
0.0010459947952767834
0.00042151457698006806
0.0006842296425020322
0.0011518828444726144
0.0024218123196785906
0.0007957316959488837
0.0022257562765541175
0.0018547226421052234
0.0020152423070006384
0.0005598266831769322
0.0005166004118039452
0.0009368451370391995
0.0011429951548052486
0.004860802497205441
0.000721864878207932
0.0009376171583426185
0.0025933391152648255
0.0024460958476993254
0.0029051858437014744
0.0014769456365684164
0.00043095948280501436
0.004996000843675574
0.0008057440914853942
0.002310378051333828
0.002935253804753302
0.0009829346319228144
0.0012405184954534182
0.002306267508174642
0.0005615334375761449
0.0020015418776893057
0.002530640062104794
0.0014203127429937012
0.0032201529247686267
0.0014267079321952122
Solved 47/200 episodes
0.00044307998147378823
Evaluated model in 23.2 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-5
Round 6
Generated trajectories in 71.4 seconds
epoch: 0	train loss: 44016.97265625	(16.0s)
epoch: 1	train loss: 37038.73046875	(16.0s)
epoch: 2	train loss: 33764.1953125	(16.1s)
epoch: 3	train loss: 31403.435546875	(16.1s)
epoch: 4	train loss: 29864.376953125	(16.1s)
epoch: 5	train loss: 28938.42578125	(16.1s)
epoch: 6	train loss: 27811.548828125	(16.1s)
epoch: 7	train loss: 27249.22265625	(16.2s)
epoch: 8	train loss: 26349.623046875	(16.2s)
epoch: 9	train loss: 25921.7578125	(16.1s)
epoch: 10	train loss: 25455.642578125	(16.1s)
epoch: 11	train loss: 24792.619140625	(16.1s)
epoch: 12	train loss: 24409.884765625	(16.2s)
epoch: 13	train loss: 24499.38671875	(16.2s)
epoch: 14	train loss: 24281.5078125	(16.1s)
epoch: 15	train loss: 23121.65234375	(16.1s)
epoch: 16	train loss: 22959.04296875	(16.1s)
epoch: 17	train loss: 22586.4765625	(16.1s)
epoch: 18	train loss: 22524.5390625	(16.2s)
epoch: 19	train loss: 22274.34765625	(16.1s)
epoch: 20	train loss: 21975.91796875	(16.1s)
epoch: 21	train loss: 21772.41796875	(16.1s)
epoch: 22	train loss: 21542.478515625	(16.1s)
epoch: 23	train loss: 21882.02734375	(16.2s)
epoch: 24	train loss: 21300.021484375	(16.1s)
epoch: 25	train loss: 21184.830078125	(16.1s)
epoch: 26	train loss: 21845.490234375	(16.1s)
epoch: 27	train loss: 20804.037109375	(16.1s)
epoch: 28	train loss: 20653.3046875	(16.1s)
epoch: 29	train loss: 20619.732421875	(16.1s)
epoch: 30	train loss: 20647.296875	(16.1s)
epoch: 31	train loss: 20296.2890625	(16.0s)
epoch: 32	train loss: 19914.224609375	(16.0s)
epoch: 33	train loss: 19471.13671875	(16.1s)
epoch: 34	train loss: 19442.9453125	(16.1s)
epoch: 35	train loss: 19190.625	(16.2s)
epoch: 36	train loss: 19739.857421875	(16.1s)
epoch: 37	train loss: 19221.947265625	(16.1s)
epoch: 38	train loss: 18641.845703125	(16.1s)
epoch: 39	train loss: 18607.568359375	(16.2s)
epoch: 40	train loss: 18672.611328125	(16.2s)
epoch: 41	train loss: 18728.876953125	(16.1s)
epoch: 42	train loss: 18246.865234375	(16.1s)
epoch: 43	train loss: 18341.634765625	(16.1s)
epoch: 44	train loss: 18685.08984375	(16.2s)
epoch: 45	train loss: 17865.8203125	(16.2s)
epoch: 46	train loss: 17521.818359375	(16.1s)
epoch: 47	train loss: 17888.41015625	(16.1s)
epoch: 48	train loss: 17652.474609375	(16.1s)
epoch: 49	train loss: 17690.232421875	(16.2s)
epoch: 50	train loss: 17775.740234375	(16.2s)
epoch: 51	train loss: 17624.4609375	(16.2s)
epoch: 52	train loss: 17364.49609375	(16.1s)
epoch: 53	train loss: 17733.82421875	(16.1s)
epoch: 54	train loss: 17371.876953125	(16.1s)
epoch: 55	train loss: 17367.376953125	(16.2s)
epoch: 56	train loss: 17413.6953125	(16.2s)
epoch: 57	train loss: 17276.6015625	(16.1s)
epoch: 58	train loss: 17596.48828125	(16.1s)
epoch: 59	train loss: 16722.322265625	(16.1s)
epoch: 60	train loss: 16674.296875	(16.2s)
epoch: 61	train loss: 16275.37890625	(16.2s)
epoch: 62	train loss: 16353.3232421875	(16.1s)
epoch: 63	train loss: 17935.576171875	(16.1s)
epoch: 64	train loss: 16530.38671875	(16.1s)
epoch: 65	train loss: 16242.65625	(16.1s)
epoch: 66	train loss: 15948.107421875	(16.1s)
epoch: 67	train loss: 15821.1123046875	(16.1s)
epoch: 68	train loss: 16985.951171875	(16.1s)
epoch: 69	train loss: 16123.046875	(16.1s)
epoch: 70	train loss: 15800.390625	(16.1s)
epoch: 71	train loss: 15446.73046875	(16.2s)
epoch: 72	train loss: 15452.81640625	(16.2s)
epoch: 73	train loss: 15847.2890625	(16.1s)
epoch: 74	train loss: 15816.884765625	(16.1s)
epoch: 75	train loss: 15740.7265625	(16.2s)
epoch: 76	train loss: 15557.509765625	(16.2s)
epoch: 77	train loss: 15934.248046875	(16.2s)
epoch: 78	train loss: 16749.95703125	(16.1s)
epoch: 79	train loss: 15388.4560546875	(16.1s)
epoch: 80	train loss: 15221.98046875	(16.1s)
epoch: 81	train loss: 14955.822265625	(16.2s)
epoch: 82	train loss: 14865.3798828125	(16.2s)
epoch: 83	train loss: 14736.880859375	(16.1s)
epoch: 84	train loss: 14874.736328125	(16.1s)
epoch: 85	train loss: 14691.3125	(16.1s)
epoch: 86	train loss: 15047.470703125	(16.2s)
epoch: 87	train loss: 14544.232421875	(16.2s)
epoch: 88	train loss: 14709.162109375	(16.2s)
epoch: 89	train loss: 14351.35546875	(16.1s)
epoch: 90	train loss: 15840.7451171875	(16.1s)
epoch: 91	train loss: 14919.099609375	(16.1s)
epoch: 92	train loss: 14579.1435546875	(16.2s)
epoch: 93	train loss: 14999.0439453125	(16.2s)
epoch: 94	train loss: 15206.435546875	(16.1s)
epoch: 95	train loss: 14556.7587890625	(16.1s)
epoch: 96	train loss: 15840.1435546875	(16.2s)
epoch: 97	train loss: 14481.578125	(16.2s)
epoch: 98	train loss: 14178.23046875	(16.2s)
epoch: 99	train loss: 14607.669921875	(16.1s)
Evaluating model on 200 episodes
0.0012768089363817126
0.003081592481314808
0.0010025108443057565
0.0008945620200912734
0.0025771749969862867
0.002801034010190051
0.002666967962299664
0.0013833013050316366
0.0024204978350705155
0.0022876670118421316
0.005935876193689182
0.005094363437300282
0.002238766036148613
0.0020716882427223027
0.000714656707107982
0.0033555700510987663
0.004428527845043896
0.0009408060895194384
0.001708042657066306
0.0025961505614557406
0.0010047516880149487
0.003147970848658588
0.0022156392806209624
0.0030418136520893313
0.0017295939634839835
0.0008753141810302623
0.0011642731824395014
0.003937469853553921
0.003267765633857132
0.0005936997067692573
0.0015654936305509182
0.0004906943270018221
0.002135560885653831
0.0020104218889173352
0.0019266718979148816
0.0008442735070275376
0.0030800798595009837
0.001514250699983677
0.0021595433912201165
0.0037379215813416523
0.0013676265807589516
0.0016761305326196765
0.0014585394907044246
0.005081600625999272
0.002120755327632651
0.001386495322852473
0.0012091520402464084
0.0023760293200515057
0.0015013724322281113
0.003014294889807287
0.002337362265097909
0.002345137695859497
0.0025207001963281073
0.0031445158485439604
0.0031166826374828815
Solved 55/200 episodes
0.0006228808204525505
Evaluated model in 21.5 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-6
Round 7
Generated trajectories in 71.9 seconds
epoch: 0	train loss: 43437.546875	(16.3s)
epoch: 1	train loss: 36208.01953125	(16.2s)
epoch: 2	train loss: 32502.0	(16.2s)
epoch: 3	train loss: 30316.9453125	(16.3s)
epoch: 4	train loss: 29014.73046875	(16.4s)
epoch: 5	train loss: 27558.5703125	(16.4s)
epoch: 6	train loss: 26559.052734375	(16.3s)
epoch: 7	train loss: 25857.791015625	(16.3s)
epoch: 8	train loss: 25999.91796875	(16.3s)
epoch: 9	train loss: 25332.001953125	(16.4s)
epoch: 10	train loss: 24655.01953125	(16.4s)
epoch: 11	train loss: 24143.30078125	(16.3s)
epoch: 12	train loss: 23273.51953125	(16.3s)
epoch: 13	train loss: 22918.234375	(16.3s)
epoch: 14	train loss: 22452.537109375	(16.4s)
epoch: 15	train loss: 22272.4453125	(16.4s)
epoch: 16	train loss: 22557.3671875	(16.3s)
epoch: 17	train loss: 22070.68359375	(16.3s)
epoch: 18	train loss: 21287.81640625	(16.3s)
epoch: 19	train loss: 20806.05078125	(16.4s)
epoch: 20	train loss: 20499.578125	(16.4s)
epoch: 21	train loss: 20076.029296875	(16.3s)
epoch: 22	train loss: 20070.40625	(16.3s)
epoch: 23	train loss: 19576.404296875	(16.4s)
epoch: 24	train loss: 19187.033203125	(16.4s)
epoch: 25	train loss: 18871.818359375	(16.4s)
epoch: 26	train loss: 18677.25	(16.4s)
epoch: 27	train loss: 18225.455078125	(16.3s)
epoch: 28	train loss: 17989.14453125	(16.3s)
epoch: 29	train loss: 17969.5703125	(16.3s)
epoch: 30	train loss: 18101.1796875	(16.4s)
epoch: 31	train loss: 19255.548828125	(16.3s)
epoch: 32	train loss: 18478.380859375	(16.3s)
epoch: 33	train loss: 17829.736328125	(16.3s)
epoch: 34	train loss: 17543.130859375	(16.3s)
epoch: 35	train loss: 17524.4296875	(16.3s)
epoch: 36	train loss: 17411.697265625	(16.4s)
epoch: 37	train loss: 17885.283203125	(16.3s)
epoch: 38	train loss: 16947.958984375	(16.3s)
epoch: 39	train loss: 16560.6015625	(16.3s)
epoch: 40	train loss: 16810.0625	(16.4s)
epoch: 41	train loss: 16917.05078125	(16.4s)
epoch: 42	train loss: 16162.4970703125	(16.3s)
epoch: 43	train loss: 16537.455078125	(16.3s)
epoch: 44	train loss: 15976.5693359375	(16.3s)
epoch: 45	train loss: 15976.859375	(16.4s)
epoch: 46	train loss: 16021.9423828125	(16.4s)
epoch: 47	train loss: 15461.064453125	(16.4s)
epoch: 48	train loss: 15612.92578125	(16.4s)
epoch: 49	train loss: 15464.8408203125	(16.3s)
epoch: 50	train loss: 15793.662109375	(16.4s)
epoch: 51	train loss: 15799.9365234375	(16.4s)
epoch: 52	train loss: 15262.4521484375	(16.4s)
epoch: 53	train loss: 15016.6142578125	(16.3s)
epoch: 54	train loss: 15079.453125	(16.3s)
epoch: 55	train loss: 14610.431640625	(16.4s)
epoch: 56	train loss: 15205.4873046875	(16.4s)
epoch: 57	train loss: 14472.7666015625	(16.4s)
epoch: 58	train loss: 14055.720703125	(16.3s)
epoch: 59	train loss: 14313.451171875	(16.3s)
epoch: 60	train loss: 13952.6083984375	(16.4s)
epoch: 61	train loss: 14002.64453125	(16.4s)
epoch: 62	train loss: 14528.3251953125	(16.4s)
epoch: 63	train loss: 13865.7705078125	(16.4s)
epoch: 64	train loss: 14134.9423828125	(16.3s)
epoch: 65	train loss: 13609.734375	(16.4s)
epoch: 66	train loss: 13662.345703125	(16.4s)
epoch: 67	train loss: 14142.986328125	(16.4s)
epoch: 68	train loss: 13081.359375	(16.3s)
epoch: 69	train loss: 13100.1328125	(16.3s)
epoch: 70	train loss: 12926.6826171875	(16.3s)
epoch: 71	train loss: 12680.0869140625	(16.4s)
epoch: 72	train loss: 13639.9267578125	(16.4s)
epoch: 73	train loss: 12963.302734375	(16.3s)
epoch: 74	train loss: 13212.1083984375	(16.3s)
epoch: 75	train loss: 12920.2275390625	(16.3s)
epoch: 76	train loss: 12713.7216796875	(16.3s)
epoch: 77	train loss: 12169.2138671875	(16.4s)
epoch: 78	train loss: 11792.96484375	(16.4s)
epoch: 79	train loss: 11893.509765625	(16.3s)
epoch: 80	train loss: 11993.74609375	(16.3s)
epoch: 81	train loss: 12028.4453125	(16.3s)
epoch: 82	train loss: 12077.4287109375	(16.4s)
epoch: 83	train loss: 11854.291015625	(16.4s)
epoch: 84	train loss: 12111.00390625	(16.4s)
epoch: 85	train loss: 11775.6083984375	(16.4s)
epoch: 86	train loss: 11593.751953125	(16.5s)
epoch: 87	train loss: 12697.37890625	(16.5s)
epoch: 88	train loss: 12002.02734375	(16.4s)
epoch: 89	train loss: 12429.53515625	(16.4s)
epoch: 90	train loss: 11982.517578125	(16.4s)
epoch: 91	train loss: 11569.703125	(16.4s)
epoch: 92	train loss: 11388.5439453125	(16.4s)
epoch: 93	train loss: 11339.931640625	(16.4s)
epoch: 94	train loss: 11245.48828125	(16.3s)
epoch: 95	train loss: 11273.986328125	(16.3s)
epoch: 96	train loss: 11195.3271484375	(16.3s)
epoch: 97	train loss: 11216.015625	(16.3s)
epoch: 98	train loss: 11419.4345703125	(16.4s)
epoch: 99	train loss: 11437.4755859375	(16.4s)
Evaluating model on 200 episodes
0.002526619247267566
0.006581124073515336
0.0008883216016531353
0.0012419053527992219
0.0017658509632383357
0.0019558183277175495
0.004825799571335665
0.0016278742299288172
0.0009775942956669799
0.001312775544647593
0.001103927363146795
0.002731337764998898
0.0008784763079650778
0.0018569591920822859
0.003170926347733117
0.004046112054008215
0.0015225230187449295
0.005248321493854746
0.0006706867807224626
0.0017546853167004883
0.0019970132198068313
0.003519623099626707
0.0016726518848978837
0.002631908558604274
0.0024131196036857243
0.0020034990642064563
0.003060702457635974
0.004863979427480242
0.003469226573846148
0.0007736718273788158
0.004001433115465463
0.0012360893470031442
0.0018091356281989387
0.002451329834002536
0.0031681962563280567
0.0007398276105353337
0.0051304627559147775
0.0005679577670889557
0.0004692265308851627
0.005074465754053866
0.001136944777416912
0.0018558953277533873
0.0006382377291124107
0.0034919834496187313
0.0020246419049866605
0.0026871305017266423
0.00488191735833728
0.0032643117330735548
0.0011454597843112424
0.007941627443263618
0.000933195803372655
0.0030251059055444785
0.001148690702393651
0.002916511009001018
0.0023138416436268017
0.0025843006651484757
0.0031783477548742666
0.0014876618991062666
0.0010954573372146115
0.00349954266234168
0.001389914508763468
0.0023582707486639264
0.002976515145168047
0.003032018046360463
0.0005691944121887597
0.003699616581434384
0.002630925858587337
0.0022079163713247646
0.0032756209302533534
0.0010061858367407695
0.0008067213077863146
0.0031976706856463957
0.0030416975296247983
0.004250472609083562
0.0012699962512670027
0.0007556310059347501
0.003159699224246045
0.004623622804501792
0.0016629438614472747
0.003370125818764791
0.003640753566287458
0.002482020056656135
0.0007031628762585266
0.0013039540463068988
0.0023975598613525597
Solved 85/200 episodes
0.0010440208725062319
Evaluated model in 21.8 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-7
Round 8
Generated trajectories in 72.0 seconds
epoch: 0	train loss: 34401.8046875	(16.4s)
epoch: 1	train loss: 28075.02734375	(16.2s)
epoch: 2	train loss: 25095.953125	(16.2s)
epoch: 3	train loss: 23104.25	(16.2s)
epoch: 4	train loss: 21444.59765625	(16.3s)
epoch: 5	train loss: 20745.716796875	(16.3s)
epoch: 6	train loss: 20512.400390625	(16.3s)
epoch: 7	train loss: 19165.16015625	(16.2s)
epoch: 8	train loss: 18548.298828125	(16.3s)
epoch: 9	train loss: 17923.6796875	(16.3s)
epoch: 10	train loss: 17502.2578125	(16.3s)
epoch: 11	train loss: 16842.359375	(16.3s)
epoch: 12	train loss: 16574.330078125	(16.3s)
epoch: 13	train loss: 16845.029296875	(16.3s)
epoch: 14	train loss: 15712.1025390625	(16.3s)
epoch: 15	train loss: 15580.1201171875	(16.4s)
epoch: 16	train loss: 15008.158203125	(16.3s)
epoch: 17	train loss: 14672.90625	(16.3s)
epoch: 18	train loss: 14633.306640625	(16.3s)
epoch: 19	train loss: 15865.916015625	(16.3s)
epoch: 20	train loss: 14532.0927734375	(16.3s)
epoch: 21	train loss: 14231.87109375	(16.4s)
epoch: 22	train loss: 13875.1787109375	(16.3s)
epoch: 23	train loss: 13813.455078125	(16.2s)
epoch: 24	train loss: 15118.83203125	(16.3s)
epoch: 25	train loss: 13775.9599609375	(16.3s)
epoch: 26	train loss: 13539.970703125	(16.3s)
epoch: 27	train loss: 13307.55078125	(16.3s)
epoch: 28	train loss: 13227.1044921875	(16.4s)
epoch: 29	train loss: 12964.935546875	(16.4s)
epoch: 30	train loss: 12978.078125	(16.3s)
epoch: 31	train loss: 13436.3544921875	(16.3s)
epoch: 32	train loss: 12888.2763671875	(16.4s)
epoch: 33	train loss: 12891.705078125	(16.3s)
epoch: 34	train loss: 12521.3974609375	(16.2s)
epoch: 35	train loss: 12398.982421875	(16.3s)
epoch: 36	train loss: 12376.0234375	(16.3s)
epoch: 37	train loss: 12388.3916015625	(16.3s)
epoch: 38	train loss: 12426.287109375	(16.3s)
epoch: 39	train loss: 12878.38671875	(16.3s)
epoch: 40	train loss: 12775.8046875	(16.3s)
epoch: 41	train loss: 12159.8876953125	(16.3s)
epoch: 42	train loss: 12003.134765625	(16.3s)
epoch: 43	train loss: 12052.796875	(16.3s)
epoch: 44	train loss: 11657.171875	(16.3s)
epoch: 45	train loss: 11253.4755859375	(16.3s)
epoch: 46	train loss: 12005.7060546875	(16.4s)
epoch: 47	train loss: 11487.041015625	(16.3s)
epoch: 48	train loss: 11437.5478515625	(16.3s)
epoch: 49	train loss: 11055.095703125	(16.3s)
epoch: 50	train loss: 11010.51171875	(16.3s)
epoch: 51	train loss: 10887.4013671875	(16.3s)
epoch: 52	train loss: 11277.2548828125	(16.3s)
epoch: 53	train loss: 11666.4091796875	(16.3s)
epoch: 54	train loss: 11393.3876953125	(16.3s)
epoch: 55	train loss: 11191.5400390625	(16.3s)
epoch: 56	train loss: 11074.5	(16.3s)
epoch: 57	train loss: 10696.1904296875	(16.3s)
epoch: 58	train loss: 10875.794921875	(16.3s)
epoch: 59	train loss: 11906.8408203125	(16.3s)
epoch: 60	train loss: 10901.33203125	(16.3s)
epoch: 61	train loss: 10569.3369140625	(16.3s)
epoch: 62	train loss: 10737.3310546875	(16.3s)
epoch: 63	train loss: 10818.2392578125	(16.3s)
epoch: 64	train loss: 10963.3671875	(16.3s)
epoch: 65	train loss: 11087.978515625	(16.3s)
epoch: 66	train loss: 10475.2744140625	(16.3s)
epoch: 67	train loss: 11223.0185546875	(16.3s)
epoch: 68	train loss: 11183.2529296875	(16.3s)
epoch: 69	train loss: 10107.609375	(16.3s)
epoch: 70	train loss: 10113.62109375	(16.3s)
epoch: 71	train loss: 9920.6630859375	(16.3s)
epoch: 72	train loss: 10278.3759765625	(16.3s)
epoch: 73	train loss: 9902.2724609375	(16.3s)
epoch: 74	train loss: 10623.474609375	(16.3s)
epoch: 75	train loss: 10126.1064453125	(16.3s)
epoch: 76	train loss: 10495.1044921875	(16.3s)
epoch: 77	train loss: 11048.3974609375	(16.3s)
epoch: 78	train loss: 10473.7119140625	(16.3s)
epoch: 79	train loss: 10170.2109375	(16.3s)
epoch: 80	train loss: 9698.3583984375	(16.3s)
epoch: 81	train loss: 11866.310546875	(16.3s)
epoch: 82	train loss: 10644.0771484375	(16.3s)
epoch: 83	train loss: 10335.8349609375	(16.3s)
epoch: 84	train loss: 10540.39453125	(16.3s)
epoch: 85	train loss: 9655.2109375	(16.3s)
epoch: 86	train loss: 9231.861328125	(16.3s)
epoch: 87	train loss: 9258.5166015625	(16.3s)
epoch: 88	train loss: 9852.1708984375	(16.3s)
epoch: 89	train loss: 9364.0244140625	(16.3s)
epoch: 90	train loss: 9079.3583984375	(16.3s)
epoch: 91	train loss: 9657.978515625	(16.3s)
epoch: 92	train loss: 11107.9296875	(16.3s)
epoch: 93	train loss: 10038.3623046875	(16.3s)
epoch: 94	train loss: 9701.3662109375	(16.3s)
epoch: 95	train loss: 9504.9619140625	(16.3s)
epoch: 96	train loss: 9493.9775390625	(16.3s)
epoch: 97	train loss: 9612.4853515625	(16.3s)
epoch: 98	train loss: 10868.78515625	(16.3s)
epoch: 99	train loss: 9468.9970703125	(16.3s)
Evaluating model on 200 episodes
0.003155943043696295
0.0018111767365973299
0.002285345054518145
0.0025533504085615277
0.0007088358272864882
0.0011517964878294152
0.001119202205700276
0.001721612825909128
0.0009961582066299504
0.0016602289529559625
0.00256117635872215
0.002588188974186778
0.0008391024845574672
0.008247547331848182
0.0013833391451044009
0.0009971075669454876
0.0009483980507606507
0.0030428982184578976
0.001972267338715028
0.0007011353424916576
0.0021121468675523326
0.002509819004141415
0.0019353920615685638
0.001229814242105931
0.0025461202403675998
0.0021745999531251276
0.0010875774969463237
0.0021897443342216625
0.0011194226637599059
0.0014097073771363983
0.0012846436813041301
0.0021986923784425016
0.0013385188261357446
0.00152488374130501
0.0022218162837361888
0.0010709810464201535
0.0012230319168468537
0.0018190701885032468
0.001839148275394109
0.0007479157942394853
0.000758960317600415
0.001434802390576806
0.004703247808860256
0.001543922100609052
0.002405179995217953
0.0014256156324076333
0.0012522236419109894
0.0005671233108538823
0.002063469470886048
0.003104480742227419
0.0008625793861938291
0.0020417076909022094
0.004249854370507158
0.0037467840702447575
0.0009992650912653727
0.005187892199804385
0.0023043453175988463
0.001569366970215924
0.0014464267830286797
0.0021520912834805334
0.0019364994310308247
0.0007155865714594256
0.0015855912424740382
0.0006530868413392454
0.006034099234966561
0.001730426331050694
0.004106786449847277
0.0028814721226808615
0.00255224239047563
0.002759219167769929
0.000633065055548546
0.002932433705912748
0.0030881663468400283
0.0009671783222334593
0.0036034590084454976
0.0011777026411436964
0.0020930069328572313
0.0030945007731336066
0.001543911100270634
0.002758894715225324
0.0025890580228323497
0.0009037117924890481
0.004393422805151204
0.0020515001378953457
0.001630612959464391
0.0025396009212792186
0.0016191759659704985
Solved 87/200 episodes
0.0009021080324945321
Evaluated model in 22.8 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-8
Round 9
Generated trajectories in 71.8 seconds
epoch: 0	train loss: 29951.23828125	(16.3s)
epoch: 1	train loss: 23878.28515625	(16.3s)
epoch: 2	train loss: 21557.31640625	(16.4s)
epoch: 3	train loss: 20115.525390625	(16.4s)
epoch: 4	train loss: 18875.916015625	(16.5s)
epoch: 5	train loss: 18105.904296875	(16.5s)
epoch: 6	train loss: 17575.072265625	(16.5s)
epoch: 7	train loss: 16937.08203125	(16.5s)
epoch: 8	train loss: 16579.9609375	(16.3s)
epoch: 9	train loss: 16415.724609375	(16.5s)
epoch: 10	train loss: 16085.208984375	(16.5s)
epoch: 11	train loss: 15518.390625	(16.5s)
epoch: 12	train loss: 15238.15234375	(16.4s)
epoch: 13	train loss: 15413.6455078125	(16.5s)
epoch: 14	train loss: 14654.599609375	(16.4s)
epoch: 15	train loss: 14584.1640625	(16.5s)
epoch: 16	train loss: 14273.994140625	(16.5s)
epoch: 17	train loss: 14337.1982421875	(16.5s)
epoch: 18	train loss: 13451.318359375	(16.5s)
epoch: 19	train loss: 13084.3134765625	(16.5s)
epoch: 20	train loss: 13184.169921875	(16.4s)
epoch: 21	train loss: 12923.2353515625	(16.5s)
epoch: 22	train loss: 13062.3583984375	(16.5s)
epoch: 23	train loss: 12711.875	(16.3s)
epoch: 24	train loss: 13026.9052734375	(16.5s)
epoch: 25	train loss: 12375.0009765625	(16.3s)
epoch: 26	train loss: 12981.2744140625	(16.5s)
epoch: 27	train loss: 12663.4677734375	(16.5s)
epoch: 28	train loss: 12884.083984375	(16.5s)
epoch: 29	train loss: 12338.783203125	(16.5s)
epoch: 30	train loss: 12348.998046875	(16.3s)
epoch: 31	train loss: 14194.134765625	(16.5s)
epoch: 32	train loss: 12263.015625	(16.4s)
epoch: 33	train loss: 11772.0869140625	(16.4s)
epoch: 34	train loss: 11456.04296875	(16.5s)
epoch: 35	train loss: 11565.5263671875	(16.5s)
epoch: 36	train loss: 11493.19140625	(16.5s)
epoch: 37	train loss: 11427.1796875	(16.4s)
epoch: 38	train loss: 11146.1943359375	(16.3s)
epoch: 39	train loss: 11287.3154296875	(16.4s)
epoch: 40	train loss: 13503.5703125	(16.5s)
epoch: 41	train loss: 11650.4873046875	(16.6s)
epoch: 42	train loss: 11147.83203125	(16.5s)
epoch: 43	train loss: 10907.0498046875	(16.3s)
epoch: 44	train loss: 11088.94140625	(16.4s)
epoch: 45	train loss: 11056.6953125	(16.5s)
epoch: 46	train loss: 11150.25	(16.4s)
epoch: 47	train loss: 11499.1376953125	(16.5s)
epoch: 48	train loss: 10882.1875	(16.5s)
epoch: 49	train loss: 11013.90234375	(16.4s)
epoch: 50	train loss: 11111.7314453125	(16.4s)
epoch: 51	train loss: 11234.6220703125	(16.5s)
epoch: 52	train loss: 11808.41015625	(16.5s)
epoch: 53	train loss: 11599.314453125	(16.5s)
epoch: 54	train loss: 10901.4658203125	(16.5s)
epoch: 55	train loss: 10646.724609375	(16.4s)
epoch: 56	train loss: 10549.7919921875	(16.5s)
epoch: 57	train loss: 10501.4208984375	(16.5s)
epoch: 58	train loss: 11466.2822265625	(16.5s)
epoch: 59	train loss: 10614.91015625	(16.3s)
epoch: 60	train loss: 10437.3896484375	(16.4s)
epoch: 61	train loss: 10312.70703125	(16.5s)
epoch: 62	train loss: 10080.3076171875	(16.5s)
epoch: 63	train loss: 10213.96875	(16.5s)
epoch: 64	train loss: 10158.4208984375	(16.5s)
epoch: 65	train loss: 9911.3857421875	(16.4s)
epoch: 66	train loss: 9952.5595703125	(16.5s)
epoch: 67	train loss: 11614.8603515625	(16.5s)
epoch: 68	train loss: 10425.9228515625	(16.4s)
epoch: 69	train loss: 10410.384765625	(16.5s)
epoch: 70	train loss: 9898.44921875	(16.4s)
epoch: 71	train loss: 10549.8447265625	(16.5s)
epoch: 72	train loss: 9892.318359375	(16.4s)
epoch: 73	train loss: 9559.7392578125	(16.4s)
epoch: 74	train loss: 9524.572265625	(16.5s)
epoch: 75	train loss: 9509.2119140625	(16.3s)
epoch: 76	train loss: 9555.90234375	(16.5s)
epoch: 77	train loss: 9315.2919921875	(16.5s)
epoch: 78	train loss: 9596.4013671875	(16.5s)
epoch: 79	train loss: 9758.3583984375	(16.4s)
epoch: 80	train loss: 9650.4736328125	(16.5s)
epoch: 81	train loss: 9418.3134765625	(16.3s)
epoch: 82	train loss: 8862.05859375	(16.5s)
epoch: 83	train loss: 10603.12109375	(16.5s)
epoch: 84	train loss: 9561.5009765625	(16.5s)
epoch: 85	train loss: 9670.18359375	(16.5s)
epoch: 86	train loss: 9156.55078125	(16.5s)
epoch: 87	train loss: 9589.9033203125	(16.5s)
epoch: 88	train loss: 9582.240234375	(16.5s)
epoch: 89	train loss: 9356.3408203125	(16.5s)
epoch: 90	train loss: 9655.865234375	(16.5s)
epoch: 91	train loss: 9488.890625	(16.3s)
epoch: 92	train loss: 8963.546875	(16.4s)
epoch: 93	train loss: 9106.0576171875	(16.4s)
epoch: 94	train loss: 9333.6865234375	(16.5s)
epoch: 95	train loss: 8941.4052734375	(16.5s)
epoch: 96	train loss: 9400.748046875	(16.3s)
epoch: 97	train loss: 9590.0751953125	(16.5s)
epoch: 98	train loss: 9093.70703125	(16.5s)
epoch: 99	train loss: 8996.2900390625	(16.5s)
Evaluating model on 200 episodes
0.001709209757141902
0.0019568089774111286
0.0013451614533551038
0.0024514932384980576
0.00043107641370928224
0.0015400712977030448
0.0016640363664919278
0.003980167153349612
0.0022036233955683806
0.0008080672143312933
0.005940426409674378
0.0006425431881022329
0.002581428102631536
0.000928773769980277
0.0020913230853491565
0.002271625700814184
0.001987336465390399
0.005456234949330489
0.0009686222474556416
0.0023853317761677317
0.004702016547006289
0.0014368091331562027
0.0025746282655745743
0.004128414012666326
0.0010356645507272333
0.0012464218227912418
0.0013592398006747114
0.0016090578234676893
0.004051117936614901
0.0025843418234217097
0.0022785670164012116
0.0007204421926871873
0.0010351869022997562
0.0018557549871426697
0.0026022012049603895
0.0034154522931203247
0.002424675057144751
0.0025214830457116477
0.0014497969050353277
0.0017492647190617088
0.0023041603975192024
0.002008033319725655
0.001973554000869626
0.0015658472484574304
0.0018516311954174722
0.0023533988060080446
0.0022657412220723925
0.0026574262283247663
0.0028618938182868683
0.0032755828215158544
0.002702517798752524
0.0012017613771604374
0.001251083531921419
0.0026632551259050765
0.002622562616703693
0.0021863243719880384
0.0011366522293038933
0.0014142333779350988
0.001109063055212443
0.0014390083864176024
0.001678033383541333
0.0017817305841682745
0.0036213635455799075
0.005288144329097122
0.001887355691906123
0.0022972737006057287
0.0010336433292000688
0.0022993092231142023
0.001842935974127613
0.0026415388816861773
0.004573407476224626
0.0014618224115110934
0.007444942952133715
0.002452133407132351
0.0016880266114392064
0.003431969079732274
0.001792271388694644
0.0029826577136356554
0.0026420475359455768
0.001935373293235898
0.0014896296731118734
0.0008196182010578923
0.0030590281052614693
0.0014102584173087962
0.0027734071572922403
0.0015266834137340386
0.0027448709835880434
0.0031853544221828794
0.004477690519706812
0.0021131831975556756
0.0009412911955829865
0.0019028333597816526
0.0009072038180117185
0.0018791077006123185
0.0034754230825152868
0.0018198333236796316
0.0013119110746109591
0.0036024973960593343
0.0021253114857245235
0.0010624689457472414
Solved 100/200 episodes
0.0011318560695872803
Evaluated model in 23.9 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-9
Round 10
Generated trajectories in 72.3 seconds
epoch: 0	train loss: 26428.75390625	(16.2s)
epoch: 1	train loss: 21292.591796875	(16.2s)
epoch: 2	train loss: 19273.19921875	(16.1s)
epoch: 3	train loss: 18541.82421875	(16.2s)
epoch: 4	train loss: 17013.345703125	(16.3s)
epoch: 5	train loss: 16401.59375	(16.3s)
epoch: 6	train loss: 15567.2666015625	(16.3s)
epoch: 7	train loss: 15529.0234375	(16.2s)
epoch: 8	train loss: 14616.546875	(16.2s)
epoch: 9	train loss: 14767.220703125	(16.2s)
epoch: 10	train loss: 15000.4677734375	(16.3s)
epoch: 11	train loss: 14584.19140625	(16.3s)
epoch: 12	train loss: 13855.8134765625	(16.2s)
epoch: 13	train loss: 13712.6953125	(16.2s)
epoch: 14	train loss: 13060.90234375	(16.2s)
epoch: 15	train loss: 13357.125	(16.3s)
epoch: 16	train loss: 13142.041015625	(16.3s)
epoch: 17	train loss: 14010.302734375	(16.2s)
epoch: 18	train loss: 12881.806640625	(16.2s)
epoch: 19	train loss: 12699.2294921875	(16.2s)
epoch: 20	train loss: 12236.794921875	(16.2s)
epoch: 21	train loss: 12257.7216796875	(16.3s)
epoch: 22	train loss: 12216.521484375	(16.3s)
epoch: 23	train loss: 12646.150390625	(16.2s)
epoch: 24	train loss: 12041.5048828125	(16.2s)
epoch: 25	train loss: 11818.78125	(16.2s)
epoch: 26	train loss: 11847.587890625	(16.3s)
epoch: 27	train loss: 12139.55078125	(16.3s)
epoch: 28	train loss: 11833.2900390625	(16.2s)
epoch: 29	train loss: 11493.9921875	(16.2s)
epoch: 30	train loss: 11133.919921875	(16.2s)
epoch: 31	train loss: 11938.1650390625	(16.2s)
epoch: 32	train loss: 11180.9541015625	(16.2s)
epoch: 33	train loss: 11133.78125	(16.2s)
epoch: 34	train loss: 11032.6982421875	(16.1s)
epoch: 35	train loss: 11490.125	(16.1s)
epoch: 36	train loss: 10955.2744140625	(16.2s)
epoch: 37	train loss: 10623.923828125	(16.2s)
epoch: 38	train loss: 11306.78125	(16.2s)
epoch: 39	train loss: 10604.92578125	(16.2s)
epoch: 40	train loss: 10877.47265625	(16.2s)
epoch: 41	train loss: 10253.375	(16.2s)
epoch: 42	train loss: 10485.3330078125	(16.3s)
epoch: 43	train loss: 11041.25390625	(16.3s)
epoch: 44	train loss: 10489.3017578125	(16.2s)
epoch: 45	train loss: 10547.5390625	(16.2s)
epoch: 46	train loss: 10143.0810546875	(16.2s)
epoch: 47	train loss: 10384.279296875	(16.3s)
epoch: 48	train loss: 10277.939453125	(16.3s)
epoch: 49	train loss: 10102.9404296875	(16.2s)
epoch: 50	train loss: 9978.21484375	(16.2s)
epoch: 51	train loss: 11109.3759765625	(16.2s)
epoch: 52	train loss: 10172.138671875	(16.3s)
epoch: 53	train loss: 9986.302734375	(16.3s)
epoch: 54	train loss: 9751.8974609375	(16.2s)
epoch: 55	train loss: 10207.5029296875	(16.2s)
epoch: 56	train loss: 9478.7666015625	(16.2s)
epoch: 57	train loss: 9711.728515625	(16.3s)
epoch: 58	train loss: 10040.888671875	(16.3s)
epoch: 59	train loss: 10356.095703125	(16.2s)
epoch: 60	train loss: 9767.66796875	(16.2s)
epoch: 61	train loss: 9740.740234375	(16.2s)
epoch: 62	train loss: 9605.015625	(16.2s)
epoch: 63	train loss: 9426.5810546875	(16.2s)
epoch: 64	train loss: 9298.693359375	(16.3s)
epoch: 65	train loss: 10205.1494140625	(16.2s)
epoch: 66	train loss: 10413.9033203125	(16.2s)
epoch: 67	train loss: 9470.9013671875	(16.2s)
epoch: 68	train loss: 9863.630859375	(16.3s)
epoch: 69	train loss: 9366.2275390625	(16.3s)
epoch: 70	train loss: 9329.546875	(16.2s)
epoch: 71	train loss: 9131.3837890625	(16.2s)
epoch: 72	train loss: 8912.7021484375	(16.2s)
epoch: 73	train loss: 16149.357421875	(16.3s)
epoch: 74	train loss: 11272.314453125	(16.3s)
epoch: 75	train loss: 9791.8564453125	(16.2s)
epoch: 76	train loss: 9124.9423828125	(16.2s)
epoch: 77	train loss: 9010.599609375	(16.2s)
epoch: 78	train loss: 9146.4599609375	(16.2s)
epoch: 79	train loss: 9010.8056640625	(16.3s)
epoch: 80	train loss: 8674.140625	(16.2s)
epoch: 81	train loss: 9405.41796875	(16.2s)
epoch: 82	train loss: 9200.9033203125	(16.2s)
epoch: 83	train loss: 8681.974609375	(16.2s)
epoch: 84	train loss: 8563.4736328125	(16.3s)
epoch: 85	train loss: 8873.9091796875	(16.3s)
epoch: 86	train loss: 8980.5595703125	(16.2s)
epoch: 87	train loss: 8416.40625	(16.2s)
epoch: 88	train loss: 8635.3818359375	(16.3s)
epoch: 89	train loss: 11163.3701171875	(16.3s)
epoch: 90	train loss: 9109.8828125	(16.3s)
epoch: 91	train loss: 8709.48828125	(16.2s)
epoch: 92	train loss: 8570.1630859375	(16.2s)
epoch: 93	train loss: 8459.1875	(16.2s)
epoch: 94	train loss: 10747.443359375	(16.3s)
epoch: 95	train loss: 9322.38671875	(16.3s)
epoch: 96	train loss: 9083.77734375	(16.2s)
epoch: 97	train loss: 8518.69921875	(16.2s)
epoch: 98	train loss: 8459.947265625	(16.2s)
epoch: 99	train loss: 8554.6416015625	(16.3s)
Evaluating model on 200 episodes
0.0017615987395402043
0.0016251329594524576
0.002896290156786563
0.003582277102395892
0.0013042965947533958
0.002088436931704304
0.0008694978526667304
0.0026666160159617325
0.003309275180072291
0.0014970789545461433
0.0010067673418754047
0.0008918529794755159
0.002276472198321823
0.003942409246519674
0.001727961908644912
0.0021052385105017144
0.0015190187983534996
0.0002894487988669425
0.0029178348486311734
0.0007989058846890527
0.001211799717566464
0.0009096349961261942
0.0006253068647559055
0.003137835022547127
0.003800585138378665
0.002555625139393669
0.0014104879768897913
0.0008062293763941852
0.0014819335337961093
0.0018695806968025864
0.002682063279540411
0.0017521627984630566
0.001878557279789155
0.001584319759881749
0.004871346462095971
0.0007401370039588073
0.004176251329356871
0.0012485748193284962
0.0023432227928424252
0.003152414335878954
0.0018584602412374807
0.0025688989990158007
0.001756362875312334
0.0022448535197327145
0.0015220620480249636
0.0013108941590568672
0.0021485256758751346
0.0007766791095491499
0.0017238900682059466
0.0013167158390737872
0.0016735887358663604
0.0024485431940775015
0.0011946651735343038
0.0013895555687971257
0.0033641003816834805
0.002559681663115043
0.0011668568682320515
0.001846223502070643
0.0018843484722310677
0.0012689375726040453
0.0016133174566285951
0.0018497166976365925
0.0011730667706265545
0.0016680121298122685
0.00146322278851585
0.001736330059227637
0.0019870887917932123
0.0036423062347239465
0.002217922732234001
0.0013703548271829884
0.0008636530686392992
0.0008097427853499539
0.0015459394882782363
0.0025425220679608173
0.002636339543823851
0.0010226579562675818
0.001997455886036429
0.0031523702175036304
0.001349863229228908
0.00151977029745467
0.0026049250742029733
0.0014629245650632163
0.0014676439295726595
0.0011604024475673214
0.001100049565138761
0.0014748128619430645
0.0010433529751026072
0.0006979250829317607
0.0011998507709197132
0.0009524763668499266
0.0014278104887731995
0.0016087323149948038
0.00027499380909527343
0.0015348077340604505
0.0012593130031746114
0.001113354743574746
0.0014424349508772138
0.0010033622278974184
0.001010209591640887
0.002981636457964972
0.002236743877195598
0.0023249768928508274
0.0013950038082839455
0.003986000578152016
0.0014413400741222387
0.0028449631718103774
0.0020266710891840324
0.0012057781800866082
0.0010179538543785124
0.0017288226680158493
0.0025037723476998507
0.0032044353844997075
0.0019785687826307757
0.001562139370435034
0.002248540108177202
0.0018733373986573756
0.002633538501363041
0.0009030153357473734
0.0017343132667519967
0.001379519716768603
0.0003306121024334888
0.002737460816408404
0.002874122394132428
0.004116808839171426
0.00257622517793524
0.0016769510136024717
0.0012393926554068457
0.001357846403455672
0.0023127719481029394
0.002729874484690039
0.0022183174961745472
0.001575014214674746
0.004639134761722137
0.0033400204265490174
0.001042684450351915
0.0027949808194534854
Solved 136/200 episodes
0.0012905632369809416
Evaluated model in 24.1 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-10
Round 11
Generated trajectories in 72.0 seconds
epoch: 0	train loss: 23081.46484375	(16.3s)
epoch: 1	train loss: 18856.35546875	(16.2s)
epoch: 2	train loss: 17172.810546875	(16.3s)
epoch: 3	train loss: 16110.5546875	(16.3s)
epoch: 4	train loss: 15405.47265625	(16.4s)
epoch: 5	train loss: 15011.0771484375	(16.5s)
epoch: 6	train loss: 14594.447265625	(16.4s)
epoch: 7	train loss: 14031.2607421875	(16.4s)
epoch: 8	train loss: 14047.607421875	(16.4s)
epoch: 9	train loss: 13580.724609375	(16.4s)
epoch: 10	train loss: 13229.7705078125	(16.4s)
epoch: 11	train loss: 12831.6083984375	(16.4s)
epoch: 12	train loss: 13894.615234375	(16.4s)
epoch: 13	train loss: 12906.2158203125	(16.4s)
epoch: 14	train loss: 12325.1611328125	(16.4s)
epoch: 15	train loss: 13012.91796875	(16.4s)
epoch: 16	train loss: 12420.2802734375	(16.5s)
epoch: 17	train loss: 11697.857421875	(16.4s)
epoch: 18	train loss: 11574.9853515625	(16.4s)
epoch: 19	train loss: 11240.04296875	(16.4s)
epoch: 20	train loss: 11432.8779296875	(16.4s)
epoch: 21	train loss: 12647.6103515625	(16.5s)
epoch: 22	train loss: 11513.423828125	(16.4s)
epoch: 23	train loss: 12065.064453125	(16.4s)
epoch: 24	train loss: 11349.5185546875	(16.4s)
epoch: 25	train loss: 10857.54296875	(16.4s)
epoch: 26	train loss: 10960.6875	(16.5s)
epoch: 27	train loss: 10511.9619140625	(16.4s)
epoch: 28	train loss: 10226.759765625	(16.4s)
epoch: 29	train loss: 10741.0732421875	(16.4s)
epoch: 30	train loss: 9968.423828125	(16.4s)
epoch: 31	train loss: 10155.9501953125	(16.4s)
epoch: 32	train loss: 10484.3583984375	(16.4s)
epoch: 33	train loss: 10573.1171875	(16.4s)
epoch: 34	train loss: 14054.7421875	(16.3s)
epoch: 35	train loss: 11381.1279296875	(16.3s)
epoch: 36	train loss: 10456.240234375	(16.3s)
epoch: 37	train loss: 9908.3720703125	(16.4s)
epoch: 38	train loss: 9578.25390625	(16.4s)
epoch: 39	train loss: 9492.3359375	(16.4s)
epoch: 40	train loss: 9674.732421875	(16.4s)
epoch: 41	train loss: 9170.3837890625	(16.4s)
epoch: 42	train loss: 9856.7587890625	(16.4s)
epoch: 43	train loss: 9971.71484375	(16.4s)
epoch: 44	train loss: 9924.263671875	(16.4s)
epoch: 45	train loss: 9758.6142578125	(16.4s)
epoch: 46	train loss: 9292.4970703125	(16.4s)
epoch: 47	train loss: 9419.9453125	(16.4s)
epoch: 48	train loss: 10284.3037109375	(16.4s)
epoch: 49	train loss: 9212.1015625	(16.4s)
epoch: 50	train loss: 8866.2880859375	(16.4s)
epoch: 51	train loss: 9560.2568359375	(16.4s)
epoch: 52	train loss: 9355.5263671875	(16.4s)
epoch: 53	train loss: 9310.318359375	(16.4s)
epoch: 54	train loss: 9205.9658203125	(16.4s)
epoch: 55	train loss: 9646.20703125	(16.4s)
epoch: 56	train loss: 9574.3779296875	(16.4s)
epoch: 57	train loss: 9388.8818359375	(16.4s)
epoch: 58	train loss: 9449.9658203125	(16.4s)
epoch: 59	train loss: 9933.15234375	(16.4s)
epoch: 60	train loss: 9434.99609375	(16.4s)
epoch: 61	train loss: 8982.4287109375	(16.4s)
epoch: 62	train loss: 10033.625	(16.4s)
epoch: 63	train loss: 10970.81640625	(16.4s)
epoch: 64	train loss: 9873.658203125	(16.4s)
epoch: 65	train loss: 8754.3564453125	(16.4s)
epoch: 66	train loss: 8908.23046875	(16.4s)
epoch: 67	train loss: 9179.013671875	(16.4s)
epoch: 68	train loss: 8687.19921875	(16.5s)
epoch: 69	train loss: 8691.6806640625	(16.4s)
epoch: 70	train loss: 9294.939453125	(16.4s)
epoch: 71	train loss: 8475.205078125	(16.4s)
epoch: 72	train loss: 8471.8994140625	(16.4s)
epoch: 73	train loss: 8557.84375	(16.5s)
epoch: 74	train loss: 8285.3115234375	(16.4s)
epoch: 75	train loss: 8628.2705078125	(16.4s)
epoch: 76	train loss: 8173.8671875	(16.4s)
epoch: 77	train loss: 8277.2255859375	(16.4s)
epoch: 78	train loss: 9152.9921875	(16.4s)
epoch: 79	train loss: 8664.0361328125	(16.4s)
epoch: 80	train loss: 8640.56640625	(16.4s)
epoch: 81	train loss: 8098.52783203125	(16.4s)
epoch: 82	train loss: 8321.77734375	(16.4s)
epoch: 83	train loss: 8443.8896484375	(16.4s)
epoch: 84	train loss: 8231.22265625	(16.4s)
epoch: 85	train loss: 8244.9453125	(16.4s)
epoch: 86	train loss: 8670.27734375	(16.4s)
epoch: 87	train loss: 8398.37890625	(16.4s)
epoch: 88	train loss: 8287.9794921875	(16.4s)
epoch: 89	train loss: 8656.79296875	(16.4s)
epoch: 90	train loss: 9994.9755859375	(16.4s)
epoch: 91	train loss: 8828.119140625	(16.4s)
epoch: 92	train loss: 7892.1494140625	(16.4s)
epoch: 93	train loss: 7993.48095703125	(16.4s)
epoch: 94	train loss: 7988.92138671875	(16.5s)
epoch: 95	train loss: 7926.12841796875	(16.4s)
epoch: 96	train loss: 8524.7705078125	(16.4s)
epoch: 97	train loss: 8452.7265625	(16.4s)
epoch: 98	train loss: 8644.908203125	(16.4s)
epoch: 99	train loss: 7815.72021484375	(16.4s)
Evaluating model on 200 episodes
0.0020936348332725174
0.001227604511465567
0.0018808172635544906
0.0015588163423672086
0.0008356265783991798
0.0031954886853782227
0.0009039397054948495
0.00113064322275542
0.0027114623226225376
0.0011617607502557802
0.0012537808157503604
0.0013511439541285984
0.0020001730352470822
0.0018276019691256806
0.0018865058111259714
0.001593420138427367
0.0017637742130318657
0.0023276033405695232
0.0025814350422782204
0.0016637559660011903
0.001792229571340916
0.0018717421712608484
0.0016944164623661588
0.0010443057108204812
0.0019335060159474697
0.0017123120864305425
0.0027729791722127368
0.002077227836707607
0.0031588474618426212
0.001282018497295212
0.0013184485851525096
0.001058528880700275
0.0017412349101505243
0.0014711337324115447
0.0007223978958791122
0.0015761048671265598
0.0012428906084096525
0.001483061635305851
0.0026417746036410486
0.0030515139156098788
0.0012183778211952553
0.001100793309888104
0.001668737434859698
0.0031567436948535033
0.0027338985195716043
0.0013164136362320278
0.0013721001146006443
0.0009449027475056937
0.003287030728824902
0.0013665103469975293
0.001020786386756559
0.0016148206545040012
0.001974679735010593
0.0014437298054872372
0.0010770432301796973
0.0022825152237518605
0.002166004266738045
0.000879242668036438
0.0008637765500819883
0.0018299597237880032
0.0007767229217279237
0.0022450440214015544
0.0012830820932532123
0.0016864412943201994
0.0010272520848957356
0.0015800976398168132
0.0018143068705934898
0.0011465519242587366
0.0009246045827988864
0.003920064748464418
0.0026134431951504665
0.001731247644056566
0.000877584597808035
0.0032187521784281773
0.0028889773675473407
0.001868799672453091
0.0015154198164444616
0.0009692503933495443
0.0020783877483297962
0.0014494319199002347
0.0014312828340431831
0.0029870570888306247
0.0005890650427318178
0.0017769186072958594
0.0017805788676404127
0.00190940385509748
0.000798318537514812
0.0017236245679669083
0.001284964715725639
0.002636595928589276
0.001352043983598168
0.0029190251017488273
0.0014102296881901566
0.002021959808189422
0.0024370654179316866
0.0023568209464428945
0.0010426089614735577
0.0009898270135246484
0.0009692947765870486
0.00041224104514934453
0.002858453001993309
0.002307269604128893
0.0025012383921421133
0.0018805048334448098
0.0006727432184908096
0.0022326185324345717
0.0013890078116673976
0.001786445826424328
0.0013269780463035983
0.0010317807318642735
0.0016880120735246844
0.002555387573213213
0.0017776144668459892
0.0018387984649355833
0.0024225383198687007
0.0019960222824819437
0.001387931351408562
0.0026382965309797632
0.001089540481189033
0.0014150862730780824
0.0013102497614454478
0.0017320344625096325
0.000701723079449342
0.0010009792772157548
0.0017443449550438135
0.0017329060889702912
0.0023627985621195485
0.001757698719706645
0.001733754788438091
0.0015849816241825464
Solved 130/200 episodes
0.001124079291797489
Evaluated model in 25.5 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-11
Round 12
Generated trajectories in 72.8 seconds
epoch: 0	train loss: 20881.09765625	(16.3s)
epoch: 1	train loss: 17084.40234375	(16.3s)
epoch: 2	train loss: 15389.984375	(16.4s)
epoch: 3	train loss: 14448.7421875	(16.4s)
epoch: 4	train loss: 13985.390625	(16.5s)
epoch: 5	train loss: 13294.9169921875	(16.5s)
epoch: 6	train loss: 12920.33984375	(16.5s)
epoch: 7	train loss: 14080.904296875	(16.5s)
epoch: 8	train loss: 12821.095703125	(16.5s)
epoch: 9	train loss: 12221.2919921875	(16.5s)
epoch: 10	train loss: 12475.0302734375	(16.5s)
epoch: 11	train loss: 11974.4697265625	(16.5s)
epoch: 12	train loss: 11546.109375	(16.5s)
epoch: 13	train loss: 11422.58984375	(16.5s)
epoch: 14	train loss: 11278.1318359375	(16.5s)
epoch: 15	train loss: 11107.6708984375	(16.5s)
epoch: 16	train loss: 11153.5595703125	(16.5s)
epoch: 17	train loss: 10927.763671875	(16.5s)
epoch: 18	train loss: 12474.6220703125	(16.5s)
epoch: 19	train loss: 11508.6962890625	(16.5s)
epoch: 20	train loss: 10708.0732421875	(16.5s)
epoch: 21	train loss: 10566.5068359375	(16.6s)
epoch: 22	train loss: 10819.1787109375	(16.5s)
epoch: 23	train loss: 10275.7666015625	(16.5s)
epoch: 24	train loss: 9938.84375	(16.5s)
epoch: 25	train loss: 10100.3798828125	(16.5s)
epoch: 26	train loss: 9639.4892578125	(16.6s)
epoch: 27	train loss: 9704.1083984375	(16.5s)
epoch: 28	train loss: 10087.365234375	(16.5s)
epoch: 29	train loss: 9781.4375	(16.5s)
epoch: 30	train loss: 10425.5986328125	(16.5s)
epoch: 31	train loss: 9844.509765625	(16.5s)
epoch: 32	train loss: 10125.408203125	(16.5s)
epoch: 33	train loss: 10034.912109375	(16.5s)
epoch: 34	train loss: 9608.3662109375	(16.4s)
epoch: 35	train loss: 9510.8349609375	(16.5s)
epoch: 36	train loss: 9399.0537109375	(16.6s)
epoch: 37	train loss: 10365.1044921875	(16.6s)
epoch: 38	train loss: 9571.986328125	(16.4s)
epoch: 39	train loss: 10861.6982421875	(16.4s)
epoch: 40	train loss: 9283.986328125	(16.5s)
epoch: 41	train loss: 8985.39453125	(16.5s)
epoch: 42	train loss: 9518.3984375	(16.5s)
epoch: 43	train loss: 9807.4443359375	(16.5s)
epoch: 44	train loss: 8870.1650390625	(16.5s)
epoch: 45	train loss: 8906.3466796875	(16.5s)
epoch: 46	train loss: 8590.7578125	(16.5s)
epoch: 47	train loss: 8690.056640625	(16.5s)
epoch: 48	train loss: 9636.166015625	(16.6s)
epoch: 49	train loss: 9411.65625	(16.5s)
epoch: 50	train loss: 9322.2919921875	(16.5s)
epoch: 51	train loss: 10953.0771484375	(16.5s)
epoch: 52	train loss: 9528.974609375	(16.6s)
epoch: 53	train loss: 8913.662109375	(16.5s)
epoch: 54	train loss: 8841.2109375	(16.5s)
epoch: 55	train loss: 9598.2861328125	(16.5s)
epoch: 56	train loss: 10753.87109375	(16.5s)
epoch: 57	train loss: 9030.3369140625	(16.5s)
epoch: 58	train loss: 8692.197265625	(16.6s)
epoch: 59	train loss: 8350.779296875	(16.5s)
epoch: 60	train loss: 8552.0146484375	(16.5s)
epoch: 61	train loss: 8353.201171875	(16.5s)
epoch: 62	train loss: 9220.1103515625	(16.5s)
epoch: 63	train loss: 8659.560546875	(16.5s)
epoch: 64	train loss: 8256.7294921875	(16.5s)
epoch: 65	train loss: 8332.6220703125	(16.5s)
epoch: 66	train loss: 8656.056640625	(16.5s)
epoch: 67	train loss: 8900.0966796875	(16.5s)
epoch: 68	train loss: 8611.951171875	(16.5s)
epoch: 69	train loss: 8450.3779296875	(16.5s)
epoch: 70	train loss: 8313.1064453125	(16.5s)
epoch: 71	train loss: 8594.013671875	(16.5s)
epoch: 72	train loss: 8755.2236328125	(16.5s)
epoch: 73	train loss: 8398.7041015625	(16.6s)
epoch: 74	train loss: 8612.6787109375	(16.6s)
epoch: 75	train loss: 8322.6650390625	(16.5s)
epoch: 76	train loss: 8004.59814453125	(16.5s)
epoch: 77	train loss: 8142.46337890625	(16.5s)
epoch: 78	train loss: 8392.298828125	(16.5s)
epoch: 79	train loss: 8549.8759765625	(16.6s)
epoch: 80	train loss: 8482.6474609375	(16.5s)
epoch: 81	train loss: 8295.5927734375	(16.5s)
epoch: 82	train loss: 7897.01513671875	(16.5s)
epoch: 83	train loss: 7907.8505859375	(16.5s)
epoch: 84	train loss: 7980.7021484375	(16.5s)
epoch: 85	train loss: 9243.9423828125	(16.5s)
epoch: 86	train loss: 8112.0771484375	(16.5s)
epoch: 87	train loss: 8341.2802734375	(16.5s)
epoch: 88	train loss: 7742.10986328125	(16.5s)
epoch: 89	train loss: 7946.14990234375	(16.5s)
epoch: 90	train loss: 8291.7099609375	(16.5s)
epoch: 91	train loss: 8107.14794921875	(16.5s)
epoch: 92	train loss: 8127.61279296875	(16.5s)
epoch: 93	train loss: 8578.876953125	(16.5s)
epoch: 94	train loss: 8273.56640625	(16.5s)
epoch: 95	train loss: 7931.697265625	(16.5s)
epoch: 96	train loss: 7644.12060546875	(16.4s)
epoch: 97	train loss: 8371.2490234375	(16.4s)
epoch: 98	train loss: 8458.25	(16.5s)
epoch: 99	train loss: 7755.05859375	(16.5s)
Evaluating model on 200 episodes
0.0003592606980029294
0.0023482840527625135
0.0014522505252071742
0.0016779153578681872
0.0027450929961847386
0.0019658403447725243
0.0020737175291287713
0.0018830994740710594
0.0019395296646204467
0.0015026802994886086
0.0011153633628661435
0.002112852559478667
0.001277306757401675
0.0005623850471844586
0.0016474930081231934
0.0008178926364053041
0.0013192847145546693
0.0028916888368257787
0.0012834328226745129
0.0012801028903649079
0.0010513135691629335
0.0007872141930939895
0.001804923772579059
0.00140603906280982
0.0021412601927295327
0.0018858458672184498
0.0011096384622154194
0.0020796074475381626
0.0011986761164735071
0.0024443848451483063
0.0010794980721079304
0.001363608189752664
0.0011639920606588323
0.0024304293927141568
0.0013095226859149989
0.00294128400502924
0.002486157053176107
0.0021488623396079573
0.002022628216582234
0.0017985300940684585
0.001377941418987965
0.0008871522374895655
0.0008498041299224661
0.0015853491137162424
0.0020678087915127564
0.0019167948903486831
0.002358776621096897
0.00292550493577437
0.0007978364175704138
0.001262043656121629
0.0022435369308888248
0.002764377443236299
0.001204708277197954
0.002925945363434342
0.0006472919649240794
0.004665046010464162
0.0014706851870869286
0.0010340829777608937
0.001309001072513638
0.0011273402827403818
0.0018071805607178248
0.001549858358115531
0.002841481990303278
0.0009224445151630789
0.0015935166147755808
0.0028309503695174284
0.0020336742703588435
0.002115400133334333
0.0006034778343746439
0.001771086419466883
0.0007740085609839297
0.0018227254491648636
0.0015404493809910492
0.0028121074697082804
0.0010361626339848347
0.0022653856593443868
0.0013538886911192093
0.001799864793792949
0.0024314650887390598
0.0017951185178720732
0.0028084370727204564
0.0012931907377523285
0.0010754090985756193
0.002085447045586382
0.0016596777548849267
0.0007405949735483672
0.004253392832582064
0.002885043179127894
0.001814094342989847
0.0022821347779427015
0.0018064014304680033
0.0022260296460618043
0.002232110964743899
0.0013253774589732832
0.0006035048354533501
0.0024048211441064873
0.002070762697258033
0.0017655346618994372
0.0010239264967302525
0.002467971392552895
0.0010497779290744801
0.004591635457472876
0.002771147107523575
0.0016181614927994641
0.002416934520376592
0.0012612781676580199
0.0017748926269739646
0.003291040868498385
0.0007232452270626607
0.0007827109227946493
0.0013118965628867347
0.0014117671084932226
0.003093957124898831
0.0021634742729474965
0.002305887405012842
0.0013535660080378875
0.001436701388063904
0.001618688539190985
0.00144113163525403
0.005089379817945883
0.0022192192923284892
0.0017486123167480784
0.0021624219152727164
0.001723188157484401
0.0025453946318087957
0.0012937304219866003
0.0015033349336590618
0.001975998526904732
0.001912765628471139
0.0008407382859635012
0.0020836486166647067
0.0006035200236841443
0.0018014070786496934
0.001139973446177984
0.0011837026686407626
0.005199101222387981
0.002238361909154004
0.001683697258088684
0.0016898648192229877
0.0016009559595576402
0.001101010522688739
0.0033631831878058924
0.0002825082196472471
0.0018557513249106705
0.0015344346476402798
0.0017632666410160407
0.0007017755051492713
0.0013911659648329078
0.001969762139513319
0.001257427017040563
0.0010149443864625378
0.001970393981277344
Solved 152/200 episodes
0.0013685828081447507
Evaluated model in 25.1 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-12
Round 13
Generated trajectories in 71.5 seconds
epoch: 0	train loss: 18080.904296875	(16.3s)
epoch: 1	train loss: 14437.2734375	(16.2s)
epoch: 2	train loss: 13531.275390625	(16.3s)
epoch: 3	train loss: 12191.619140625	(16.4s)
epoch: 4	train loss: 11455.6787109375	(16.4s)
epoch: 5	train loss: 11146.5263671875	(16.5s)
epoch: 6	train loss: 11436.8212890625	(16.5s)
epoch: 7	train loss: 10737.94140625	(16.4s)
epoch: 8	train loss: 10587.0234375	(16.4s)
epoch: 9	train loss: 10862.982421875	(16.4s)
epoch: 10	train loss: 10394.3193359375	(16.5s)
epoch: 11	train loss: 10127.6787109375	(16.4s)
epoch: 12	train loss: 10278.419921875	(16.4s)
epoch: 13	train loss: 9936.3466796875	(16.4s)
epoch: 14	train loss: 9733.25	(16.4s)
epoch: 15	train loss: 9702.6591796875	(16.5s)
epoch: 16	train loss: 10166.0419921875	(16.5s)
epoch: 17	train loss: 9655.6787109375	(16.4s)
epoch: 18	train loss: 9398.8544921875	(16.4s)
epoch: 19	train loss: 9398.263671875	(16.4s)
epoch: 20	train loss: 9814.541015625	(16.5s)
epoch: 21	train loss: 9650.6689453125	(16.5s)
epoch: 22	train loss: 10059.0400390625	(16.5s)
epoch: 23	train loss: 9395.6435546875	(16.4s)
epoch: 24	train loss: 8982.6591796875	(16.5s)
epoch: 25	train loss: 9117.4541015625	(16.5s)
epoch: 26	train loss: 9539.5126953125	(16.5s)
epoch: 27	train loss: 8744.078125	(16.5s)
epoch: 28	train loss: 8529.6953125	(16.5s)
epoch: 29	train loss: 8317.900390625	(16.5s)
epoch: 30	train loss: 9555.1865234375	(16.6s)
epoch: 31	train loss: 9209.4306640625	(16.5s)
epoch: 32	train loss: 8447.96484375	(16.5s)
epoch: 33	train loss: 8690.037109375	(16.5s)
epoch: 34	train loss: 8729.0380859375	(16.5s)
epoch: 35	train loss: 9032.619140625	(16.5s)
epoch: 36	train loss: 9294.96875	(16.5s)
epoch: 37	train loss: 9066.8046875	(16.5s)
epoch: 38	train loss: 8316.7490234375	(16.4s)
epoch: 39	train loss: 7950.5771484375	(16.4s)
epoch: 40	train loss: 8273.6015625	(16.5s)
epoch: 41	train loss: 8967.9375	(16.6s)
epoch: 42	train loss: 9894.0458984375	(16.5s)
epoch: 43	train loss: 8383.8212890625	(16.5s)
epoch: 44	train loss: 8272.1240234375	(16.5s)
epoch: 45	train loss: 8267.1279296875	(16.5s)
epoch: 46	train loss: 7758.541015625	(16.6s)
epoch: 47	train loss: 8119.01708984375	(16.5s)
epoch: 48	train loss: 8149.67919921875	(16.5s)
epoch: 49	train loss: 8081.6123046875	(16.5s)
epoch: 50	train loss: 8746.9765625	(16.5s)
epoch: 51	train loss: 8065.09814453125	(16.6s)
epoch: 52	train loss: 8347.904296875	(16.5s)
epoch: 53	train loss: 8175.4775390625	(16.5s)
epoch: 54	train loss: 8074.2490234375	(16.5s)
epoch: 55	train loss: 7988.11279296875	(16.5s)
epoch: 56	train loss: 7944.59716796875	(16.6s)
epoch: 57	train loss: 8608.8994140625	(16.5s)
epoch: 58	train loss: 8642.8017578125	(16.5s)
epoch: 59	train loss: 7964.23095703125	(16.5s)
epoch: 60	train loss: 7634.5244140625	(16.5s)
epoch: 61	train loss: 7703.81005859375	(16.6s)
epoch: 62	train loss: 8589.3466796875	(16.5s)
epoch: 63	train loss: 9146.4287109375	(16.5s)
epoch: 64	train loss: 8704.6279296875	(16.5s)
epoch: 65	train loss: 8374.830078125	(16.5s)
epoch: 66	train loss: 8290.7919921875	(16.6s)
epoch: 67	train loss: 7508.24853515625	(16.5s)
epoch: 68	train loss: 7659.1474609375	(16.5s)
epoch: 69	train loss: 7480.5810546875	(16.5s)
epoch: 70	train loss: 7881.423828125	(16.5s)
epoch: 71	train loss: 7450.4833984375	(16.5s)
epoch: 72	train loss: 7591.48095703125	(16.5s)
epoch: 73	train loss: 7431.1728515625	(16.6s)
epoch: 74	train loss: 8241.5849609375	(16.5s)
epoch: 75	train loss: 8697.490234375	(16.5s)
epoch: 76	train loss: 8069.3515625	(16.6s)
epoch: 77	train loss: 7527.08984375	(16.6s)
epoch: 78	train loss: 7790.11572265625	(16.5s)
epoch: 79	train loss: 7752.798828125	(16.5s)
epoch: 80	train loss: 7667.9775390625	(16.5s)
epoch: 81	train loss: 8399.376953125	(16.6s)
epoch: 82	train loss: 7594.24365234375	(16.6s)
epoch: 83	train loss: 7490.2294921875	(16.5s)
epoch: 84	train loss: 7787.11962890625	(16.5s)
epoch: 85	train loss: 7388.06005859375	(16.5s)
epoch: 86	train loss: 7511.1416015625	(16.5s)
epoch: 87	train loss: 7617.48193359375	(16.6s)
epoch: 88	train loss: 7709.4130859375	(16.6s)
epoch: 89	train loss: 8017.029296875	(16.5s)
epoch: 90	train loss: 7420.02294921875	(16.5s)
epoch: 91	train loss: 7620.8720703125	(16.5s)
epoch: 92	train loss: 8184.138671875	(16.6s)
epoch: 93	train loss: 7215.3349609375	(16.6s)
epoch: 94	train loss: 7288.4267578125	(16.5s)
epoch: 95	train loss: 7881.50634765625	(16.5s)
epoch: 96	train loss: 7929.08447265625	(16.5s)
epoch: 97	train loss: 7839.1220703125	(16.6s)
epoch: 98	train loss: 7459.89892578125	(16.6s)
epoch: 99	train loss: 7206.36328125	(16.5s)
Evaluating model on 200 episodes
0.0007425228213833179
0.001055890368297696
0.0015068120986912758
0.0012595708855466607
0.0010573425366041751
0.0038158432462296332
0.0014550135305258804
0.0029342955102846543
0.0007060131921951489
0.0010455864648974966
0.0011709642574260215
0.0016839164794267465
0.005318457251026605
0.0013434449865599163
0.0011699601202149335
0.001049766331561841
0.0014229642360338143
0.0009244584515626128
0.0018848517756850924
0.0018739741486462984
0.0014615389469933386
0.0008373557199471785
0.0009460863981682147
0.0008697541321453173
0.00257046647871343
0.0017738687982071294
0.0020233593399594115
0.0011144502828496375
0.0018261970137245954
0.00039790770897525364
0.0014553230973363568
0.0007648113330235771
0.002052950470477719
0.0008359997915580276
0.001320560104266571
0.0014953813151805662
0.0012598584826264414
0.0016381412902514317
0.002706738916458562
0.000982090641348175
0.0015863808803260326
0.0010746919122704473
0.002588886742159957
0.002038727346290317
0.0017220449135493254
0.0018552471669145713
0.001621086935783216
0.0018229822599096224
0.001040646117568637
0.0006857301422213721
0.0012573254699908341
0.000988040510492283
0.0024108708081863974
0.0012748707314520808
0.00084726914355997
0.0015194249797125601
0.0012203016991406911
0.0016888111697173575
0.0025029454263858497
0.001530265170367784
0.0010959464910099217
0.0030964351350452157
0.0009811291558435187
0.0017538666979817208
0.0005204829756300623
0.002966137410516644
0.001791192131349817
0.0015181843496975488
0.0024454423142338377
0.0028888073976850136
0.0021779530922815737
0.0017047484922971176
0.001660107437055558
0.0021925859318798757
0.0017611817540455377
0.0007998676010174677
0.0038288819778244942
0.0016886332793676826
0.0014476807751634623
0.0038328498412738554
0.0014374668994255795
0.0006030704539625731
0.0015305393637390808
0.0009417606052011251
0.0013806876413582359
0.00027850995485840195
0.0004925626027605696
0.0017370450623275246
0.0005017473400585004
0.0008705551372258924
0.0008950352857937105
0.0009022813290357589
0.001811113376918781
0.0013281857074096998
0.0015932427407030693
0.0016962450590654043
0.002171003951595842
0.001721388115887142
0.0015838662977330387
0.001912278767961722
0.000909594980140145
0.0024112530300044455
0.002561332832071154
0.0012794429486265553
0.001939182925049127
0.001457889842276927
0.0014389180105354171
0.0018180069455411285
0.0007296297304232471
0.0012891270286802734
0.000926633074413985
0.0010413592446841197
0.002334834213308148
0.001456272702246982
0.0014743197868180384
0.001383756248937321
0.001332530458967085
0.0028475216968217865
0.001718216780621636
0.0023140244289110838
0.0018853839343743555
0.00441225687037721
0.001978427927287367
0.0014560473471647128
0.0015107716939787173
0.0012778005766449496
0.0013510135831893422
0.0009728844743221998
0.0023783836489504515
0.0014297800220943332
0.0012466733930826498
0.0008402068924624473
0.0018232886255386152
0.0006741907968716987
0.00048032394697656856
0.0009065489224061215
0.0005925987667675751
0.0029327838294973064
0.0006626023379794788
0.0014341667119879276
0.002132026590697933
0.0022744598682038486
0.0004222351679345593
0.0020164247155965618
0.0016154941346030681
0.0013681528576211456
0.0020647417441068683
0.001700094850093592
0.0005963806241717455
0.0019661941703154957
0.0022199432213609624
0.002326649447240925
0.0017914290910994168
0.0016301155527697927
0.001134279035613872
0.0014064176406723813
0.001218969286128413
0.0004946033033775166
0.0012565852034640557
Solved 159/200 episodes
0.0012555845806566826
Evaluated model in 26.8 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-13
Round 14
Generated trajectories in 73.3 seconds
epoch: 0	train loss: 15063.568359375	(16.4s)
epoch: 1	train loss: 12201.0048828125	(16.4s)
epoch: 2	train loss: 11109.798828125	(16.4s)
epoch: 3	train loss: 11173.115234375	(16.5s)
epoch: 4	train loss: 10755.2978515625	(16.5s)
epoch: 5	train loss: 10516.6591796875	(16.5s)
epoch: 6	train loss: 10136.1044921875	(16.5s)
epoch: 7	train loss: 9814.7216796875	(16.5s)
epoch: 8	train loss: 9943.365234375	(16.5s)
epoch: 9	train loss: 10472.3642578125	(16.5s)
epoch: 10	train loss: 9807.548828125	(16.4s)
epoch: 11	train loss: 9540.1064453125	(16.4s)
epoch: 12	train loss: 9599.755859375	(16.4s)
epoch: 13	train loss: 9387.8046875	(16.4s)
epoch: 14	train loss: 8896.1484375	(16.4s)
epoch: 15	train loss: 8606.658203125	(16.4s)
epoch: 16	train loss: 8863.7822265625	(16.4s)
epoch: 17	train loss: 9418.18359375	(16.4s)
epoch: 18	train loss: 9208.8271484375	(16.4s)
epoch: 19	train loss: 8687.154296875	(16.4s)
epoch: 20	train loss: 9087.625	(16.4s)
epoch: 21	train loss: 8997.23046875	(16.4s)
epoch: 22	train loss: 8704.4951171875	(16.4s)
epoch: 23	train loss: 9777.775390625	(16.4s)
epoch: 24	train loss: 8788.6923828125	(16.4s)
epoch: 25	train loss: 8601.4755859375	(16.4s)
epoch: 26	train loss: 8484.119140625	(16.4s)
epoch: 27	train loss: 8316.9150390625	(16.4s)
epoch: 28	train loss: 8393.1845703125	(16.4s)
epoch: 29	train loss: 8454.4404296875	(16.4s)
epoch: 30	train loss: 9340.4609375	(16.4s)
epoch: 31	train loss: 8607.267578125	(16.4s)
epoch: 32	train loss: 8692.41796875	(16.4s)
epoch: 33	train loss: 8132.8818359375	(16.5s)
epoch: 34	train loss: 8712.064453125	(16.4s)
epoch: 35	train loss: 8414.8447265625	(16.4s)
epoch: 36	train loss: 8018.88818359375	(16.3s)
epoch: 37	train loss: 7784.9951171875	(16.3s)
epoch: 38	train loss: 8063.220703125	(16.5s)
epoch: 39	train loss: 8273.9033203125	(16.5s)
epoch: 40	train loss: 10229.7109375	(16.4s)
epoch: 41	train loss: 8722.0341796875	(16.5s)
epoch: 42	train loss: 7920.09619140625	(16.4s)
epoch: 43	train loss: 7708.51513671875	(16.5s)
epoch: 44	train loss: 7781.85791015625	(16.5s)
epoch: 45	train loss: 7884.025390625	(16.5s)
epoch: 46	train loss: 7886.169921875	(16.4s)
epoch: 47	train loss: 8585.6572265625	(16.5s)
epoch: 48	train loss: 8200.380859375	(16.5s)
epoch: 49	train loss: 8273.9970703125	(16.5s)
epoch: 50	train loss: 8793.9794921875	(16.4s)
epoch: 51	train loss: 7860.6455078125	(16.5s)
epoch: 52	train loss: 7731.2314453125	(16.4s)
epoch: 53	train loss: 7493.1416015625	(16.5s)
epoch: 54	train loss: 7443.28369140625	(16.5s)
epoch: 55	train loss: 9001.5009765625	(16.5s)
epoch: 56	train loss: 8526.99609375	(16.4s)
epoch: 57	train loss: 8359.4990234375	(16.5s)
epoch: 58	train loss: 7907.61572265625	(16.5s)
epoch: 59	train loss: 9333.0830078125	(16.5s)
epoch: 60	train loss: 7984.61279296875	(16.5s)
epoch: 61	train loss: 7541.92626953125	(16.5s)
epoch: 62	train loss: 7486.2080078125	(16.4s)
epoch: 63	train loss: 7397.69580078125	(16.5s)
epoch: 64	train loss: 7854.82275390625	(16.5s)
epoch: 65	train loss: 7545.9072265625	(16.5s)
epoch: 66	train loss: 7532.73828125	(16.4s)
epoch: 67	train loss: 7559.455078125	(16.5s)
epoch: 68	train loss: 7965.298828125	(16.4s)
epoch: 69	train loss: 7913.97412109375	(16.5s)
epoch: 70	train loss: 8840.6796875	(16.5s)
epoch: 71	train loss: 7573.1572265625	(16.4s)
epoch: 72	train loss: 7441.98046875	(16.4s)
epoch: 73	train loss: 7156.1591796875	(16.4s)
epoch: 74	train loss: 7171.384765625	(16.5s)
epoch: 75	train loss: 7492.9130859375	(16.5s)
epoch: 76	train loss: 10743.0419921875	(16.4s)
epoch: 77	train loss: 8079.5771484375	(16.4s)
epoch: 78	train loss: 7748.95166015625	(16.5s)
epoch: 79	train loss: 7323.59619140625	(16.5s)
epoch: 80	train loss: 7540.78466796875	(16.5s)
epoch: 81	train loss: 7639.48388671875	(16.4s)
epoch: 82	train loss: 7358.15771484375	(16.4s)
epoch: 83	train loss: 7675.28662109375	(16.4s)
epoch: 84	train loss: 7384.97021484375	(16.5s)
epoch: 85	train loss: 9022.7177734375	(16.5s)
epoch: 86	train loss: 7591.1884765625	(16.5s)
epoch: 87	train loss: 7144.2119140625	(16.4s)
epoch: 88	train loss: 7407.791015625	(16.5s)
epoch: 89	train loss: 7652.4755859375	(16.5s)
epoch: 90	train loss: 8287.13671875	(16.5s)
epoch: 91	train loss: 7403.55322265625	(16.4s)
epoch: 92	train loss: 7226.13818359375	(16.4s)
epoch: 93	train loss: 7801.21826171875	(16.4s)
epoch: 94	train loss: 7308.08984375	(16.5s)
epoch: 95	train loss: 8372.560546875	(16.4s)
epoch: 96	train loss: 7245.185546875	(16.4s)
epoch: 97	train loss: 7246.72900390625	(16.4s)
epoch: 98	train loss: 7987.56494140625	(16.4s)
epoch: 99	train loss: 8401.626953125	(16.5s)
Evaluating model on 200 episodes
0.0025531260971547454
0.001348168938420713
0.002907197194872424
0.0017532809401422532
0.0014688375900732352
0.0018984656766406261
0.0013228836610323924
0.0016312969494790125
0.0010772323666969896
0.0022363562417343596
0.00084178365137805
0.0015654744030061092
0.0016986519774599444
0.001546462160250586
0.0018011063131360481
0.0009946281898010056
0.0016785940949505726
0.00261273941908836
0.002262090648097607
0.0024516143986891784
0.0010380203223446318
0.001299578223169144
0.001081696078472305
0.0027404575657783425
0.0009746861105668359
0.0017632637334047882
0.0015868709657903917
0.0017675408016657457
0.0015810789918759838
0.0016590773702773731
0.002126489122019848
0.001877073957240844
0.0019461786079253735
0.0014519020267471205
0.0017364228371297941
0.000645845387355491
0.0011583136053689355
0.002304920480431368
0.0017681357738397578
0.0011743409850168973
0.0018794891566358274
0.0019911742056137882
0.0006005522591294721
0.0018247815801853153
0.001029923220182984
0.0014369318711995969
0.0009641621541959467
0.002453983404848259
0.0018035956244501802
0.0015631116397604014
0.003209141248791841
0.0015884842912782915
0.0015594258483570928
0.0011467239586636425
0.0023571515746880324
0.0014971526252338663
0.00199670032695091
0.0023228183369307467
0.0017126908714999445
0.0010886470284146657
0.00038501508606714197
0.0018603040698508266
0.00133478138159262
0.0028157578120814064
0.0022600220137974246
0.003051876681898388
0.0007075998692016583
0.0008127835323896513
0.0019075968037941494
0.0010327804511689465
0.0015709437330339202
0.002712110852573662
0.0011531040848543246
0.00291436090298124
0.0015474033367354422
0.0007428085071016819
0.003995569626567885
0.0019885381187027392
0.0008595727839357485
0.0014726989425253123
0.0012016644468531013
0.0011089050327427684
0.002184743182442617
0.0025768472914933227
0.0009488966202803163
0.0019392395042814314
0.0026735877812219164
0.004455616926861694
0.0009287087615865113
0.0016056382541137283
0.0010867490394351382
0.0014558985910428288
0.0013743180067346354
0.001069287412489454
0.0011504448015734346
0.0012355097132967785
0.0012273370448383503
0.0012636772622095628
0.001854259792889934
0.0017365071042149793
0.0013709429800251706
0.0014493188690444779
0.001383056399804032
0.0014368707779794931
0.0014546942315064371
0.0008030741246329853
0.001795163898350438
0.002095350555795118
0.0021353409560665917
0.0016676961589837448
0.0017981329087474892
0.0013413703206672587
0.0018019629613263532
0.0015226374904159456
0.0008069915895569412
0.0006198515520736692
0.0010363417841290357
0.0024196175048321797
0.0021433318771804416
0.0014945989508608666
0.0010786752399629526
0.0028265052678762004
0.0006594671964800606
0.0019035841224346465
0.0014433478354476392
0.0007699529069213895
0.0022401713119636823
0.001916160127231186
0.0012650507596845273
0.001106296101009802
0.0005501395750219341
0.0015543282847829687
0.002887924964306876
0.0015055397764496905
0.0011324254949365488
0.00194804401575024
0.001592113485936639
0.002767434586227561
0.0014247643069692833
0.0009356229711556807
0.001006941226660274
0.0020585264394278914
0.0015511254627759068
0.0007110698034011875
0.001487072790041566
0.0016558006878207938
0.0019008129051404207
0.0007651470946257177
0.002717725974175015
0.001689103202527174
0.002473900111259094
0.0017684878257568926
0.0011644711266853847
0.0033850263118640417
0.0019351698365426273
0.001991175649560649
0.0009305671096976417
0.0014480684059284008
0.0018552733987841445
0.0020217731278255368
0.0023541562908836306
0.00202695441354687
0.001622961987907599
0.0007209658369367175
0.0027714124818365754
0.0009382987388692397
0.002572563898866065
0.0017887237174818438
0.0015611533191986382
0.0013711336603433745
Solved 170/200 episodes
0.0014194572365294595
Evaluated model in 26.8 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-14
Round 15
Generated trajectories in 71.6 seconds
epoch: 0	train loss: 13688.021484375	(16.3s)
epoch: 1	train loss: 11275.9208984375	(16.2s)
epoch: 2	train loss: 10359.8984375	(16.3s)
epoch: 3	train loss: 9811.666015625	(16.4s)
epoch: 4	train loss: 9927.9326171875	(16.4s)
epoch: 5	train loss: 10004.6875	(16.5s)
epoch: 6	train loss: 9514.650390625	(16.4s)
epoch: 7	train loss: 9178.90234375	(16.4s)
epoch: 8	train loss: 9282.859375	(16.4s)
epoch: 9	train loss: 9138.537109375	(16.4s)
epoch: 10	train loss: 9264.091796875	(16.5s)
epoch: 11	train loss: 8850.16796875	(16.4s)
epoch: 12	train loss: 8753.39453125	(16.4s)
epoch: 13	train loss: 8670.7333984375	(16.4s)
epoch: 14	train loss: 8774.044921875	(16.4s)
epoch: 15	train loss: 8272.3818359375	(16.4s)
epoch: 16	train loss: 8445.3359375	(16.4s)
epoch: 17	train loss: 8298.9091796875	(16.4s)
epoch: 18	train loss: 8838.2275390625	(16.4s)
epoch: 19	train loss: 8788.818359375	(16.4s)
epoch: 20	train loss: 9652.185546875	(16.4s)
epoch: 21	train loss: 9123.1904296875	(16.5s)
epoch: 22	train loss: 8317.43359375	(16.4s)
epoch: 23	train loss: 7940.29443359375	(16.4s)
epoch: 24	train loss: 8127.490234375	(16.4s)
epoch: 25	train loss: 7778.76953125	(16.4s)
epoch: 26	train loss: 7981.0244140625	(16.4s)
epoch: 27	train loss: 8151.39404296875	(16.4s)
epoch: 28	train loss: 8047.24365234375	(16.4s)
epoch: 29	train loss: 8599.595703125	(16.3s)
epoch: 30	train loss: 9137.0693359375	(16.3s)
epoch: 31	train loss: 8439.7470703125	(16.3s)
epoch: 32	train loss: 7973.95751953125	(16.2s)
epoch: 33	train loss: 7743.38330078125	(16.2s)
epoch: 34	train loss: 7813.8564453125	(16.2s)
epoch: 35	train loss: 7965.79443359375	(16.2s)
epoch: 36	train loss: 8049.93505859375	(16.2s)
epoch: 37	train loss: 8490.490234375	(16.2s)
epoch: 38	train loss: 8316.9951171875	(16.1s)
epoch: 39	train loss: 7900.26416015625	(16.1s)
epoch: 40	train loss: 7672.27587890625	(16.2s)
epoch: 41	train loss: 7691.9990234375	(16.3s)
epoch: 42	train loss: 9051.6708984375	(16.2s)
epoch: 43	train loss: 7863.95751953125	(16.2s)
epoch: 44	train loss: 7461.048828125	(16.2s)
epoch: 45	train loss: 7992.51953125	(16.2s)
epoch: 46	train loss: 9032.384765625	(16.3s)
epoch: 47	train loss: 7952.4228515625	(16.2s)
epoch: 48	train loss: 7659.4677734375	(16.2s)
epoch: 49	train loss: 7766.869140625	(16.2s)
epoch: 50	train loss: 7892.82177734375	(16.3s)
epoch: 51	train loss: 7350.4287109375	(16.4s)
epoch: 52	train loss: 7210.1279296875	(16.4s)
epoch: 53	train loss: 7201.33544921875	(16.3s)
epoch: 54	train loss: 7892.61376953125	(16.3s)
epoch: 55	train loss: 7906.23486328125	(16.3s)
epoch: 56	train loss: 7769.17626953125	(16.4s)
epoch: 57	train loss: 7817.72021484375	(16.5s)
epoch: 58	train loss: 8170.43505859375	(16.4s)
epoch: 59	train loss: 7403.7158203125	(16.3s)
epoch: 60	train loss: 7212.3818359375	(16.3s)
epoch: 61	train loss: 7162.1298828125	(16.3s)
epoch: 62	train loss: 7582.39892578125	(16.3s)
epoch: 63	train loss: 7869.4599609375	(16.3s)
epoch: 64	train loss: 7697.21044921875	(16.3s)
epoch: 65	train loss: 7001.6220703125	(16.3s)
epoch: 66	train loss: 7123.10791015625	(16.3s)
epoch: 67	train loss: 7903.576171875	(16.4s)
epoch: 68	train loss: 7528.15087890625	(16.3s)
epoch: 69	train loss: 8697.9912109375	(16.3s)
epoch: 70	train loss: 8502.8134765625	(16.3s)
epoch: 71	train loss: 7497.18603515625	(16.4s)
epoch: 72	train loss: 7745.25390625	(16.4s)
epoch: 73	train loss: 7608.9736328125	(16.3s)
epoch: 74	train loss: 7549.501953125	(16.3s)
epoch: 75	train loss: 7057.7353515625	(16.3s)
epoch: 76	train loss: 7449.91357421875	(16.3s)
epoch: 77	train loss: 7285.77197265625	(16.3s)
epoch: 78	train loss: 7265.76123046875	(16.3s)
epoch: 79	train loss: 7414.37548828125	(16.3s)
epoch: 80	train loss: 7614.88134765625	(16.3s)
epoch: 81	train loss: 7003.99853515625	(16.3s)
epoch: 82	train loss: 7159.76708984375	(16.4s)
epoch: 83	train loss: 7320.72412109375	(16.3s)
epoch: 84	train loss: 6964.48583984375	(16.3s)
epoch: 85	train loss: 7316.505859375	(16.3s)
epoch: 86	train loss: 7263.2705078125	(16.3s)
epoch: 87	train loss: 7232.84765625	(16.3s)
epoch: 88	train loss: 6936.33837890625	(16.3s)
epoch: 89	train loss: 7146.8779296875	(16.3s)
epoch: 90	train loss: 7322.9267578125	(16.3s)
epoch: 91	train loss: 7210.8935546875	(16.4s)
epoch: 92	train loss: 7097.90234375	(16.4s)
epoch: 93	train loss: 7388.193359375	(16.4s)
epoch: 94	train loss: 7926.01220703125	(16.3s)
epoch: 95	train loss: 7105.79296875	(16.3s)
epoch: 96	train loss: 7048.45703125	(16.3s)
epoch: 97	train loss: 7295.57958984375	(16.3s)
epoch: 98	train loss: 7193.24072265625	(16.3s)
epoch: 99	train loss: 6979.5751953125	(16.2s)
Evaluating model on 200 episodes
0.0008930525883832681
0.0015732227576126417
0.0005378150840856092
0.0012179609613147996
0.0005589850126333269
0.0010230827475814813
0.0016959346114442898
0.0008679683684022166
0.0012336362719493495
0.001270620308945502
0.002012015754428974
0.001609630516031757
0.0012173955198401625
0.0007639523810212268
0.0011474010626491356
0.001177702223815556
0.0006050904281437397
0.0007117331411931305
0.0006440934317652136
0.0008045332557230722
0.002476517478498863
0.0012599507172126323
0.0011360417331161444
0.0007984616386238486
0.0007418560756680866
0.0016039082329371013
0.001769575755073068
0.001173842625384599
0.0016988129646051675
0.0016096981859542818
0.0016280522395391017
0.00042488625048362237
0.0012325346491707024
0.0007932516600703821
0.0007467624272976536
0.0010497970118497808
0.0010092321849697328
0.0010886893282481146
0.0011818002005990517
0.00114834240230266
0.0011522078250556277
0.00039925921217737984
0.0004771091844304465
0.0009828985497733812
0.0020780980376002844
0.0011120509765392994
0.0014840891933999955
0.0005217521465965547
0.0014112641921896925
0.001478780356895489
0.001735632715281099
0.0009429457303485833
0.0017648862085479777
0.001034087614971213
0.001574470171776325
0.0011998715526715387
0.0014249130445023184
0.0009090004969039001
0.0012804814337869174
0.000881340733030811
0.0010746884072432294
0.0009370654646007001
0.001599031274963636
0.0008781334974628408
0.000881895819994887
0.0007097209616202033
0.0018239246106215592
0.001698228828900028
0.001361645101018152
0.0016185824774549388
0.000813434791052714
0.0020676982596341984
0.0009137913672020659
0.001989258068947079
0.001034613653246197
0.0017039697583337936
0.0013695749609420698
0.00227943689808146
0.001464131782995537
0.0008804444779260931
0.0012114618245201807
0.0011966298799961806
0.002268562100653071
0.001535876231112828
0.0020337812699532756
0.0017916894422443067
0.002032806590755677
0.0017899755200089299
0.0023409837788979835
0.0017036124939165478
0.003863063645743144
0.001959665875574501
0.0014049064794610451
0.0012105240472010336
0.001666357605245139
0.0013518213781935629
0.001458083502560233
0.0018880098525966918
0.0016380565007588861
0.0010604076378513128
0.00044865935342386365
0.001502650903906518
0.0013364134677734303
0.0011203664653294254
0.0012215022021943394
0.0005652786778227892
0.0009475324456746291
0.0011613388905971078
0.001565185811509414
0.0006952064993259098
0.0008626697267573036
0.0011976410987699637
0.001353560157804168
0.0016977113991742954
0.001966747804544866
0.0008130657054910747
0.0027681173887685873
0.001302817981922999
0.0010920743760917421
0.0012429440238823494
0.0013894436764530838
0.0019008467454113998
0.0008590260646694029
0.0007922729288111441
0.0017972261775867082
0.0018524637271184474
0.0019487925690974896
0.0005173613983060932
0.001656737094890559
0.0012507717765402049
0.0016889858299691696
0.0014614324827562086
0.0005660471215378493
0.0016152292638170447
0.0012169932422693818
0.0013573426840594038
0.0008486550327183472
0.0018483498335828902
0.001617343725748859
0.0018928458501415496
0.0011903391273032564
0.0010795902713393193
0.001558660650906469
0.0008550577954987862
0.004167134120680636
0.0016665689399815165
0.0009861352490062065
0.0010270703336573207
0.0009545594017254189
0.0009577825549058616
0.0007441669131367235
0.000797929543366011
0.00044267390088255826
0.0008199621324820328
0.0015551498867213153
0.0019813743769191206
0.0008311545752803795
0.001760935188749021
0.0014688116472611641
0.0020235623834499467
0.0014113757133600302
0.0008236029073790027
0.0013473644704880337
0.0014985428751970176
0.0015197636370430701
0.0008843650751562867
0.0012402286320138955
0.0016430408770247596
0.0013274782639042394
0.0017633105954802078
0.0012473515453166328
0.0013336897191038588
Solved 172/200 episodes
0.0011416845529775203
Evaluated model in 26.2 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-15
Round 16
Generated trajectories in 72.1 seconds
epoch: 0	train loss: 13213.2880859375	(16.2s)
epoch: 1	train loss: 10636.34375	(16.1s)
epoch: 2	train loss: 10045.1708984375	(16.3s)
epoch: 3	train loss: 9924.349609375	(16.4s)
epoch: 4	train loss: 9484.2294921875	(16.4s)
epoch: 5	train loss: 9244.30078125	(16.4s)
epoch: 6	train loss: 9243.697265625	(16.3s)
epoch: 7	train loss: 9019.068359375	(16.3s)
epoch: 8	train loss: 8913.595703125	(16.4s)
epoch: 9	train loss: 9131.7197265625	(16.4s)
epoch: 10	train loss: 8798.412109375	(16.4s)
epoch: 11	train loss: 10095.775390625	(16.3s)
epoch: 12	train loss: 8798.6787109375	(16.3s)
epoch: 13	train loss: 9380.966796875	(16.4s)
epoch: 14	train loss: 8440.2021484375	(16.4s)
epoch: 15	train loss: 8308.6259765625	(16.4s)
epoch: 16	train loss: 8285.9130859375	(16.4s)
epoch: 17	train loss: 8038.3232421875	(16.4s)
epoch: 18	train loss: 8281.017578125	(16.4s)
epoch: 19	train loss: 7991.16162109375	(16.4s)
epoch: 20	train loss: 8266.9453125	(16.4s)
epoch: 21	train loss: 8055.72802734375	(16.4s)
epoch: 22	train loss: 8063.56396484375	(16.3s)
epoch: 23	train loss: 8479.400390625	(16.4s)
epoch: 24	train loss: 8170.51953125	(16.4s)
epoch: 25	train loss: 8410.7451171875	(16.4s)
epoch: 26	train loss: 7997.50927734375	(16.4s)
epoch: 27	train loss: 7957.9482421875	(16.3s)
epoch: 28	train loss: 7921.712890625	(16.4s)
epoch: 29	train loss: 7752.42236328125	(16.4s)
epoch: 30	train loss: 8195.8037109375	(16.4s)
epoch: 31	train loss: 8282.62890625	(16.4s)
epoch: 32	train loss: 7805.7607421875	(16.3s)
epoch: 33	train loss: 7737.30078125	(16.4s)
epoch: 34	train loss: 7779.10205078125	(16.4s)
epoch: 35	train loss: 8452.6533203125	(16.4s)
epoch: 36	train loss: 9373.404296875	(16.3s)
epoch: 37	train loss: 7813.0703125	(16.3s)
epoch: 38	train loss: 7596.28076171875	(16.3s)
epoch: 39	train loss: 7747.75634765625	(16.3s)
epoch: 40	train loss: 7594.8779296875	(16.3s)
epoch: 41	train loss: 7649.671875	(16.3s)
epoch: 42	train loss: 7521.5576171875	(16.3s)
epoch: 43	train loss: 7734.9033203125	(16.4s)
epoch: 44	train loss: 8376.8232421875	(16.4s)
epoch: 45	train loss: 8889.34375	(16.4s)
epoch: 46	train loss: 7979.97705078125	(16.4s)
epoch: 47	train loss: 7902.466796875	(16.4s)
epoch: 48	train loss: 7473.4453125	(16.4s)
epoch: 49	train loss: 7308.68115234375	(16.4s)
epoch: 50	train loss: 7754.6005859375	(16.4s)
epoch: 51	train loss: 7262.6611328125	(16.4s)
epoch: 52	train loss: 7247.92041015625	(16.4s)
epoch: 53	train loss: 7716.78271484375	(16.4s)
epoch: 54	train loss: 7602.52392578125	(16.4s)
epoch: 55	train loss: 8024.40869140625	(16.5s)
epoch: 56	train loss: 7175.2412109375	(16.4s)
epoch: 57	train loss: 7145.8662109375	(16.4s)
epoch: 58	train loss: 7212.533203125	(16.4s)
epoch: 59	train loss: 8248.6767578125	(16.4s)
epoch: 60	train loss: 7446.1669921875	(16.4s)
epoch: 61	train loss: 7966.8408203125	(16.4s)
epoch: 62	train loss: 7354.115234375	(16.4s)
epoch: 63	train loss: 8443.4228515625	(16.4s)
epoch: 64	train loss: 8159.62158203125	(16.3s)
epoch: 65	train loss: 7558.7822265625	(16.5s)
epoch: 66	train loss: 7334.18798828125	(16.4s)
epoch: 67	train loss: 7017.19091796875	(16.4s)
epoch: 68	train loss: 7608.49755859375	(16.4s)
epoch: 69	train loss: 6869.55859375	(16.4s)
epoch: 70	train loss: 6728.59033203125	(16.4s)
epoch: 71	train loss: 7093.05078125	(16.4s)
epoch: 72	train loss: 8383.26171875	(16.4s)
epoch: 73	train loss: 8383.7158203125	(16.4s)
epoch: 74	train loss: 7590.75048828125	(16.4s)
epoch: 75	train loss: 7468.3798828125	(16.4s)
epoch: 76	train loss: 7078.81005859375	(16.4s)
epoch: 77	train loss: 6988.35546875	(16.4s)
epoch: 78	train loss: 6779.8486328125	(16.4s)
epoch: 79	train loss: 6938.98291015625	(16.4s)
epoch: 80	train loss: 7336.99462890625	(16.4s)
epoch: 81	train loss: 7800.34716796875	(16.4s)
epoch: 82	train loss: 7034.431640625	(16.4s)
epoch: 83	train loss: 7292.7587890625	(16.4s)
epoch: 84	train loss: 6794.107421875	(16.4s)
epoch: 85	train loss: 6890.52880859375	(16.4s)
epoch: 86	train loss: 7115.57275390625	(16.4s)
epoch: 87	train loss: 7161.3212890625	(16.4s)
epoch: 88	train loss: 8436.162109375	(16.4s)
epoch: 89	train loss: 7054.88916015625	(16.4s)
epoch: 90	train loss: 6883.85546875	(16.4s)
epoch: 91	train loss: 6596.01171875	(16.4s)
epoch: 92	train loss: 7837.4638671875	(16.4s)
epoch: 93	train loss: 7918.61376953125	(16.4s)
epoch: 94	train loss: 7208.81884765625	(16.4s)
epoch: 95	train loss: 7076.91259765625	(16.4s)
epoch: 96	train loss: 7010.22607421875	(16.4s)
epoch: 97	train loss: 7195.08349609375	(16.4s)
epoch: 98	train loss: 6784.78466796875	(16.4s)
epoch: 99	train loss: 6958.9072265625	(16.4s)
Evaluating model on 200 episodes
0.0011906025358092104
0.0015267811686499044
0.0009964003937270986
0.000754009352224077
0.0009251992727513425
0.0013879602641585127
0.0030379491467707946
0.001431822061931598
0.0016019913236959837
0.000749487542634597
0.001072702448644642
0.001210514930010374
0.001254671613828072
0.001654598418519107
0.0015361741607193835
0.001776085926394444
0.0010115761037417946
0.0011636837474118995
0.0017370828194803575
0.0007919462368590757
0.0016644177352039453
0.0017089451701560226
0.0015919135978037958
0.0020575069990757455
0.000949370407549092
0.0016930489978221885
0.0010102997633415118
0.0008692246378814161
0.0015705459310473607
0.002498856349120615
0.000918320611659014
0.0010437686778459465
0.0016337546985596418
0.0012839530838391511
0.0011480791630934359
0.0012609544949858293
0.001762403805009348
0.001980520290089771
0.00045020249076928434
0.00039989886265954865
0.0011248198933411484
0.0022219316199172884
0.0031698785838671028
0.000914900602462391
0.00042990325091523117
0.0010072830683544882
0.0009056464783498086
0.0013852132896621091
0.0019805030518909917
0.00189592651132933
0.0010655633523128927
0.00047140203225188253
0.0010144885000045178
0.0027322664638631975
0.0006217691477692878
0.0005936449422733858
0.002248140488202056
0.0006643816653065317
0.0008348989121211497
0.0005417514574433238
0.001309602077162708
0.0023588702509490154
0.001337180504479976
0.0020536579997497027
0.0019118667560178437
0.0005221906071710691
0.0009935914543651354
0.0011815359950560378
0.0006118119885546289
0.0008693480295358528
0.0009139705913791355
0.0011835896511911415
0.001737835542735411
0.003759251798328478
0.0008604431528106539
0.0012917921301171494
0.0012239692387083778
0.0008376880998386898
0.002139923409281942
0.001618436867699659
0.000809943545027636
0.0014395024691111757
0.003262306106626056
0.0015293631780271728
0.00033537545358639263
0.0010569625063605295
0.001647408872356339
0.0005213706033828202
0.0015974713811980716
0.0009153545561275678
0.0014074592755230911
0.0010437784189321064
0.0010481489243829856
0.0005941535582678625
0.0009895312480997138
0.0015404136194976875
0.0030312769158626907
0.0012985555555739363
0.001731854608199986
0.0022772155358931436
0.0013900612111683586
0.003634470854608067
0.0016599413760228674
0.0004177346579256534
0.0012207152571396104
0.0017113872454501688
0.0013593691276452904
0.00105091774767061
0.0019383754866873421
0.0019928241640627675
0.001310603265924978
0.0014762089510137837
0.000950936725447801
0.0018714797085428064
0.0014532402982594147
0.0016734279543015873
0.0006420065243825471
0.0013611858844847626
0.0007446982182206257
0.0031977484856421747
0.001634893710919035
0.0010833214204467368
0.0011184564177443355
0.0004311718234930595
0.0013602513191632599
0.0006610263051697984
0.00010836946583165425
0.001265329480312565
0.0010074536502625879
0.0014641797952208435
0.0021140032412614138
0.0009083609590615036
0.0009605882419742153
0.0016744544854667592
0.0008212833551321334
0.0010963578956412116
0.0017837868104574486
0.0015568725066259503
0.0011947407906105086
0.0012049093435052781
0.002150622038041345
0.0018630897247930989
0.0016168765306128594
0.0010580635247275825
0.0019002415703233476
0.0015545732920145383
0.0008016425808818894
0.0004976386711859959
0.0008521481529235396
0.001069036035460158
0.0018538670002661612
0.001655669970075126
0.0009333572646331353
0.0012781461005033634
0.0017271048373703917
0.0008221175958169624
0.0017659425273970036
0.0009671002851693908
0.0008705423787807162
0.0006066128255042714
0.0007439752053786916
0.000879617750797479
0.0011255362019605504
0.0020887644517642912
0.0008634089356443534
0.001414162458281175
0.0011345337277982087
0.0011903521704620548
0.0011346927059583743
0.0013336577685549855
0.001581991687999107
0.0008302620399263105
0.0010492261629094304
Solved 173/200 episodes
0.0011632362968055
Evaluated model in 27.1 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-16
Round 17
Generated trajectories in 72.2 seconds
epoch: 0	train loss: 13533.5947265625	(16.4s)
epoch: 1	train loss: 10404.2001953125	(16.3s)
epoch: 2	train loss: 9619.7470703125	(16.4s)
epoch: 3	train loss: 9233.5068359375	(16.5s)
epoch: 4	train loss: 9001.73046875	(16.4s)
epoch: 5	train loss: 8819.876953125	(16.5s)
epoch: 6	train loss: 8930.46875	(16.4s)
epoch: 7	train loss: 8633.595703125	(16.6s)
epoch: 8	train loss: 9242.13671875	(16.5s)
epoch: 9	train loss: 8667.953125	(16.6s)
epoch: 10	train loss: 8945.4990234375	(16.6s)
epoch: 11	train loss: 8820.9453125	(16.5s)
epoch: 12	train loss: 8923.259765625	(16.6s)
epoch: 13	train loss: 8757.8173828125	(16.6s)
epoch: 14	train loss: 8845.5439453125	(16.5s)
epoch: 15	train loss: 8256.0625	(16.4s)
epoch: 16	train loss: 8529.2783203125	(16.6s)
epoch: 17	train loss: 8069.0546875	(16.6s)
epoch: 18	train loss: 7963.75732421875	(16.6s)
epoch: 19	train loss: 8533.1220703125	(16.4s)
epoch: 20	train loss: 8265.669921875	(16.6s)
epoch: 21	train loss: 8322.0830078125	(16.6s)
epoch: 22	train loss: 8088.44873046875	(16.6s)
epoch: 23	train loss: 7954.67626953125	(16.6s)
epoch: 24	train loss: 8038.0908203125	(16.4s)
epoch: 25	train loss: 8016.02294921875	(16.6s)
epoch: 26	train loss: 7832.7587890625	(16.6s)
epoch: 27	train loss: 7862.5625	(16.6s)
epoch: 28	train loss: 7737.2578125	(16.6s)
epoch: 29	train loss: 7977.57958984375	(16.4s)
epoch: 30	train loss: 8008.60546875	(16.5s)
epoch: 31	train loss: 8006.4521484375	(16.5s)
epoch: 32	train loss: 8997.0283203125	(16.6s)
epoch: 33	train loss: 7918.73828125	(16.6s)
epoch: 34	train loss: 7600.923828125	(16.5s)
epoch: 35	train loss: 7393.99951171875	(16.5s)
epoch: 36	train loss: 7411.265625	(16.4s)
epoch: 37	train loss: 8050.65380859375	(16.5s)
epoch: 38	train loss: 7664.7451171875	(16.4s)
epoch: 39	train loss: 7713.87353515625	(16.4s)
epoch: 40	train loss: 7735.50390625	(16.3s)
epoch: 41	train loss: 8483.0380859375	(16.4s)
epoch: 42	train loss: 8110.27978515625	(16.4s)
epoch: 43	train loss: 7556.66650390625	(16.5s)
epoch: 44	train loss: 7429.11474609375	(16.5s)
epoch: 45	train loss: 7230.95849609375	(16.5s)
epoch: 46	train loss: 6907.03466796875	(16.5s)
epoch: 47	train loss: 7297.75146484375	(16.6s)
epoch: 48	train loss: 7595.23388671875	(16.4s)
epoch: 49	train loss: 7408.88916015625	(16.5s)
epoch: 50	train loss: 9440.8076171875	(16.4s)
epoch: 51	train loss: 7470.70458984375	(16.4s)
epoch: 52	train loss: 7908.916015625	(16.6s)
epoch: 53	train loss: 7777.10205078125	(16.5s)
epoch: 54	train loss: 7170.57861328125	(16.5s)
epoch: 55	train loss: 7393.3994140625	(16.5s)
epoch: 56	train loss: 7327.14111328125	(16.5s)
epoch: 57	train loss: 7457.62744140625	(16.6s)
epoch: 58	train loss: 7191.1484375	(16.6s)
epoch: 59	train loss: 7272.11376953125	(16.5s)
epoch: 60	train loss: 7303.939453125	(16.4s)
epoch: 61	train loss: 7095.4072265625	(16.4s)
epoch: 62	train loss: 7282.78173828125	(16.6s)
epoch: 63	train loss: 7406.33642578125	(16.6s)
epoch: 64	train loss: 7471.07763671875	(16.4s)
epoch: 65	train loss: 8415.763671875	(16.5s)
epoch: 66	train loss: 7263.361328125	(16.5s)
epoch: 67	train loss: 6880.0244140625	(16.6s)
epoch: 68	train loss: 7482.26953125	(16.4s)
epoch: 69	train loss: 7217.81103515625	(16.5s)
epoch: 70	train loss: 7276.25732421875	(16.5s)
epoch: 71	train loss: 7047.82275390625	(16.5s)
epoch: 72	train loss: 7021.49462890625	(16.4s)
epoch: 73	train loss: 7159.75244140625	(16.5s)
epoch: 74	train loss: 7338.10205078125	(16.5s)
epoch: 75	train loss: 12801.23828125	(16.5s)
epoch: 76	train loss: 8197.7421875	(16.5s)
epoch: 77	train loss: 7329.2021484375	(16.6s)
epoch: 78	train loss: 6991.85400390625	(16.6s)
epoch: 79	train loss: 7005.84326171875	(16.4s)
epoch: 80	train loss: 7441.79541015625	(16.5s)
epoch: 81	train loss: 7045.66943359375	(16.5s)
epoch: 82	train loss: 7039.88232421875	(16.5s)
epoch: 83	train loss: 7270.22265625	(16.4s)
epoch: 84	train loss: 7721.64306640625	(16.4s)
epoch: 85	train loss: 7066.78173828125	(16.5s)
epoch: 86	train loss: 6932.58203125	(16.5s)
epoch: 87	train loss: 7128.04931640625	(16.5s)
epoch: 88	train loss: 6834.03955078125	(16.6s)
epoch: 89	train loss: 7136.15380859375	(16.5s)
epoch: 90	train loss: 7545.71435546875	(16.5s)
epoch: 91	train loss: 6783.81298828125	(16.4s)
epoch: 92	train loss: 7227.56689453125	(16.5s)
epoch: 93	train loss: 6971.31298828125	(16.6s)
epoch: 94	train loss: 6757.36669921875	(16.5s)
epoch: 95	train loss: 6788.85400390625	(16.5s)
epoch: 96	train loss: 6848.5751953125	(16.4s)
epoch: 97	train loss: 6927.087890625	(16.4s)
epoch: 98	train loss: 7466.53759765625	(16.6s)
epoch: 99	train loss: 7560.798828125	(16.4s)
Evaluating model on 200 episodes
0.0007707846764658045
0.0005918539880605584
0.0007349676365265623
0.0010392611036781762
0.0009857602271949872
0.0009339380016576292
0.0034858829094446264
0.0012706940085211369
0.000640679631032981
0.000861502062535793
0.000927290177969553
0.0017297358075059794
0.0006739337957696989
0.0012854854859532289
0.0013493757736190066
0.0008489252108524346
0.0016044215863075806
0.002303781614803787
0.0014512891364574898
0.0008819918610762605
0.0012433264462742954
0.0016461333261059525
0.0012735319743275315
0.0007017527914285893
0.00122219613415207
0.0008230799503508024
0.0012147167209251267
0.0010452526184963062
0.001076712561189197
0.0018834468919521896
0.00195671682304237
0.0019817235849283557
0.0021676932697624173
0.0014157329060253687
0.0018705011687416117
0.0022363085103279446
0.0005531664937734604
0.0017720604988021983
0.0007024905662547098
0.0007620472861162852
0.001086144955843338
0.0006118184241391386
0.0016707839895389043
0.0012311218699323945
0.0011109830340597239
0.0009818907285359902
0.0012801204441833154
0.002872310604895271
0.0007559395770923319
0.001449089328525588
0.0011270915835796455
0.0011317127203007906
0.0012989761469523525
0.0018563925424435487
0.0007168327671630929
0.0020507436925072398
0.0009611466912247124
0.0016858964880322508
0.0011418470191226031
0.0009511938409963881
0.0027326627415378527
0.0008468995329167228
0.001023409591450925
0.0011642639874480665
0.0012853006264776923
0.0014109411027415522
0.00018930513761006296
0.0022993773701063427
0.002615450560551835
0.0015319506452545863
0.0014018206824807243
0.001081938857169007
0.000890113983457143
0.0012758073783267967
0.001300522055836544
0.0015677441539689123
0.004907685590069377
0.0013718012981779135
0.0009165164833225739
0.0014526090828719763
0.00213677196855618
0.003365227645796646
0.0018127764347949638
0.0008695046876360161
0.0031793808375368826
0.0009746250104096058
0.0009010348577349861
0.0019211900576515796
0.0024674470914760605
0.0011797860696484955
0.0008125195745378733
0.0009257902784156613
0.00198789966859348
0.03833885762159137
0.0013645030972838867
0.0014548506975794833
0.0004596810015047797
0.0015541190410398745
0.0003298524954718434
0.0016471477387572772
0.001435578186146813
0.002080752539768582
0.0011214245459996164
0.0014798412215895952
0.0008154174785077986
0.0020183077330539864
0.0011357928908017295
0.0026982024379928284
0.0012279840142582544
0.0011556586473489006
0.000800642277681618
0.0010674490389646963
0.000881471679652653
0.0009106462050112896
0.0004472651671676431
0.0007673304723236166
0.0008398008777965935
0.0033200134712387808
0.0008664692843012745
0.0017061155958799645
0.0011633591566351242
0.0018769718328258023
0.001018385830927097
0.0008154764260931794
0.0012772293991404765
0.001388567437364448
0.0018754531138256425
0.001249554414243903
0.000729127953281232
0.0010208638486801647
0.0012236492155352607
0.0016509159297129373
0.0007859036722948076
0.001279788399080965
0.0008104880354949274
0.0008103704654226581
0.0017568606599525083
0.0012359522903958957
0.00181248300599263
0.0010459228651598096
0.0017222761634911876
0.0014632418105440378
0.0009359261354499419
0.0010532054277617767
0.0019289247402412002
0.0022698136272083502
0.0014339797029000085
0.0023485057678884072
0.0016515820927452296
0.00270154107693088
0.0025190136168800692
0.002558687416239991
0.0008916743429906021
0.002305180891198688
0.0002963886974612251
0.001310662441890025
0.00035810824495759636
0.002175635113962926
0.0011808263854098705
0.0007620065637941783
0.000826484815358223
0.0009923018342306023
0.0019524902209013817
0.002145850545477717
0.0029507068160455675
0.001027091298965388
0.001633448252202167
0.0014956823402566702
0.0008456244744593277
0.0010674672031360816
0.0013424826039616683
0.0017749398612068035
0.0018104735645465553
0.0007048586988027885
0.001368035095462498
0.000967536005191505
0.0008192614739181451
0.002227293382300279
Solved 178/200 episodes
0.0014313198243459378
Evaluated model in 27.5 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-17
Round 18
Generated trajectories in 71.2 seconds
epoch: 0	train loss: 11583.689453125	(16.2s)
epoch: 1	train loss: 9752.1064453125	(16.1s)
epoch: 2	train loss: 9695.8994140625	(16.3s)
epoch: 3	train loss: 9070.6943359375	(16.4s)
epoch: 4	train loss: 8800.828125	(16.4s)
epoch: 5	train loss: 8565.4697265625	(16.4s)
epoch: 6	train loss: 8903.16796875	(16.4s)
epoch: 7	train loss: 8432.443359375	(16.4s)
epoch: 8	train loss: 8281.7548828125	(16.4s)
epoch: 9	train loss: 8159.31201171875	(16.4s)
epoch: 10	train loss: 8045.802734375	(16.4s)
epoch: 11	train loss: 8461.6572265625	(16.4s)
epoch: 12	train loss: 8542.79296875	(16.4s)
epoch: 13	train loss: 8375.6328125	(16.4s)
epoch: 14	train loss: 8049.42333984375	(16.4s)
epoch: 15	train loss: 7849.173828125	(16.4s)
epoch: 16	train loss: 8108.7216796875	(16.4s)
epoch: 17	train loss: 7837.29052734375	(16.4s)
epoch: 18	train loss: 7832.7890625	(16.4s)
epoch: 19	train loss: 7637.6728515625	(16.5s)
epoch: 20	train loss: 7860.9580078125	(16.4s)
epoch: 21	train loss: 7713.81005859375	(16.4s)
epoch: 22	train loss: 7690.96728515625	(16.4s)
epoch: 23	train loss: 8023.34765625	(16.4s)
epoch: 24	train loss: 7730.1318359375	(16.5s)
epoch: 25	train loss: 8314.349609375	(16.4s)
epoch: 26	train loss: 7492.48876953125	(16.3s)
epoch: 27	train loss: 7635.0283203125	(16.4s)
epoch: 28	train loss: 7810.75537109375	(16.4s)
epoch: 29	train loss: 8205.09375	(16.4s)
epoch: 30	train loss: 8990.861328125	(16.4s)
epoch: 31	train loss: 8905.2607421875	(16.4s)
epoch: 32	train loss: 7869.572265625	(16.4s)
epoch: 33	train loss: 7387.79638671875	(16.4s)
epoch: 34	train loss: 7139.6787109375	(16.4s)
epoch: 35	train loss: 7063.13818359375	(16.4s)
epoch: 36	train loss: 7508.55810546875	(16.4s)
epoch: 37	train loss: 7547.32568359375	(16.4s)
epoch: 38	train loss: 7505.5556640625	(16.4s)
epoch: 39	train loss: 7443.12353515625	(16.4s)
epoch: 40	train loss: 6902.787109375	(16.2s)
epoch: 41	train loss: 7131.41748046875	(16.2s)
epoch: 42	train loss: 7140.171875	(16.2s)
epoch: 43	train loss: 7232.68115234375	(16.2s)
epoch: 44	train loss: 7435.6708984375	(16.2s)
epoch: 45	train loss: 7299.56103515625	(16.2s)
epoch: 46	train loss: 7380.85009765625	(16.2s)
epoch: 47	train loss: 7425.00927734375	(16.2s)
epoch: 48	train loss: 7716.5166015625	(16.2s)
epoch: 49	train loss: 7613.36083984375	(16.2s)
epoch: 50	train loss: 8054.685546875	(16.2s)
epoch: 51	train loss: 7533.00439453125	(16.2s)
epoch: 52	train loss: 7291.36767578125	(16.2s)
epoch: 53	train loss: 7046.77392578125	(16.2s)
epoch: 54	train loss: 7064.47021484375	(16.2s)
epoch: 55	train loss: 6967.1708984375	(16.2s)
epoch: 56	train loss: 7112.6357421875	(16.2s)
epoch: 57	train loss: 7428.1689453125	(16.2s)
epoch: 58	train loss: 8146.95947265625	(16.2s)
epoch: 59	train loss: 7499.24658203125	(16.2s)
epoch: 60	train loss: 7073.68701171875	(16.2s)
epoch: 61	train loss: 8262.9140625	(16.2s)
epoch: 62	train loss: 7309.37841796875	(16.2s)
epoch: 63	train loss: 7273.98193359375	(16.2s)
epoch: 64	train loss: 7003.43896484375	(16.2s)
epoch: 65	train loss: 7142.77197265625	(16.2s)
epoch: 66	train loss: 7139.51611328125	(16.2s)
epoch: 67	train loss: 7187.14697265625	(16.2s)
epoch: 68	train loss: 7401.404296875	(16.2s)
epoch: 69	train loss: 7011.8486328125	(16.2s)
epoch: 70	train loss: 6709.998046875	(16.2s)
epoch: 71	train loss: 6821.42236328125	(16.2s)
epoch: 72	train loss: 6819.58935546875	(16.2s)
epoch: 73	train loss: 7077.8125	(16.2s)
epoch: 74	train loss: 10262.044921875	(16.2s)
epoch: 75	train loss: 7413.00048828125	(16.2s)
epoch: 76	train loss: 7760.48779296875	(16.1s)
epoch: 77	train loss: 6744.26416015625	(16.2s)
epoch: 78	train loss: 6657.0009765625	(16.2s)
epoch: 79	train loss: 6690.474609375	(16.2s)
epoch: 80	train loss: 6656.3984375	(16.2s)
epoch: 81	train loss: 6784.75	(16.1s)
epoch: 82	train loss: 7074.25390625	(16.2s)
epoch: 83	train loss: 7005.50390625	(16.2s)
epoch: 84	train loss: 6866.1318359375	(16.2s)
epoch: 85	train loss: 6701.69677734375	(16.2s)
epoch: 86	train loss: 6593.88671875	(16.2s)
epoch: 87	train loss: 7369.93798828125	(16.2s)
epoch: 88	train loss: 7137.56396484375	(16.2s)
epoch: 89	train loss: 6805.763671875	(16.2s)
epoch: 90	train loss: 7125.3525390625	(16.2s)
epoch: 91	train loss: 6870.439453125	(16.3s)
epoch: 92	train loss: 6562.7255859375	(16.2s)
epoch: 93	train loss: 6503.66943359375	(16.2s)
epoch: 94	train loss: 6745.02587890625	(16.2s)
epoch: 95	train loss: 7142.5517578125	(16.2s)
epoch: 96	train loss: 7005.5732421875	(16.2s)
epoch: 97	train loss: 6869.94482421875	(16.2s)
epoch: 98	train loss: 6884.06787109375	(16.2s)
epoch: 99	train loss: 6988.521484375	(16.2s)
Evaluating model on 200 episodes
0.0009014718769841627
0.000722819940710906
0.0015881175730520715
0.0006391522260855709
0.0005338422638487827
0.0020963163660780992
0.0007827939934941242
0.001651359200574613
0.001365186537441332
0.001567253979481072
0.0006636237034399528
0.0004894797252745775
0.0007387996998399363
0.0021238749934983548
0.0006282703434408177
0.0024569143989698634
0.0005488154598755169
0.0015901417536952067
0.0012504753033092985
0.0006369875802192837
0.002844951874067192
0.0012472949424085931
0.0015960473018432303
0.0015446688121301122
0.002310519339516759
0.0008273339743027463
0.001188903263634226
0.002111676087224623
0.002038349661653718
0.0018199831970074835
0.0019378079329423297
0.0012872737198550669
0.0012158167408094468
0.0006998567896516761
0.0004233963134336389
0.0013928161300581826
0.0011475162464194
0.0018649631321230637
0.00233728093319639
0.0010104191180863907
0.0009673917655683388
0.001237057340663991
0.00034187265854208296
0.0011221123470022576
0.0010014302332075947
0.0010330142145643809
0.0006383131170878187
0.0009740985499645051
0.0016753092148484518
0.0012383291209213591
0.0008922107942150129
0.0031604033574694768
0.0024043480532580158
0.0010422225540798321
0.001550578748911472
0.0009124493085169781
0.000518272000287349
0.0023616688823412087
0.0017673139991529752
0.0009090580606425647
0.002106177614768967
0.0013416864147240465
0.0010959509971952583
0.0011649527959889383
0.0008046794729515179
0.0008113003605103586
0.0015964874320688021
0.0008268324061646126
0.0003873648456647061
0.0014684743235613075
0.0009667195534513502
0.002128456636758832
0.0009286840698526552
0.00044287421405897476
0.0017082881131500471
0.0009341005837389579
0.002295983573680537
0.0006597167860794192
0.0022426298839325846
0.001989879605793653
0.0011202910463907756
0.0010227470126361975
0.0017135498536744206
0.0007520097043358392
0.0014919193878692265
0.0015372041145217902
0.0012675371003751934
0.001393801554793474
0.011114971615218868
0.001587084548373241
0.0007159376372227497
0.0016869772920141588
0.0015409044603334873
0.00305559512344189
0.001005301446379495
0.0007216598146202925
0.0006807184327044524
0.0013898004375126523
0.0014194215030632524
0.0006737630812697009
0.001496985849891124
0.0021219778742912845
0.0017215643523134834
0.00214365569408983
0.0007249036769078682
0.0011238883159551602
0.0029736268301753122
0.0007494666329876054
0.0015531108480277716
0.0004416009014676092
0.0006166486900138597
0.0009813469710374752
0.0008399455109611154
0.001867210182050864
0.0036230925485142506
0.001079257182573201
0.0008717604090634268
0.0007772592536639423
0.002477270472445525
0.001239777315442387
0.001350213102947843
0.0015146184887271374
0.0007496774451283272
0.0016494836389837372
0.0009732053789775818
0.0013295332387582783
0.0011303550841561942
0.0017770146296243183
0.0018035713960257867
0.0011087873062933795
0.0008429985802130042
0.0010307774777174928
0.0009857709464995423
0.0028843088744906708
0.0006216435554051714
0.0004811208834455881
0.001559689328132663
0.0008344006283940482
0.0011019428610084374
0.0012020006787762131
0.0010998802981703193
0.0008964943390310509
0.0018866849473852198
0.0016414683019607573
0.0010305946180939347
0.0009709745387557301
0.0005852127787875361
0.000634169953991659
0.0015408155979292298
0.0007881040205358572
0.0014945836469451024
0.0009191649114654865
0.001039572140598466
0.0016618457146770588
0.0007400547571402664
0.0013532297421729059
0.0009814217143381636
0.0012794763349423496
0.0015597410633095673
0.0019796666277285954
0.0017846068243670744
0.0009503069304628298
0.0007916161528555676
0.0011188875258085317
0.00048405767180762876
0.0007325648864655074
0.0014888239236622918
0.0012145474723253686
0.0019889098475687206
0.0014527525992207302
0.0016587595013593923
0.0011315977800612537
0.0010295981456610289
0.0009074576926650479
0.0010395792399192163
0.001202287894860395
0.0017104130260122475
0.000791282802051033
Solved 178/200 episodes
0.0012063956530122088
Evaluated model in 27.4 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-18
Round 19
Generated trajectories in 70.9 seconds
epoch: 0	train loss: 11614.9853515625	(16.1s)
epoch: 1	train loss: 9839.0458984375	(16.1s)
epoch: 2	train loss: 8942.8388671875	(16.2s)
epoch: 3	train loss: 8658.2578125	(16.2s)
epoch: 4	train loss: 8922.1611328125	(16.2s)
epoch: 5	train loss: 8600.375	(16.2s)
epoch: 6	train loss: 8919.2587890625	(16.2s)
epoch: 7	train loss: 8393.203125	(16.2s)
epoch: 8	train loss: 8427.3291015625	(16.2s)
epoch: 9	train loss: 7934.7177734375	(16.2s)
epoch: 10	train loss: 8643.08984375	(16.2s)
epoch: 11	train loss: 8371.81640625	(16.2s)
epoch: 12	train loss: 8565.3037109375	(16.2s)
epoch: 13	train loss: 8254.73046875	(16.2s)
epoch: 14	train loss: 7912.6826171875	(16.2s)
epoch: 15	train loss: 7982.52294921875	(16.2s)
epoch: 16	train loss: 7738.7099609375	(16.2s)
epoch: 17	train loss: 7973.568359375	(16.2s)
epoch: 18	train loss: 8209.7548828125	(16.3s)
epoch: 19	train loss: 7859.7373046875	(16.2s)
epoch: 20	train loss: 8290.529296875	(16.2s)
epoch: 21	train loss: 7833.61669921875	(16.2s)
epoch: 22	train loss: 7633.0966796875	(16.2s)
epoch: 23	train loss: 7989.009765625	(16.3s)
epoch: 24	train loss: 7745.2626953125	(16.2s)
epoch: 25	train loss: 7864.50732421875	(16.2s)
epoch: 26	train loss: 7661.787109375	(16.2s)
epoch: 27	train loss: 7516.62890625	(16.2s)
epoch: 28	train loss: 7387.31982421875	(16.3s)
epoch: 29	train loss: 7246.5126953125	(16.2s)
epoch: 30	train loss: 7598.40869140625	(16.2s)
epoch: 31	train loss: 7298.49072265625	(16.2s)
epoch: 32	train loss: 7677.921875	(16.2s)
epoch: 33	train loss: 7251.8818359375	(16.3s)
epoch: 34	train loss: 7480.60009765625	(16.2s)
epoch: 35	train loss: 7587.43603515625	(16.2s)
epoch: 36	train loss: 9280.0400390625	(16.2s)
epoch: 37	train loss: 8223.140625	(16.2s)
epoch: 38	train loss: 7431.70849609375	(16.2s)
epoch: 39	train loss: 7236.2607421875	(16.2s)
epoch: 40	train loss: 6980.9345703125	(16.2s)
epoch: 41	train loss: 7605.48046875	(16.2s)
epoch: 42	train loss: 7730.3984375	(16.2s)
epoch: 43	train loss: 7331.09423828125	(16.2s)
epoch: 44	train loss: 7011.14794921875	(16.2s)
epoch: 45	train loss: 7219.6142578125	(16.2s)
epoch: 46	train loss: 7236.0263671875	(16.2s)
epoch: 47	train loss: 7216.6533203125	(16.2s)
epoch: 48	train loss: 7077.14794921875	(16.2s)
epoch: 49	train loss: 7527.884765625	(16.3s)
epoch: 50	train loss: 7672.2001953125	(16.2s)
epoch: 51	train loss: 7116.6015625	(16.2s)
epoch: 52	train loss: 7185.083984375	(16.2s)
epoch: 53	train loss: 7635.95947265625	(16.2s)
epoch: 54	train loss: 7543.59130859375	(16.3s)
epoch: 55	train loss: 7538.7060546875	(16.2s)
epoch: 56	train loss: 7214.650390625	(16.2s)
epoch: 57	train loss: 7423.75830078125	(16.2s)
epoch: 58	train loss: 8704.470703125	(16.2s)
epoch: 59	train loss: 7292.38232421875	(16.3s)
epoch: 60	train loss: 7083.60546875	(16.2s)
epoch: 61	train loss: 6872.61865234375	(16.2s)
epoch: 62	train loss: 7005.7216796875	(16.2s)
epoch: 63	train loss: 7416.1259765625	(16.2s)
epoch: 64	train loss: 7082.6748046875	(16.2s)
epoch: 65	train loss: 7137.82470703125	(16.2s)
epoch: 66	train loss: 7060.33935546875	(16.2s)
epoch: 67	train loss: 7061.6181640625	(16.2s)
epoch: 68	train loss: 7838.57568359375	(16.2s)
epoch: 69	train loss: 7173.490234375	(16.2s)
epoch: 70	train loss: 7242.67578125	(16.2s)
epoch: 71	train loss: 7079.24951171875	(16.2s)
epoch: 72	train loss: 6839.00341796875	(16.2s)
epoch: 73	train loss: 6859.76025390625	(16.2s)
epoch: 74	train loss: 7470.80810546875	(16.2s)
epoch: 75	train loss: 7063.45556640625	(16.3s)
epoch: 76	train loss: 7911.8486328125	(16.2s)
epoch: 77	train loss: 7138.5361328125	(16.2s)
epoch: 78	train loss: 6782.0517578125	(16.2s)
epoch: 79	train loss: 6608.87548828125	(16.2s)
epoch: 80	train loss: 6944.078125	(16.3s)
epoch: 81	train loss: 6818.73583984375	(16.2s)
epoch: 82	train loss: 7449.39013671875	(16.2s)
epoch: 83	train loss: 6863.11376953125	(16.2s)
epoch: 84	train loss: 6870.583984375	(16.2s)
epoch: 85	train loss: 7475.68212890625	(16.3s)
epoch: 86	train loss: 9154.29296875	(16.2s)
epoch: 87	train loss: 7942.61669921875	(16.2s)
epoch: 88	train loss: 7295.7138671875	(16.2s)
epoch: 89	train loss: 6854.4833984375	(16.2s)
epoch: 90	train loss: 7381.22607421875	(16.3s)
epoch: 91	train loss: 6545.130859375	(16.3s)
epoch: 92	train loss: 6421.4013671875	(16.2s)
epoch: 93	train loss: 6913.17919921875	(16.2s)
epoch: 94	train loss: 6657.08203125	(16.2s)
epoch: 95	train loss: 7176.81640625	(16.2s)
epoch: 96	train loss: 6717.541015625	(16.2s)
epoch: 97	train loss: 6418.3623046875	(16.2s)
epoch: 98	train loss: 6633.61572265625	(16.2s)
epoch: 99	train loss: 7514.84033203125	(16.2s)
Evaluating model on 200 episodes
0.0011153722436921228
0.0012496312545711892
0.0020017413726236555
0.0016118890198413283
0.0010306439544365276
0.0014555729632775183
0.0019350783677509753
0.00107425346141099
0.0015539289695678032
0.0008189651591237635
0.0016448463377725733
0.002230341000454029
0.0008610403124643759
0.0028699359941128932
0.0013405646950559458
0.0013719115204845654
0.0012611220194230555
0.0014362030047373588
0.002019738288464186
0.0021879481022811626
0.0006054359045164245
0.0005919501767493784
0.0014415029436349869
0.0010094267901357462
0.0011608240663917968
0.002554920770080571
0.0007193135489311923
0.0022605575704801595
0.0017229693767149001
0.0033997890229026475
0.0016987835047378515
0.00118714397194708
0.002524006120640681
0.0011593291960057936
0.00140008197474974
0.001655794999599245
0.0006060500327293994
0.000709309269344279
0.0018824504799600358
0.0019240096483442287
0.001295323529616739
0.0012997853392724729
0.0010463171303045885
0.002356519161548931
0.0011887886006055244
0.0010020448983899162
0.0006740816402270866
0.0011978885918057024
0.0007549509760186386
0.0016764102842631366
0.0014554017515264503
0.0007749321366039415
0.0019056480044897539
0.0012815466828518158
0.0027446868099265404
0.0010490117829855686
0.0012862998282798799
0.0014180857078827103
0.0026937392895989534
0.002057813432267202
0.0015364126302301884
0.002129945238670593
0.0015401699090597546
0.0016624826384941116
0.0012752785539935129
0.0010992828125987824
0.0041360024037286084
0.0013053027747042102
0.0016718504800585586
0.0011480393877718598
0.0021224163214025216
0.0013486190820393468
0.002385667148960887
0.0022872179563689444
0.001154585748736281
0.0023706709767793654
0.0007429902178465944
0.0023878088109086093
0.0015515493786162032
0.0018364466362375727
0.0016644071612972767
0.0013961940850484617
0.004041405042153201
0.001329425945004914
0.0024290415874150184
0.0022216285666218027
0.0022990228614313915
0.0016142091660003644
0.000772602243766111
0.0014420001631044537
0.0015305114851798863
0.0018116032042598817
0.0016363128940741131
0.001798594758535425
0.0014686053657308495
0.0010827956187616412
0.001007333647766455
0.0014267751089805229
0.001427131023228867
0.0014630116934313264
0.0010172981968095812
0.0017228936432123496
0.0031359694860709298
0.0012360489527054597
0.0016729000219908943
0.002432216348097427
0.0033878520548569213
0.000838165907265173
0.001219247452354466
0.0009901935564729765
0.0009567400437744153
0.0005198082308197627
0.0011322932930650292
0.0014630154819315067
0.0010564476194433934
0.0009965966849752779
0.0008834878885514067
0.0010681064328916061
0.000864398945850553
0.0013442897766253736
0.0007190126515423697
0.0012867763633901057
0.0009621232529752888
0.0011944302961334567
0.0013689724772334825
0.0005190833787006947
0.0015249878170935619
0.001516367427778759
0.0011923716468800194
0.0011679206715295247
0.0014172098144626943
0.0017870897099783178
0.0008011936770344619
0.001698838510492351
0.0017984564891032112
0.00029895514035160886
0.0010127135537914
0.0024163367485347076
0.0009182954662329783
0.0006049624178558588
0.0012015625655364532
0.0013482707949555737
0.0005237408957327716
0.002426522043606383
0.0011982454749522732
0.0030580696566175902
0.0012984699509722892
0.0016591998099521822
0.0017825590512074996
0.0007072842921703481
0.0020183516558063275
0.0014726710778631968
0.0020565289836667945
0.0018247202222720143
0.0008850701051414944
0.0010616068660359208
0.0010692417118763034
0.0009459626104217023
0.0006717341927772698
0.0014292353655037005
0.0008409453233374128
0.002487133451888274
0.0018976038013837145
0.0022549100358446594
0.0010467033528887744
0.004309707810170949
0.0021532098883956983
0.000870118559760158
0.0026056756693063877
Solved 169/200 episodes
0.0012962406323325476
Evaluated model in 26.8 seconds
Saved model for run
6ea62525595f4785a999608e14072f9a with name round-19
Completed training in 34934.6 seconds
Solved 33/200 episodes
Solved 35/200 episodes
Solved 37/200 episodes
Solved 39/200 episodes
Solved 38/200 episodes
Solved 47/200 episodes
Solved 55/200 episodes
Solved 85/200 episodes
Solved 87/200 episodes
Solved 100/200 episodes
Solved 136/200 episodes
Solved 130/200 episodes
Solved 152/200 episodes
Solved 159/200 episodes
Solved 170/200 episodes
Solved 172/200 episodes
Solved 173/200 episodes
Solved 178/200 episodes
Solved 178/200 episodes
Solved 169/200 episodes
