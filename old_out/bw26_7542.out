Using torch device NVIDIA GeForce RTX 2080 Ti
Starting run:
0a6c8cb218854c8ca1e998debb551c8c
params=Namespace(note='', n=20000, b=10, abstract_pen=1.0, model='cc', seed=1, lr=0.0001, abstract_dim=32, tau_noise_std=0.0, freeze=False, load=False, ellis=False, no_log=False, fine_tune=False, tau_precompute=False, replace_trans_net=False, batch_norm=False, no_tau_norm=False, relational_micro=False, toy_test=False, separate_option_nets=False, gumbel=False, g_start_temp=1, g_stop_temp=1, num_categories=8, shrink_micro_net=False, shrink_loss_scale=1, solution_length=(1, 2, 3, 4), muzero=False, muzero_scratch=False, num_test=200, test_every=60, save_every=180, neurosym=True, cc_neurosym=False, sv_options=False, dim=64, num_attn_blocks=2, num_heads=4, cc_weight=1.0, fake_cc_neurosym=False, symbolic_sv=False, micro_net2=False, num_out=None, check_ix=-1, num_check=0, sv_micro=False, sv_micro_data_type='full_traj', relational_macro=False, move_loss=True, state_loss=True, cc_loss=False, batch_size=128, traj_updates=100000000.0, model_load_path='models/7caf148820a04ce3bbd8bbfb43a8cd9c.pt', gumbel_sched=False, device='NVIDIA GeForce RTX 2080 Ti', id='0a6c8cb218854c8ca1e998debb551c8c')
wandb: Currently logged in as: simonalford42. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/sca63/abstraction/wandb/run-20221109_150612-1m6f7a2y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-violet-650
wandb: ⭐️ View project at https://wandb.ai/simonalford42/abstraction
wandb: 🚀 View run at https://wandb.ai/simonalford42/abstraction/runs/1m6f7a2y
no dataset found at data/abstract-20000, generating dataset from scratch
saved dataset at data/abstract-20000
Net has 57832 parameters
Completed training in 22639.4 seconds
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: \ 0.011 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: | 0.016 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: / 0.016 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: - 0.016 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: \ 0.016 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: | 0.016 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: / 0.016 MB of 0.016 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:        cc_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▅▇██▇█▅▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁
wandb:   eval_cc_loss ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▄▇█▆█▃▃▆▄▅▃▂▃▁▂▂▁▁▁▂▁▁
wandb:  eval_move_acc ▁▆▇▇█▇██████████████████████████████████
wandb: eval_state_acc █▇▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▄▄▃▃▃▃▂▂▂▃▂▂▂▁▂▂▁▁▁▁▁▁▁
wandb:           loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       move_acc ▁▇▇█████████████████████████████████████
wandb:      move_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      state_acc ▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇█▇▇███████
wandb:     state_loss ██▇▇▆▆▆▆▅▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:        cc_loss 42295272915123.46
wandb:   eval_cc_loss 275573249283.5078
wandb:  eval_move_acc 1.0
wandb: eval_state_acc 0.00729
wandb:           loss 5.74369
wandb:       move_acc 0.99922
wandb:      move_loss 1.14797
wandb:      state_acc 0.98906
wandb:     state_loss 4.59571
wandb: 
wandb: Synced celestial-violet-650: https://wandb.ai/simonalford42/abstraction/runs/1m6f7a2y
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221109_150612-1m6f7a2y/logs
