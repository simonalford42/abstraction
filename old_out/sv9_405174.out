Using torch device NVIDIA GeForce RTX 3090
traj-level training without hmm
Net has 37803 parameters
Round 0
Generated trajectories in 75.6 seconds
/home/sca63/.conda/envs/gcsl/lib/python3.9/site-packages/torch/nn/modules/conv.py:443: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:680.)
  return F.conv2d(input, weight, bias, self.stride,
epoch: 0	train loss: 3659.4423828125	(17s)
epoch: 1	train loss: 3659.236328125	(14s)
epoch: 2	train loss: 3657.978271484375	(14s)
epoch: 3	train loss: 3629.8154296875	(14s)
epoch: 4	train loss: 3279.288330078125	(14s)
epoch: 5	train loss: 3119.519775390625	(14s)
epoch: 6	train loss: 3022.44287109375	(14s)
epoch: 7	train loss: 2971.253173828125	(14s)
epoch: 8	train loss: 2908.8427734375	(14s)
epoch: 9	train loss: 2795.824462890625	(14s)
epoch: 10	train loss: 2701.80615234375	(14s)
epoch: 11	train loss: 2649.389404296875	(14s)
epoch: 12	train loss: 2613.595947265625	(14s)
epoch: 13	train loss: 2583.702392578125	(14s)
epoch: 14	train loss: 2558.25146484375	(14s)
epoch: 15	train loss: 2536.525390625	(14s)
epoch: 16	train loss: 2516.200927734375	(14s)
epoch: 17	train loss: 2493.56787109375	(14s)
epoch: 18	train loss: 2473.326416015625	(14s)
epoch: 19	train loss: 2453.07763671875	(14s)
epoch: 20	train loss: 2434.5283203125	(14s)
epoch: 21	train loss: 2417.25439453125	(14s)
epoch: 22	train loss: 2400.837890625	(14s)
epoch: 23	train loss: 2385.409423828125	(14s)
epoch: 24	train loss: 2370.92333984375	(14s)
epoch: 25	train loss: 2360.001708984375	(14s)
epoch: 26	train loss: 2348.77099609375	(14s)
epoch: 27	train loss: 2338.249755859375	(14s)
epoch: 28	train loss: 2328.816162109375	(14s)
epoch: 29	train loss: 2318.477783203125	(14s)
epoch: 30	train loss: 2309.751708984375	(14s)
epoch: 31	train loss: 2297.791259765625	(14s)
epoch: 32	train loss: 2289.74609375	(14s)
epoch: 33	train loss: 2281.538818359375	(14s)
epoch: 34	train loss: 2273.343017578125	(14s)
epoch: 35	train loss: 2266.740966796875	(14s)
epoch: 36	train loss: 2258.894287109375	(14s)
epoch: 37	train loss: 2252.77294921875	(14s)
epoch: 38	train loss: 2246.385986328125	(14s)
epoch: 39	train loss: 2240.637451171875	(14s)
epoch: 40	train loss: 2235.3916015625	(14s)
epoch: 41	train loss: 2229.663330078125	(14s)
epoch: 42	train loss: 2224.076171875	(14s)
epoch: 43	train loss: 2219.41845703125	(14s)
epoch: 44	train loss: 2212.77978515625	(14s)
epoch: 45	train loss: 2207.426513671875	(14s)
epoch: 46	train loss: 2201.072998046875	(14s)
epoch: 47	train loss: 2192.76708984375	(14s)
epoch: 48	train loss: 2188.335693359375	(14s)
epoch: 49	train loss: 2181.640625	(14s)
epoch: 50	train loss: 2176.902587890625	(14s)
epoch: 51	train loss: 2166.54345703125	(14s)
epoch: 52	train loss: 2160.400146484375	(14s)
epoch: 53	train loss: 2155.0517578125	(14s)
epoch: 54	train loss: 2148.519775390625	(14s)
epoch: 55	train loss: 2142.58251953125	(14s)
epoch: 56	train loss: 2132.664794921875	(14s)
epoch: 57	train loss: 2125.819091796875	(14s)
epoch: 58	train loss: 2119.206787109375	(14s)
epoch: 59	train loss: 2108.6669921875	(14s)
epoch: 60	train loss: 2103.337890625	(14s)
epoch: 61	train loss: 2096.127197265625	(14s)
epoch: 62	train loss: 2089.7939453125	(14s)
epoch: 63	train loss: 2086.841552734375	(14s)
epoch: 64	train loss: 2079.158935546875	(14s)
epoch: 65	train loss: 2071.187744140625	(14s)
epoch: 66	train loss: 2066.2568359375	(14s)
epoch: 67	train loss: 2062.6015625	(14s)
epoch: 68	train loss: 2055.818359375	(14s)
epoch: 69	train loss: 2051.078369140625	(14s)
epoch: 70	train loss: 2047.376220703125	(14s)
epoch: 71	train loss: 2041.8336181640625	(14s)
epoch: 72	train loss: 2037.962646484375	(14s)
epoch: 73	train loss: 2032.7886962890625	(14s)
epoch: 74	train loss: 2029.4384765625	(14s)
epoch: 75	train loss: 2028.6915283203125	(14s)
epoch: 76	train loss: 2023.7498779296875	(14s)
epoch: 77	train loss: 2019.8134765625	(14s)
epoch: 78	train loss: 2016.667236328125	(14s)
epoch: 79	train loss: 2013.80322265625	(14s)
epoch: 80	train loss: 2009.480712890625	(14s)
epoch: 81	train loss: 2005.3775634765625	(14s)
epoch: 82	train loss: 2004.2225341796875	(14s)
epoch: 83	train loss: 2000.8804931640625	(14s)
epoch: 84	train loss: 1999.1590576171875	(14s)
epoch: 85	train loss: 1995.17529296875	(14s)
epoch: 86	train loss: 1992.0555419921875	(14s)
epoch: 87	train loss: 1987.2340087890625	(14s)
epoch: 88	train loss: 1983.164794921875	(14s)
epoch: 89	train loss: 1980.8896484375	(14s)
epoch: 90	train loss: 1976.8045654296875	(14s)
epoch: 91	train loss: 1972.1641845703125	(14s)
epoch: 92	train loss: 1969.7891845703125	(14s)
epoch: 93	train loss: 1966.5703125	(14s)
epoch: 94	train loss: 1965.9354248046875	(14s)
epoch: 95	train loss: 1962.065185546875	(14s)
epoch: 96	train loss: 1958.9249267578125	(14s)
epoch: 97	train loss: 1957.16455078125	(14s)
epoch: 98	train loss: 1955.35400390625	(14s)
epoch: 99	train loss: 1953.0089111328125	(14s)
epoch: 100	train loss: 1948.7099609375	(14s)
epoch: 101	train loss: 1947.2421875	(14s)
epoch: 102	train loss: 1946.063720703125	(14s)
epoch: 103	train loss: 1943.8486328125	(14s)
epoch: 104	train loss: 1941.8199462890625	(14s)
epoch: 105	train loss: 1940.02490234375	(14s)
epoch: 106	train loss: 1938.1732177734375	(14s)
epoch: 107	train loss: 1936.1968994140625	(14s)
epoch: 108	train loss: 1933.286376953125	(14s)
epoch: 109	train loss: 1931.1163330078125	(14s)
epoch: 110	train loss: 1929.979248046875	(14s)
epoch: 111	train loss: 1927.801513671875	(14s)
epoch: 112	train loss: 1925.6702880859375	(14s)
epoch: 113	train loss: 1926.3050537109375	(14s)
epoch: 114	train loss: 1922.0142822265625	(14s)
epoch: 115	train loss: 1920.2122802734375	(14s)
epoch: 116	train loss: 1917.2279052734375	(14s)
epoch: 117	train loss: 1916.6357421875	(14s)
epoch: 118	train loss: 1913.77587890625	(14s)
epoch: 119	train loss: 1911.8250732421875	(14s)
epoch: 120	train loss: 1909.1180419921875	(14s)
epoch: 121	train loss: 1907.8331298828125	(14s)
epoch: 122	train loss: 1907.0728759765625	(14s)
epoch: 123	train loss: 1908.4520263671875	(14s)
epoch: 124	train loss: 1904.802001953125	(14s)
epoch: 125	train loss: 1902.4359130859375	(14s)
epoch: 126	train loss: 1903.0045166015625	(14s)
epoch: 127	train loss: 1899.170166015625	(14s)
epoch: 128	train loss: 1898.905029296875	(14s)
epoch: 129	train loss: 1897.08740234375	(14s)
epoch: 130	train loss: 1897.5091552734375	(14s)
epoch: 131	train loss: 1895.6710205078125	(14s)
epoch: 132	train loss: 1894.382080078125	(14s)
epoch: 133	train loss: 1894.28515625	(14s)
epoch: 134	train loss: 1891.904052734375	(14s)
epoch: 135	train loss: 1889.918701171875	(14s)
epoch: 136	train loss: 1891.0103759765625	(14s)
epoch: 137	train loss: 1889.5604248046875	(14s)
epoch: 138	train loss: 1887.869384765625	(14s)
epoch: 139	train loss: 1889.055908203125	(14s)
epoch: 140	train loss: 1887.55419921875	(14s)
epoch: 141	train loss: 1888.3033447265625	(14s)
epoch: 142	train loss: 1888.3895263671875	(14s)
epoch: 143	train loss: 1884.4251708984375	(14s)
epoch: 144	train loss: 1884.6956787109375	(14s)
epoch: 145	train loss: 1884.046875	(14s)
epoch: 146	train loss: 1883.5797119140625	(14s)
epoch: 147	train loss: 1883.3062744140625	(14s)
epoch: 148	train loss: 1882.8353271484375	(14s)
epoch: 149	train loss: 1881.8802490234375	(14s)
epoch: 150	train loss: 1880.4388427734375	(14s)
epoch: 151	train loss: 1880.0909423828125	(14s)
epoch: 152	train loss: 1877.2369384765625	(14s)
epoch: 153	train loss: 1879.6248779296875	(14s)
epoch: 154	train loss: 1878.4010009765625	(14s)
epoch: 155	train loss: 1874.7230224609375	(14s)
epoch: 156	train loss: 1875.4945068359375	(14s)
epoch: 157	train loss: 1873.9874267578125	(14s)
epoch: 158	train loss: 1873.64453125	(14s)
epoch: 159	train loss: 1874.12841796875	(14s)
epoch: 160	train loss: 1872.5692138671875	(14s)
epoch: 161	train loss: 1870.3408203125	(14s)
epoch: 162	train loss: 1868.7003173828125	(14s)
epoch: 163	train loss: 1869.035888671875	(14s)
epoch: 164	train loss: 1866.325927734375	(14s)
epoch: 165	train loss: 1864.96044921875	(14s)
epoch: 166	train loss: 1862.27734375	(14s)
epoch: 167	train loss: 1860.1063232421875	(14s)
slurmstepd: error: *** JOB 405174 ON ellis-compute-01 CANCELLED AT 2022-01-26T15:43:06 ***
