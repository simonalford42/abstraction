Using torch device NVIDIA GeForce RTX 3090
net: causal
cc_weight: 0.0
params: {'epochs': 100, 'lr': 0.0008, 'n': 5000}
Net has 207442 parameters
Round 0
Generated trajectories in 72.2 seconds
epoch: 0	train loss: 141169.28125	(17.1s)
epoch: 1	train loss: 111539.2265625	(16.2s)
epoch: 2	train loss: 100842.6171875	(16.1s)
epoch: 3	train loss: 88036.109375	(16.1s)
epoch: 4	train loss: 77704.3515625	(16.2s)
epoch: 5	train loss: 70995.328125	(16.1s)
epoch: 6	train loss: 65481.234375	(16.1s)
epoch: 7	train loss: 57694.40234375	(16.1s)
epoch: 8	train loss: 53083.9375	(16.2s)
epoch: 9	train loss: 50858.7890625	(16.2s)
epoch: 10	train loss: 48959.91796875	(16.1s)
epoch: 11	train loss: 47608.578125	(16.1s)
epoch: 12	train loss: 46204.73828125	(16.1s)
epoch: 13	train loss: 44514.87890625	(16.1s)
epoch: 14	train loss: 43652.98828125	(16.2s)
epoch: 15	train loss: 42816.78515625	(16.1s)
epoch: 16	train loss: 41828.51171875	(16.0s)
epoch: 17	train loss: 41284.5546875	(16.0s)
epoch: 18	train loss: 40683.3046875	(16.1s)
epoch: 19	train loss: 40498.75390625	(16.0s)
epoch: 20	train loss: 39795.578125	(16.1s)
epoch: 21	train loss: 39994.3359375	(16.1s)
epoch: 22	train loss: 39829.16015625	(16.2s)
epoch: 23	train loss: 39552.53515625	(16.1s)
epoch: 24	train loss: 38857.5546875	(16.2s)
epoch: 25	train loss: 38458.33203125	(16.1s)
epoch: 26	train loss: 37795.734375	(16.1s)
epoch: 27	train loss: 37418.08984375	(16.2s)
epoch: 28	train loss: 37236.70703125	(16.2s)
epoch: 29	train loss: 37186.8125	(16.2s)
epoch: 30	train loss: 36564.65234375	(16.2s)
epoch: 31	train loss: 35647.73046875	(16.1s)
epoch: 32	train loss: 35020.30078125	(16.2s)
epoch: 33	train loss: 34633.96875	(16.2s)
epoch: 34	train loss: 34339.77734375	(16.1s)
epoch: 35	train loss: 34086.58984375	(16.1s)
epoch: 36	train loss: 33716.3515625	(16.1s)
epoch: 37	train loss: 33599.734375	(16.1s)
epoch: 38	train loss: 33084.078125	(16.2s)
epoch: 39	train loss: 33334.1328125	(16.1s)
epoch: 40	train loss: 33178.203125	(16.1s)
epoch: 41	train loss: 33053.9921875	(16.3s)
epoch: 42	train loss: 32513.75	(16.3s)
epoch: 43	train loss: 32284.166015625	(16.3s)
epoch: 44	train loss: 32172.525390625	(16.3s)
epoch: 45	train loss: 31868.634765625	(16.4s)
epoch: 46	train loss: 31891.34765625	(16.4s)
epoch: 47	train loss: 31685.65234375	(16.3s)
epoch: 48	train loss: 32050.2421875	(16.3s)
epoch: 49	train loss: 32190.296875	(16.4s)
epoch: 50	train loss: 30984.70703125	(16.3s)
epoch: 51	train loss: 30843.091796875	(16.3s)
epoch: 52	train loss: 30554.56640625	(16.3s)
epoch: 53	train loss: 31117.5078125	(16.3s)
epoch: 54	train loss: 30335.123046875	(16.4s)
epoch: 55	train loss: 30243.298828125	(16.4s)
epoch: 56	train loss: 30294.75390625	(16.4s)
epoch: 57	train loss: 30070.78515625	(16.3s)
epoch: 58	train loss: 30482.646484375	(16.3s)
epoch: 59	train loss: 29957.47265625	(16.4s)
epoch: 60	train loss: 31140.94140625	(16.4s)
epoch: 61	train loss: 30468.353515625	(16.3s)
epoch: 62	train loss: 30030.890625	(16.4s)
epoch: 63	train loss: 29673.126953125	(16.3s)
epoch: 64	train loss: 29414.017578125	(16.4s)
epoch: 65	train loss: 29227.177734375	(16.3s)
epoch: 66	train loss: 29430.369140625	(16.4s)
epoch: 67	train loss: 29651.826171875	(16.3s)
epoch: 68	train loss: 29539.76171875	(16.3s)
epoch: 69	train loss: 29133.0	(16.5s)
epoch: 70	train loss: 28732.318359375	(16.3s)
epoch: 71	train loss: 28595.865234375	(16.3s)
epoch: 72	train loss: 28273.37109375	(16.4s)
epoch: 73	train loss: 28050.4296875	(16.4s)
epoch: 74	train loss: 27953.564453125	(16.4s)
epoch: 75	train loss: 27646.041015625	(16.3s)
epoch: 76	train loss: 27865.392578125	(16.3s)
epoch: 77	train loss: 27392.65625	(16.4s)
epoch: 78	train loss: 27741.638671875	(16.3s)
epoch: 79	train loss: 27463.66796875	(16.3s)
epoch: 80	train loss: 27162.8984375	(16.5s)
epoch: 81	train loss: 27481.607421875	(16.3s)
epoch: 82	train loss: 27591.0859375	(16.3s)
epoch: 83	train loss: 27335.37890625	(16.4s)
epoch: 84	train loss: 27305.07421875	(16.4s)
epoch: 85	train loss: 27060.8046875	(16.3s)
epoch: 86	train loss: 26883.783203125	(16.4s)
epoch: 87	train loss: 27244.54296875	(16.3s)
epoch: 88	train loss: 27240.822265625	(16.5s)
epoch: 89	train loss: 27314.4765625	(16.5s)
epoch: 90	train loss: 27583.65625	(16.3s)
epoch: 91	train loss: 27460.173828125	(16.4s)
epoch: 92	train loss: 27311.927734375	(16.4s)
epoch: 93	train loss: 27106.05078125	(16.5s)
epoch: 94	train loss: 27180.22265625	(16.5s)
epoch: 95	train loss: 27533.4453125	(16.4s)
epoch: 96	train loss: 26883.580078125	(16.4s)
epoch: 97	train loss: 26475.185546875	(16.3s)
epoch: 98	train loss: 26434.8828125	(16.4s)
epoch: 99	train loss: 26214.578125	(16.4s)
Evaluating model on 200 episodes
3853.152939414978
789.9267075856527
1533.7282059987385
1390.1194095611572
1311.7885310833271
1676.3815431594849
3083.9862434387205
1202.5692119598389
1125.1281920539009
2217.5350074768066
1822.8103892008464
1652.6528015136719
1395.5179584026337
1512.9045567512512
2294.632814847506
1434.6352971394856
1265.6667368570963
938.8094879559109
3134.14142036438
1686.1734929765973
1685.673434495926
1165.699466411884
2498.1240242852105
623.5972202845982
938.3486034393311
597.1624442652652
1639.33140707016
1572.5520558076746
1085.7827341715495
1757.8535768985748
2347.6617752075194
945.6514485677084
1312.118820041418
1087.8640392476862
1683.2105929056804
2663.339606285095
973.7631316781044
Solved 37/200 episodes
299.4999766440268
Evaluated model in 21.5 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-0
Round 1
Generated trajectories in 74.0 seconds
epoch: 0	train loss: 35458.96875	(16.6s)
epoch: 1	train loss: 32856.5625	(16.3s)
epoch: 2	train loss: 31679.57421875	(16.4s)
epoch: 3	train loss: 31141.130859375	(16.3s)
epoch: 4	train loss: 30489.28515625	(16.5s)
epoch: 5	train loss: 30220.6640625	(16.4s)
epoch: 6	train loss: 30120.728515625	(16.4s)
epoch: 7	train loss: 29923.759765625	(16.2s)
epoch: 8	train loss: 29046.666015625	(16.2s)
epoch: 9	train loss: 28837.953125	(16.1s)
epoch: 10	train loss: 28214.396484375	(16.1s)
epoch: 11	train loss: 27897.134765625	(16.0s)
epoch: 12	train loss: 28010.755859375	(16.0s)
epoch: 13	train loss: 27452.15234375	(16.1s)
epoch: 14	train loss: 27163.072265625	(16.1s)
epoch: 15	train loss: 27153.95703125	(16.3s)
epoch: 16	train loss: 28264.5546875	(16.2s)
epoch: 17	train loss: 26262.693359375	(16.2s)
epoch: 18	train loss: 25960.775390625	(16.2s)
epoch: 19	train loss: 26499.017578125	(16.2s)
epoch: 20	train loss: 25729.5625	(16.3s)
epoch: 21	train loss: 25551.490234375	(16.2s)
epoch: 22	train loss: 25743.279296875	(16.1s)
epoch: 23	train loss: 25295.51953125	(16.2s)
epoch: 24	train loss: 25492.66796875	(16.2s)
epoch: 25	train loss: 24960.07421875	(16.2s)
epoch: 26	train loss: 24583.01171875	(16.2s)
epoch: 27	train loss: 24393.6640625	(16.1s)
epoch: 28	train loss: 24381.494140625	(16.1s)
epoch: 29	train loss: 24416.931640625	(16.2s)
epoch: 30	train loss: 24007.1953125	(16.1s)
epoch: 31	train loss: 24089.140625	(16.2s)
epoch: 32	train loss: 23714.1640625	(16.1s)
epoch: 33	train loss: 24079.7265625	(16.2s)
epoch: 34	train loss: 24137.150390625	(16.2s)
epoch: 35	train loss: 22961.78515625	(16.1s)
epoch: 36	train loss: 22550.3203125	(16.2s)
epoch: 37	train loss: 23000.14453125	(16.1s)
epoch: 38	train loss: 22247.5703125	(16.1s)
epoch: 39	train loss: 22095.462890625	(16.2s)
epoch: 40	train loss: 22123.16015625	(16.1s)
epoch: 41	train loss: 21782.62890625	(16.2s)
epoch: 42	train loss: 21716.8828125	(16.2s)
epoch: 43	train loss: 22001.34375	(16.1s)
epoch: 44	train loss: 21690.060546875	(16.2s)
epoch: 45	train loss: 21452.1328125	(16.1s)
epoch: 46	train loss: 22060.8828125	(16.2s)
epoch: 47	train loss: 21772.66796875	(16.1s)
epoch: 48	train loss: 21807.83203125	(16.1s)
epoch: 49	train loss: 21960.375	(16.2s)
epoch: 50	train loss: 21529.2578125	(16.2s)
epoch: 51	train loss: 21233.294921875	(16.2s)
epoch: 52	train loss: 21488.11328125	(16.2s)
epoch: 53	train loss: 21123.828125	(16.2s)
epoch: 54	train loss: 21270.65234375	(16.2s)
epoch: 55	train loss: 20973.01171875	(16.2s)
epoch: 56	train loss: 21435.830078125	(16.2s)
epoch: 57	train loss: 21095.349609375	(16.2s)
epoch: 58	train loss: 21131.40625	(16.2s)
epoch: 59	train loss: 20942.51953125	(16.2s)
epoch: 60	train loss: 20946.2578125	(16.1s)
epoch: 61	train loss: 20919.412109375	(16.1s)
epoch: 62	train loss: 20965.77734375	(16.1s)
epoch: 63	train loss: 20593.28125	(16.0s)
epoch: 64	train loss: 20689.158203125	(16.1s)
epoch: 65	train loss: 20691.93359375	(16.1s)
epoch: 66	train loss: 19988.96484375	(16.0s)
epoch: 67	train loss: 20199.724609375	(16.1s)
epoch: 68	train loss: 20052.802734375	(16.0s)
epoch: 69	train loss: 19981.361328125	(16.0s)
epoch: 70	train loss: 20333.119140625	(16.1s)
epoch: 71	train loss: 19933.046875	(16.0s)
epoch: 72	train loss: 20195.16015625	(16.1s)
epoch: 73	train loss: 21197.96484375	(16.2s)
epoch: 74	train loss: 20148.005859375	(16.1s)
epoch: 75	train loss: 19621.3671875	(16.1s)
epoch: 76	train loss: 20982.36328125	(16.2s)
epoch: 77	train loss: 21174.48046875	(16.1s)
epoch: 78	train loss: 19765.408203125	(16.3s)
epoch: 79	train loss: 20458.22265625	(16.2s)
epoch: 80	train loss: 20036.08203125	(16.3s)
epoch: 81	train loss: 19867.611328125	(16.2s)
epoch: 82	train loss: 20093.46484375	(16.2s)
epoch: 83	train loss: 20174.064453125	(16.3s)
epoch: 84	train loss: 20514.921875	(16.3s)
epoch: 85	train loss: 20204.87109375	(16.2s)
epoch: 86	train loss: 19980.06640625	(16.3s)
epoch: 87	train loss: 19907.2890625	(16.2s)
epoch: 88	train loss: 19759.7890625	(16.3s)
epoch: 89	train loss: 19901.39453125	(16.3s)
epoch: 90	train loss: 20009.94921875	(16.2s)
epoch: 91	train loss: 19767.060546875	(16.2s)
epoch: 92	train loss: 19294.005859375	(16.2s)
epoch: 93	train loss: 19192.94921875	(16.3s)
epoch: 94	train loss: 19128.35546875	(16.3s)
epoch: 95	train loss: 20523.103515625	(16.2s)
epoch: 96	train loss: 18472.37109375	(16.3s)
epoch: 97	train loss: 18094.64453125	(16.2s)
epoch: 98	train loss: 17739.814453125	(16.3s)
epoch: 99	train loss: 17548.93359375	(16.2s)
Evaluating model on 200 episodes
2885.26020154086
1965.3136693954468
2796.942176024119
932.054329413634
2368.5944934258096
2693.0557483037314
2350.657296895981
993.6699329175448
2018.337498801095
2155.8325357620533
5664.091044616699
3656.686442375183
1633.9392451506394
3077.6593274540373
3170.9814761161806
1558.2262657483418
1220.1583626270294
1991.363888502121
1800.9703495502472
2744.1033037821453
3571.760910474337
2554.441755143079
4057.0237140655518
2469.8698719569616
2946.6736664772034
2421.3177947998047
3020.1049128638374
2108.2444263100624
2786.421140193939
3248.765945781361
1892.317239522934
3241.5834345817566
2464.4907158442907
2255.962766265869
2359.4240365982055
Solved 35/200 episodes
445.38149959641055
Evaluated model in 20.5 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-1
Round 2
Generated trajectories in 72.0 seconds
epoch: 0	train loss: 39168.92578125	(16.2s)
epoch: 1	train loss: 34180.0	(16.1s)
epoch: 2	train loss: 32346.22265625	(16.1s)
epoch: 3	train loss: 30493.900390625	(16.2s)
epoch: 4	train loss: 29505.845703125	(16.1s)
epoch: 5	train loss: 28536.099609375	(16.2s)
epoch: 6	train loss: 27845.939453125	(16.1s)
epoch: 7	train loss: 27360.640625	(16.2s)
epoch: 8	train loss: 26723.3828125	(16.2s)
epoch: 9	train loss: 26416.798828125	(16.2s)
epoch: 10	train loss: 26671.763671875	(16.2s)
epoch: 11	train loss: 25429.341796875	(16.2s)
epoch: 12	train loss: 25421.056640625	(16.2s)
epoch: 13	train loss: 25307.2421875	(16.2s)
epoch: 14	train loss: 24780.56640625	(16.2s)
epoch: 15	train loss: 24621.509765625	(16.2s)
epoch: 16	train loss: 24137.611328125	(16.2s)
epoch: 17	train loss: 23662.98046875	(16.2s)
epoch: 18	train loss: 23423.82421875	(16.2s)
epoch: 19	train loss: 23056.998046875	(16.2s)
epoch: 20	train loss: 22703.28515625	(16.3s)
epoch: 21	train loss: 24916.64453125	(16.2s)
epoch: 22	train loss: 22500.5546875	(16.2s)
epoch: 23	train loss: 22172.42578125	(16.2s)
epoch: 24	train loss: 22248.89453125	(16.2s)
epoch: 25	train loss: 22093.03515625	(16.3s)
epoch: 26	train loss: 21879.669921875	(16.2s)
epoch: 27	train loss: 21815.62890625	(16.2s)
epoch: 28	train loss: 21594.716796875	(16.2s)
epoch: 29	train loss: 22281.19140625	(16.2s)
epoch: 30	train loss: 21311.365234375	(16.3s)
epoch: 31	train loss: 21250.62890625	(16.2s)
epoch: 32	train loss: 20649.7890625	(16.2s)
epoch: 33	train loss: 20391.43359375	(16.2s)
epoch: 34	train loss: 20170.673828125	(16.2s)
epoch: 35	train loss: 20109.470703125	(16.3s)
epoch: 36	train loss: 19970.171875	(16.2s)
epoch: 37	train loss: 19794.1640625	(16.2s)
epoch: 38	train loss: 20380.638671875	(16.2s)
epoch: 39	train loss: 19798.408203125	(16.2s)
epoch: 40	train loss: 20049.529296875	(16.3s)
epoch: 41	train loss: 19767.1484375	(16.2s)
epoch: 42	train loss: 19609.6640625	(16.2s)
epoch: 43	train loss: 19365.22265625	(16.2s)
epoch: 44	train loss: 20748.408203125	(16.2s)
epoch: 45	train loss: 19108.048828125	(16.2s)
epoch: 46	train loss: 18777.01953125	(16.2s)
epoch: 47	train loss: 18473.099609375	(16.2s)
epoch: 48	train loss: 18270.673828125	(16.2s)
epoch: 49	train loss: 18060.087890625	(16.2s)
epoch: 50	train loss: 18039.912109375	(16.2s)
epoch: 51	train loss: 18502.744140625	(16.2s)
epoch: 52	train loss: 18061.83984375	(16.2s)
epoch: 53	train loss: 18128.939453125	(16.2s)
epoch: 54	train loss: 17729.0625	(16.2s)
epoch: 55	train loss: 17970.955078125	(16.3s)
epoch: 56	train loss: 18212.572265625	(16.2s)
epoch: 57	train loss: 17819.07421875	(16.2s)
epoch: 58	train loss: 17788.373046875	(16.2s)
epoch: 59	train loss: 17513.4140625	(16.2s)
epoch: 60	train loss: 17706.0078125	(16.2s)
epoch: 61	train loss: 17520.65234375	(16.2s)
epoch: 62	train loss: 17389.19140625	(16.2s)
epoch: 63	train loss: 17238.05859375	(16.2s)
epoch: 64	train loss: 17304.693359375	(16.2s)
epoch: 65	train loss: 16777.193359375	(16.2s)
epoch: 66	train loss: 16778.7578125	(16.2s)
epoch: 67	train loss: 16562.7421875	(16.2s)
epoch: 68	train loss: 16681.58984375	(16.2s)
epoch: 69	train loss: 16628.3515625	(16.2s)
epoch: 70	train loss: 17802.939453125	(16.2s)
epoch: 71	train loss: 16833.05859375	(16.2s)
epoch: 72	train loss: 16511.927734375	(16.1s)
epoch: 73	train loss: 16273.2763671875	(16.1s)
epoch: 74	train loss: 15923.4267578125	(16.1s)
epoch: 75	train loss: 15585.3349609375	(16.1s)
epoch: 76	train loss: 15471.3525390625	(16.1s)
epoch: 77	train loss: 15544.9326171875	(16.1s)
epoch: 78	train loss: 15664.0537109375	(16.2s)
epoch: 79	train loss: 15673.84375	(16.2s)
epoch: 80	train loss: 15658.59765625	(16.2s)
epoch: 81	train loss: 18173.076171875	(16.2s)
epoch: 82	train loss: 15591.5234375	(16.2s)
epoch: 83	train loss: 15422.8916015625	(16.2s)
epoch: 84	train loss: 15545.5859375	(16.2s)
epoch: 85	train loss: 15620.1416015625	(16.2s)
epoch: 86	train loss: 15485.4990234375	(16.2s)
epoch: 87	train loss: 15296.5234375	(16.2s)
epoch: 88	train loss: 15635.68359375	(16.2s)
epoch: 89	train loss: 15549.8310546875	(16.2s)
epoch: 90	train loss: 15543.3515625	(16.2s)
epoch: 91	train loss: 15444.421875	(16.2s)
epoch: 92	train loss: 15783.07421875	(16.2s)
epoch: 93	train loss: 15214.123046875	(16.2s)
epoch: 94	train loss: 15483.580078125	(16.2s)
epoch: 95	train loss: 15264.431640625	(16.2s)
epoch: 96	train loss: 15161.25390625	(16.2s)
epoch: 97	train loss: 14749.0888671875	(16.2s)
epoch: 98	train loss: 15021.439453125	(16.2s)
epoch: 99	train loss: 14998.369140625	(16.2s)
Evaluating model on 200 episodes
1570.1659297943115
2465.8218002319336
2256.4737526503477
2007.7731397702144
3284.7199115753174
2667.7588650050916
4666.505459022522
1859.9162540068994
1803.5663712501525
2757.2923834058975
1725.0766269683838
1849.2849835668292
1285.0463500389685
931.1517131328583
2101.5737935702005
1516.1429034146395
1578.0659651896533
2513.1460470438005
2033.3308054208755
2268.492132663727
1968.1007367372513
1503.6872782340417
1624.9661471098661
1616.6284272545263
1818.7691184055238
2713.5182732582093
2259.099984741211
1646.929153561592
1613.8125239372253
4246.218787193298
1337.4009887150355
2492.1807649476186
1409.1509318351746
Solved 33/200 episodes
346.95884151826607
Evaluated model in 22.4 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-2
Round 3
Generated trajectories in 72.0 seconds
epoch: 0	train loss: 42773.859375	(16.1s)
epoch: 1	train loss: 35930.67578125	(16.3s)
epoch: 2	train loss: 33001.703125	(16.2s)
epoch: 3	train loss: 31150.568359375	(16.2s)
epoch: 4	train loss: 29866.6875	(16.3s)
epoch: 5	train loss: 28866.548828125	(16.3s)
epoch: 6	train loss: 28084.92578125	(16.3s)
epoch: 7	train loss: 27237.42578125	(16.3s)
epoch: 8	train loss: 26795.0	(16.3s)
epoch: 9	train loss: 26559.353515625	(16.3s)
epoch: 10	train loss: 25815.515625	(16.3s)
epoch: 11	train loss: 25363.92578125	(16.3s)
epoch: 12	train loss: 25067.0	(16.4s)
epoch: 13	train loss: 24957.248046875	(16.3s)
epoch: 14	train loss: 24528.8828125	(16.3s)
epoch: 15	train loss: 24738.17578125	(16.3s)
epoch: 16	train loss: 24188.5390625	(16.3s)
epoch: 17	train loss: 23853.4453125	(16.3s)
epoch: 18	train loss: 23426.521484375	(16.3s)
epoch: 19	train loss: 23277.767578125	(16.3s)
epoch: 20	train loss: 23005.462890625	(16.3s)
epoch: 21	train loss: 22961.416015625	(16.3s)
epoch: 22	train loss: 22816.806640625	(16.3s)
epoch: 23	train loss: 22615.359375	(16.3s)
epoch: 24	train loss: 21929.7421875	(16.3s)
epoch: 25	train loss: 21617.47265625	(16.3s)
epoch: 26	train loss: 21683.609375	(16.3s)
epoch: 27	train loss: 21474.54296875	(16.3s)
epoch: 28	train loss: 21391.271484375	(16.3s)
epoch: 29	train loss: 21186.265625	(16.3s)
epoch: 30	train loss: 21482.654296875	(16.3s)
epoch: 31	train loss: 21057.3828125	(16.3s)
epoch: 32	train loss: 20762.505859375	(16.3s)
epoch: 33	train loss: 20495.736328125	(16.3s)
epoch: 34	train loss: 21113.458984375	(16.2s)
epoch: 35	train loss: 20179.810546875	(16.3s)
epoch: 36	train loss: 19908.197265625	(16.3s)
epoch: 37	train loss: 19722.609375	(16.3s)
epoch: 38	train loss: 19492.341796875	(16.3s)
epoch: 39	train loss: 19217.732421875	(16.3s)
epoch: 40	train loss: 18964.75	(16.2s)
epoch: 41	train loss: 18897.5078125	(16.3s)
epoch: 42	train loss: 18745.474609375	(16.3s)
epoch: 43	train loss: 18486.28125	(16.3s)
epoch: 44	train loss: 18315.80859375	(16.3s)
epoch: 45	train loss: 18159.30859375	(16.3s)
epoch: 46	train loss: 18307.041015625	(16.3s)
epoch: 47	train loss: 18540.578125	(16.3s)
epoch: 48	train loss: 17782.880859375	(16.3s)
epoch: 49	train loss: 17744.3203125	(16.3s)
epoch: 50	train loss: 18017.724609375	(16.3s)
epoch: 51	train loss: 18112.04296875	(16.3s)
epoch: 52	train loss: 17765.58203125	(16.3s)
epoch: 53	train loss: 17477.8125	(16.3s)
epoch: 54	train loss: 17934.47265625	(16.3s)
epoch: 55	train loss: 17432.689453125	(16.3s)
epoch: 56	train loss: 17252.21484375	(16.2s)
epoch: 57	train loss: 17077.767578125	(16.3s)
epoch: 58	train loss: 17229.09375	(16.3s)
epoch: 59	train loss: 17180.220703125	(16.3s)
epoch: 60	train loss: 17156.953125	(16.3s)
epoch: 61	train loss: 16601.685546875	(16.3s)
epoch: 62	train loss: 16479.5390625	(16.3s)
epoch: 63	train loss: 16606.59375	(16.3s)
epoch: 64	train loss: 16576.64453125	(16.3s)
epoch: 65	train loss: 16764.23046875	(16.3s)
epoch: 66	train loss: 16788.509765625	(16.3s)
epoch: 67	train loss: 16545.93359375	(16.2s)
epoch: 68	train loss: 16510.384765625	(16.3s)
epoch: 69	train loss: 16478.732421875	(16.3s)
epoch: 70	train loss: 16769.34375	(16.3s)
epoch: 71	train loss: 17019.083984375	(16.1s)
epoch: 72	train loss: 16343.734375	(16.1s)
epoch: 73	train loss: 16805.4140625	(16.1s)
epoch: 74	train loss: 16198.6474609375	(16.1s)
epoch: 75	train loss: 16354.470703125	(16.2s)
epoch: 76	train loss: 16203.353515625	(16.2s)
epoch: 77	train loss: 15838.931640625	(16.3s)
epoch: 78	train loss: 16061.0673828125	(16.3s)
epoch: 79	train loss: 15780.482421875	(16.3s)
epoch: 80	train loss: 16053.69921875	(16.3s)
epoch: 81	train loss: 15480.806640625	(16.2s)
epoch: 82	train loss: 16532.373046875	(16.3s)
epoch: 83	train loss: 15937.9326171875	(16.3s)
epoch: 84	train loss: 15535.271484375	(16.3s)
epoch: 85	train loss: 15491.830078125	(16.3s)
epoch: 86	train loss: 15549.830078125	(16.3s)
epoch: 87	train loss: 15720.171875	(16.3s)
epoch: 88	train loss: 15168.00390625	(16.3s)
epoch: 89	train loss: 15847.181640625	(16.3s)
epoch: 90	train loss: 15430.25	(16.3s)
epoch: 91	train loss: 15312.8525390625	(16.4s)
epoch: 92	train loss: 15267.857421875	(16.3s)
epoch: 93	train loss: 16418.322265625	(16.3s)
epoch: 94	train loss: 15005.052734375	(16.3s)
epoch: 95	train loss: 14795.890625	(16.3s)
epoch: 96	train loss: 14897.6484375	(16.3s)
epoch: 97	train loss: 14702.9892578125	(16.3s)
epoch: 98	train loss: 14797.22265625	(16.3s)
epoch: 99	train loss: 14483.4287109375	(16.3s)
Evaluating model on 200 episodes
658.9237858165394
612.9974389076233
788.2759302457174
1448.541933854421
1948.4075948927139
1295.4544052124024
927.6570907831192
2056.491527557373
1247.5718127659388
617.6674880683422
584.7612802301135
1274.9652711868287
1111.4388779534233
630.5730422973633
1092.8672334126063
1140.1609234280056
780.2113403320312
731.76395266989
1440.851258277893
907.8011237057773
594.5383960088094
1662.45575768607
1179.0946836471558
770.2392587661743
1797.2330716252327
1196.5494418144226
363.3338438272476
694.6410424368722
578.6921621322632
1982.8277782440186
1293.0619323730468
1566.5599340200424
547.1130520502726
Solved 33/200 episodes
177.61861833114878
Evaluated model in 22.3 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-3
Round 4
Generated trajectories in 72.0 seconds
epoch: 0	train loss: 40421.46484375	(16.2s)
epoch: 1	train loss: 34628.421875	(16.1s)
epoch: 2	train loss: 32055.0625	(16.2s)
epoch: 3	train loss: 30316.263671875	(16.2s)
epoch: 4	train loss: 29134.400390625	(16.1s)
epoch: 5	train loss: 28275.017578125	(16.2s)
epoch: 6	train loss: 27409.310546875	(16.2s)
epoch: 7	train loss: 26808.568359375	(16.2s)
epoch: 8	train loss: 26017.203125	(16.2s)
epoch: 9	train loss: 25717.134765625	(16.1s)
epoch: 10	train loss: 25263.009765625	(16.2s)
epoch: 11	train loss: 24863.509765625	(16.2s)
epoch: 12	train loss: 24347.5390625	(16.1s)
epoch: 13	train loss: 24553.3359375	(16.2s)
epoch: 14	train loss: 23796.150390625	(16.1s)
epoch: 15	train loss: 23541.041015625	(16.2s)
epoch: 16	train loss: 24030.013671875	(16.2s)
epoch: 17	train loss: 22811.61328125	(16.2s)
epoch: 18	train loss: 22628.328125	(16.2s)
epoch: 19	train loss: 22526.744140625	(16.2s)
epoch: 20	train loss: 22229.384765625	(16.1s)
epoch: 21	train loss: 22284.9921875	(16.2s)
epoch: 22	train loss: 22134.98828125	(16.2s)
epoch: 23	train loss: 22252.96484375	(16.2s)
epoch: 24	train loss: 21700.6796875	(16.1s)
epoch: 25	train loss: 21353.607421875	(16.2s)
epoch: 26	train loss: 21083.25390625	(16.2s)
epoch: 27	train loss: 22128.10546875	(16.1s)
epoch: 28	train loss: 21314.015625	(16.2s)
epoch: 29	train loss: 21012.287109375	(16.1s)
epoch: 30	train loss: 20650.6953125	(16.1s)
epoch: 31	train loss: 20587.125	(16.2s)
epoch: 32	train loss: 20112.365234375	(16.2s)
epoch: 33	train loss: 19710.3359375	(16.2s)
epoch: 34	train loss: 19425.974609375	(16.2s)
epoch: 35	train loss: 19470.66015625	(16.1s)
epoch: 36	train loss: 19163.802734375	(16.1s)
epoch: 37	train loss: 18955.626953125	(16.2s)
epoch: 38	train loss: 19225.328125	(16.2s)
epoch: 39	train loss: 18842.63671875	(16.2s)
epoch: 40	train loss: 18836.220703125	(16.1s)
epoch: 41	train loss: 18625.732421875	(16.2s)
epoch: 42	train loss: 18126.990234375	(16.2s)
epoch: 43	train loss: 18602.748046875	(16.1s)
epoch: 44	train loss: 18038.73828125	(16.2s)
epoch: 45	train loss: 18004.2265625	(16.1s)
epoch: 46	train loss: 17958.298828125	(16.1s)
epoch: 47	train loss: 18100.609375	(16.2s)
epoch: 48	train loss: 19018.478515625	(16.2s)
epoch: 49	train loss: 17498.44921875	(16.2s)
epoch: 50	train loss: 17260.71875	(16.1s)
epoch: 51	train loss: 17262.646484375	(16.1s)
epoch: 52	train loss: 16928.6796875	(16.2s)
epoch: 53	train loss: 17324.771484375	(16.2s)
epoch: 54	train loss: 16949.5859375	(16.2s)
epoch: 55	train loss: 16983.234375	(16.2s)
epoch: 56	train loss: 17023.15234375	(16.1s)
epoch: 57	train loss: 16770.689453125	(16.2s)
epoch: 58	train loss: 17066.810546875	(16.2s)
epoch: 59	train loss: 16816.52734375	(16.2s)
epoch: 60	train loss: 16808.94140625	(16.2s)
epoch: 61	train loss: 16330.755859375	(16.2s)
epoch: 62	train loss: 16542.408203125	(16.2s)
epoch: 63	train loss: 16558.7109375	(16.2s)
epoch: 64	train loss: 16044.0390625	(16.1s)
epoch: 65	train loss: 16113.62890625	(16.2s)
epoch: 66	train loss: 16651.673828125	(16.2s)
epoch: 67	train loss: 16197.787109375	(16.2s)
epoch: 68	train loss: 15786.162109375	(16.2s)
epoch: 69	train loss: 16354.162109375	(16.1s)
epoch: 70	train loss: 15784.892578125	(16.1s)
epoch: 71	train loss: 15335.5068359375	(16.0s)
epoch: 72	train loss: 16145.177734375	(16.0s)
epoch: 73	train loss: 15562.216796875	(16.1s)
epoch: 74	train loss: 15822.2939453125	(16.1s)
epoch: 75	train loss: 15650.2861328125	(16.1s)
epoch: 76	train loss: 16165.6943359375	(16.1s)
epoch: 77	train loss: 15196.9970703125	(16.1s)
epoch: 78	train loss: 15121.341796875	(16.2s)
epoch: 79	train loss: 14952.9482421875	(16.2s)
epoch: 80	train loss: 15239.41796875	(16.2s)
epoch: 81	train loss: 14843.7880859375	(16.2s)
epoch: 82	train loss: 14763.96484375	(16.2s)
epoch: 83	train loss: 15023.8955078125	(16.2s)
epoch: 84	train loss: 14623.5693359375	(16.2s)
epoch: 85	train loss: 14361.689453125	(16.2s)
epoch: 86	train loss: 14630.3466796875	(16.2s)
epoch: 87	train loss: 14382.2919921875	(16.1s)
epoch: 88	train loss: 14368.6240234375	(16.2s)
epoch: 89	train loss: 14443.4453125	(16.2s)
epoch: 90	train loss: 14233.7861328125	(16.2s)
epoch: 91	train loss: 14224.98828125	(16.2s)
epoch: 92	train loss: 14042.6845703125	(16.1s)
epoch: 93	train loss: 14116.572265625	(16.2s)
epoch: 94	train loss: 14378.384765625	(16.2s)
epoch: 95	train loss: 14059.8232421875	(16.2s)
epoch: 96	train loss: 14054.6767578125	(16.2s)
epoch: 97	train loss: 13606.08984375	(16.1s)
epoch: 98	train loss: 13686.6650390625	(16.1s)
epoch: 99	train loss: 14003.0693359375	(16.2s)
Evaluating model on 200 episodes
465.5632403237479
205.94247781276704
370.47191416422527
894.0058141435895
446.34554189046224
355.12016154925027
483.8555991053581
443.541652766141
362.0624780654907
425.7635528246562
483.3904759751426
578.6498056352139
579.5024847666423
354.76123258045743
563.8535995653698
441.23429572582245
704.4058062632879
343.6726778348287
471.6114677853054
602.7615386247635
354.11987664964465
938.4181749343873
555.5855746799045
737.3086335659027
303.5232858657837
685.1113811069065
612.1305994987488
477.3722029725711
311.74242042092715
561.5510871193626
536.4015197052676
458.63301372528076
437.6465549468994
410.02074252234564
438.82484436035156
738.7784136136373
881.8566104888916
496.01287732805525
599.0615243911743
519.7973374790615
Solved 40/200 episodes
103.1520624638881
Evaluated model in 21.8 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-4
Round 5
Generated trajectories in 71.5 seconds
epoch: 0	train loss: 36269.33203125	(16.2s)
epoch: 1	train loss: 30574.025390625	(16.2s)
epoch: 2	train loss: 27462.654296875	(16.3s)
epoch: 3	train loss: 25451.71875	(16.2s)
epoch: 4	train loss: 24289.73046875	(16.3s)
epoch: 5	train loss: 23249.609375	(16.3s)
epoch: 6	train loss: 22397.33203125	(16.3s)
epoch: 7	train loss: 21365.56640625	(16.3s)
epoch: 8	train loss: 20593.458984375	(16.3s)
epoch: 9	train loss: 20407.26953125	(16.3s)
epoch: 10	train loss: 20172.734375	(16.3s)
epoch: 11	train loss: 19241.7109375	(16.3s)
epoch: 12	train loss: 18530.888671875	(16.2s)
epoch: 13	train loss: 18504.9765625	(16.3s)
epoch: 14	train loss: 17759.45703125	(16.2s)
epoch: 15	train loss: 17471.6484375	(16.2s)
epoch: 16	train loss: 16733.154296875	(16.3s)
epoch: 17	train loss: 17744.1953125	(16.2s)
epoch: 18	train loss: 16232.560546875	(16.3s)
epoch: 19	train loss: 15866.5	(16.2s)
epoch: 20	train loss: 16219.8046875	(16.2s)
epoch: 21	train loss: 15495.30859375	(16.3s)
epoch: 22	train loss: 14892.259765625	(16.2s)
epoch: 23	train loss: 15228.05078125	(16.3s)
epoch: 24	train loss: 14828.451171875	(16.2s)
epoch: 25	train loss: 14505.34375	(16.2s)
epoch: 26	train loss: 14326.115234375	(16.2s)
epoch: 27	train loss: 14193.9072265625	(16.3s)
epoch: 28	train loss: 14204.5625	(16.2s)
epoch: 29	train loss: 13831.5615234375	(16.3s)
epoch: 30	train loss: 13233.4736328125	(16.2s)
epoch: 31	train loss: 13035.7724609375	(16.2s)
epoch: 32	train loss: 12542.80078125	(16.3s)
epoch: 33	train loss: 12261.59375	(16.2s)
epoch: 34	train loss: 12004.3837890625	(16.4s)
epoch: 35	train loss: 11549.06640625	(16.2s)
epoch: 36	train loss: 11410.0751953125	(16.3s)
epoch: 37	train loss: 11002.638671875	(16.3s)
epoch: 38	train loss: 11215.3232421875	(16.2s)
epoch: 39	train loss: 10229.466796875	(16.3s)
epoch: 40	train loss: 10871.8232421875	(16.2s)
epoch: 41	train loss: 9852.2646484375	(16.2s)
epoch: 42	train loss: 10237.6865234375	(16.2s)
epoch: 43	train loss: 9937.6171875	(16.2s)
epoch: 44	train loss: 9666.3896484375	(16.3s)
epoch: 45	train loss: 9062.658203125	(16.2s)
epoch: 46	train loss: 8662.4296875	(16.2s)
epoch: 47	train loss: 9383.6044921875	(16.2s)
epoch: 48	train loss: 9113.876953125	(16.3s)
epoch: 49	train loss: 8299.296875	(16.2s)
epoch: 50	train loss: 8908.9130859375	(16.3s)
epoch: 51	train loss: 8742.7333984375	(16.2s)
epoch: 52	train loss: 8353.8447265625	(16.2s)
epoch: 53	train loss: 7886.216796875	(16.3s)
epoch: 54	train loss: 7547.20849609375	(16.2s)
epoch: 55	train loss: 8158.0927734375	(16.3s)
epoch: 56	train loss: 8798.2998046875	(16.2s)
epoch: 57	train loss: 8319.0166015625	(16.2s)
epoch: 58	train loss: 7382.146484375	(16.3s)
epoch: 59	train loss: 7556.24267578125	(16.2s)
epoch: 60	train loss: 7084.91650390625	(16.3s)
epoch: 61	train loss: 6722.1376953125	(16.2s)
epoch: 62	train loss: 6739.04541015625	(16.2s)
epoch: 63	train loss: 6890.2099609375	(16.2s)
epoch: 64	train loss: 6950.4912109375	(16.3s)
epoch: 65	train loss: 6626.02197265625	(16.2s)
epoch: 66	train loss: 6156.79638671875	(16.3s)
epoch: 67	train loss: 6544.880859375	(16.2s)
epoch: 68	train loss: 6854.859375	(16.2s)
epoch: 69	train loss: 6170.544921875	(16.2s)
epoch: 70	train loss: 6933.50634765625	(16.1s)
epoch: 71	train loss: 6584.9375	(16.1s)
epoch: 72	train loss: 6335.98974609375	(16.0s)
epoch: 73	train loss: 6171.42724609375	(16.0s)
epoch: 74	train loss: 5316.7587890625	(16.1s)
epoch: 75	train loss: 5743.47314453125	(16.1s)
epoch: 76	train loss: 6018.52001953125	(16.2s)
epoch: 77	train loss: 6117.2001953125	(16.2s)
epoch: 78	train loss: 6073.5654296875	(16.2s)
epoch: 79	train loss: 5412.94091796875	(16.2s)
epoch: 80	train loss: 5391.67529296875	(16.2s)
epoch: 81	train loss: 5747.32470703125	(16.2s)
epoch: 82	train loss: 6106.9208984375	(16.2s)
epoch: 83	train loss: 4971.93408203125	(16.2s)
epoch: 84	train loss: 5942.98095703125	(16.2s)
epoch: 85	train loss: 5148.65380859375	(16.2s)
epoch: 86	train loss: 5030.07861328125	(16.2s)
epoch: 87	train loss: 5651.5048828125	(16.3s)
epoch: 88	train loss: 4988.6240234375	(16.2s)
epoch: 89	train loss: 5021.1181640625	(16.2s)
epoch: 90	train loss: 4865.8701171875	(16.2s)
epoch: 91	train loss: 4313.28515625	(16.2s)
epoch: 92	train loss: 5588.724609375	(16.3s)
epoch: 93	train loss: 4324.2900390625	(16.2s)
epoch: 94	train loss: 3966.66455078125	(16.2s)
epoch: 95	train loss: 4432.95166015625	(16.2s)
epoch: 96	train loss: 4553.1064453125	(16.2s)
epoch: 97	train loss: 5476.57373046875	(16.2s)
epoch: 98	train loss: 4310.4833984375	(16.2s)
epoch: 99	train loss: 4271.41943359375	(16.2s)
Evaluating model on 200 episodes
262.57201039349593
384.3570536295573
424.2478577136993
399.28674093882245
338.5146103734555
169.59544983777133
330.56849638621014
297.8324974985684
330.6401124883581
398.14174483372614
463.159002494812
317.1491800237585
437.5609164604774
254.50699424743652
307.49803043233936
615.8670394134522
295.99196910858154
378.9905355998448
552.5338100857205
312.1416029930115
535.057642574968
313.57707231695
315.8862849148837
391.3008977104636
601.9995736016167
304.66760472690356
427.10746121406555
1588.02200050354
558.2176682154337
456.731438930218
347.2226092598655
402.6399253148299
277.5592432022095
461.0918973514012
497.0687619249026
811.0722874005636
397.69995621272494
361.4467493693034
605.4371238284641
215.44122064113617
378.5070254516602
362.87709452992397
326.0280087210915
313.8688395541647
484.5890625953674
179.46601724624634
231.69705454508463
398.3829971313477
423.0511359084736
484.7725810000771
424.55738312857494
295.3442236801674
206.0449950002855
811.8608769503506
544.4434961742825
499.5810058684576
266.11675573349
215.7066713809967
311.81109639576505
452.9400142322887
410.2370687567669
327.4841294288635
351.6681915731991
432.0574417114258
353.71892211550755
473.6982361511751
276.8064764738083
426.52484369277954
378.5109983269985
630.2408587137858
525.6573809782664
359.9930305480957
368.93226385116577
503.4326139177595
345.2428082466125
559.268113553524
512.6766285615809
260.30603007114297
293.3709610158747
369.8991933076278
434.7833080291748
389.7590777609083
350.8991215649773
280.977718184976
378.93254137039185
417.1985538482666
336.45617899894717
533.6764524708624
793.5947555541992
421.2487281799316
526.5470226461238
312.88840793427966
743.3272817929586
472.7530960669884
369.613056789745
385.05110650313526
408.18297386169434
354.0405638845343
375.1000831127167
311.87694108486176
377.94972648620603
456.13444859272727
252.01419199430026
291.8604008810861
495.90998659635846
302.7810168457031
287.8378617167473
442.2902948591444
259.3096264492382
368.89077711105347
271.4470331915494
319.92192290810976
413.8987789154053
607.8062015533447
525.7840374537876
425.9199225743612
382.79697383533824
387.45587158203125
426.4945465087891
545.8242301940918
303.3561358885332
Solved 121/200 episodes
248.27183278084595
Evaluated model in 27.3 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-5
Round 6
Generated trajectories in 71.9 seconds
epoch: 0	train loss: 15471.52734375	(16.1s)
epoch: 1	train loss: 11946.8017578125	(16.0s)
epoch: 2	train loss: 10826.1904296875	(16.1s)
epoch: 3	train loss: 9127.8955078125	(16.1s)
epoch: 4	train loss: 8634.251953125	(16.1s)
epoch: 5	train loss: 8834.1123046875	(16.1s)
epoch: 6	train loss: 7502.6494140625	(16.1s)
epoch: 7	train loss: 7832.36572265625	(16.1s)
epoch: 8	train loss: 7143.4208984375	(16.1s)
epoch: 9	train loss: 6859.1474609375	(16.1s)
epoch: 10	train loss: 7251.7890625	(16.1s)
epoch: 11	train loss: 6958.861328125	(16.1s)
epoch: 12	train loss: 6344.802734375	(16.1s)
epoch: 13	train loss: 6264.94677734375	(16.1s)
epoch: 14	train loss: 6118.48291015625	(16.1s)
epoch: 15	train loss: 5533.9365234375	(16.1s)
epoch: 16	train loss: 4847.248046875	(16.0s)
epoch: 17	train loss: 4586.03466796875	(16.1s)
epoch: 18	train loss: 5296.24267578125	(16.1s)
epoch: 19	train loss: 4876.7587890625	(16.1s)
epoch: 20	train loss: 4915.21826171875	(16.1s)
epoch: 21	train loss: 4900.63818359375	(16.1s)
epoch: 22	train loss: 7083.2470703125	(16.1s)
epoch: 23	train loss: 4435.7041015625	(16.1s)
epoch: 24	train loss: 5085.8837890625	(16.2s)
epoch: 25	train loss: 7193.12255859375	(16.3s)
epoch: 26	train loss: 4939.24658203125	(16.2s)
epoch: 27	train loss: 4699.740234375	(16.2s)
epoch: 28	train loss: 4136.328125	(16.2s)
epoch: 29	train loss: 3983.315185546875	(16.2s)
epoch: 30	train loss: 5572.7841796875	(16.2s)
epoch: 31	train loss: 4080.760986328125	(16.2s)
epoch: 32	train loss: 4078.21826171875	(16.2s)
epoch: 33	train loss: 4235.9755859375	(16.2s)
epoch: 34	train loss: 3533.928955078125	(16.2s)
epoch: 35	train loss: 3729.291259765625	(16.2s)
epoch: 36	train loss: 3803.748779296875	(16.2s)
epoch: 37	train loss: 4380.55029296875	(16.2s)
epoch: 38	train loss: 3293.902099609375	(16.2s)
epoch: 39	train loss: 3735.675537109375	(16.3s)
epoch: 40	train loss: 5000.79248046875	(16.2s)
epoch: 41	train loss: 4140.845703125	(16.3s)
epoch: 42	train loss: 3143.354736328125	(16.2s)
epoch: 43	train loss: 3707.99951171875	(16.3s)
epoch: 44	train loss: 4064.04443359375	(16.2s)
epoch: 45	train loss: 2524.780029296875	(16.2s)
epoch: 46	train loss: 3289.85791015625	(16.2s)
epoch: 47	train loss: 2542.266357421875	(16.2s)
epoch: 48	train loss: 3979.554443359375	(16.2s)
epoch: 49	train loss: 3375.109619140625	(16.2s)
epoch: 50	train loss: 2768.663818359375	(16.2s)
epoch: 51	train loss: 2670.267333984375	(16.2s)
epoch: 52	train loss: 2430.451904296875	(16.2s)
epoch: 53	train loss: 4171.6162109375	(16.2s)
epoch: 54	train loss: 2635.8212890625	(16.2s)
epoch: 55	train loss: 2532.592041015625	(16.2s)
epoch: 56	train loss: 2192.559814453125	(16.2s)
epoch: 57	train loss: 3151.34326171875	(16.2s)
epoch: 58	train loss: 2915.587890625	(16.2s)
epoch: 59	train loss: 3143.43701171875	(16.2s)
epoch: 60	train loss: 2895.972412109375	(16.2s)
epoch: 61	train loss: 2527.6337890625	(16.2s)
epoch: 62	train loss: 2436.746337890625	(16.2s)
epoch: 63	train loss: 2230.5908203125	(16.2s)
epoch: 64	train loss: 3360.698974609375	(16.2s)
epoch: 65	train loss: 2618.97900390625	(16.2s)
epoch: 66	train loss: 2219.110595703125	(16.2s)
epoch: 67	train loss: 2781.351318359375	(16.2s)
epoch: 68	train loss: 4146.7431640625	(16.1s)
epoch: 69	train loss: 2053.454345703125	(16.1s)
epoch: 70	train loss: 1444.4117431640625	(16.1s)
epoch: 71	train loss: 2900.42724609375	(16.2s)
epoch: 72	train loss: 2359.605712890625	(16.1s)
epoch: 73	train loss: 1992.306640625	(16.1s)
epoch: 74	train loss: 2468.078857421875	(16.1s)
epoch: 75	train loss: 1855.12353515625	(16.1s)
epoch: 76	train loss: 2334.91845703125	(16.1s)
epoch: 77	train loss: 4564.138671875	(16.2s)
epoch: 78	train loss: 1949.076171875	(16.2s)
epoch: 79	train loss: 2830.811279296875	(16.2s)
epoch: 80	train loss: 2380.573974609375	(16.2s)
epoch: 81	train loss: 2848.816650390625	(16.2s)
epoch: 82	train loss: 1853.8050537109375	(16.2s)
epoch: 83	train loss: 1662.8016357421875	(16.2s)
epoch: 84	train loss: 2424.421875	(16.2s)
epoch: 85	train loss: 1777.197998046875	(16.2s)
epoch: 86	train loss: 2066.16455078125	(16.2s)
epoch: 87	train loss: 1809.5054931640625	(16.2s)
epoch: 88	train loss: 2706.5986328125	(16.2s)
epoch: 89	train loss: 1492.1207275390625	(16.2s)
epoch: 90	train loss: 1763.6741943359375	(16.2s)
epoch: 91	train loss: 2198.733642578125	(16.2s)
epoch: 92	train loss: 2025.2657470703125	(16.2s)
epoch: 93	train loss: 3017.71923828125	(16.2s)
epoch: 94	train loss: 1765.4288330078125	(16.2s)
epoch: 95	train loss: 1720.1278076171875	(16.1s)
epoch: 96	train loss: 1750.085693359375	(16.2s)
epoch: 97	train loss: 2775.560791015625	(16.2s)
epoch: 98	train loss: 2980.799072265625	(16.2s)
epoch: 99	train loss: 1366.6966552734375	(16.2s)
Evaluating model on 200 episodes
520.059961605072
487.77153657277427
504.0268485729511
500.6928007091795
431.06306648254395
416.06301641464233
863.1080619812012
494.0977485179901
425.1241286884655
503.05474758148193
227.78562817750154
453.47508460596987
396.9705379009247
578.8893092473348
320.9225423089389
689.6462614695231
299.03041797214087
415.90375006198883
437.82915115356445
408.1679000854492
461.43975162506104
390.94071635493526
434.94826256601436
562.2766554355621
408.2928890228271
408.8216445320531
544.9499671333715
577.4652786254883
438.38943974177045
550.9814384460449
424.7454904675484
558.1706993103028
381.010933810267
644.4567688306173
390.52773043314613
295.5361521584647
320.20957016944885
429.22594079375267
505.33043473561605
603.0249761104584
419.7286915010022
417.4301889780405
517.1591270110187
380.27656740755646
517.7984203925499
383.0985930633545
611.2687571048737
505.15015496148004
458.6207636727227
467.75644742117987
298.96227049827576
430.02838611602783
287.3856025864096
357.5597043718611
301.42245213190716
360.8013423681259
416.82114937430936
395.55832470787897
288.40506825080286
488.5393461954026
395.8697215960576
425.5559438705444
369.99821056638444
564.7493815702551
321.3862544910328
454.62328514686
599.7144285202027
465.2525062561035
373.4553331019832
745.9152647654215
300.07736651102704
349.8656956195831
496.64725359748394
451.42557191848755
669.1986246705055
331.91794174194337
455.1878302200981
247.32174907011145
518.2379685810635
347.1756358899568
483.9591954838146
372.72032172339306
456.9780189807598
536.6998600165049
547.1884277568143
394.2059023221334
258.7288127626692
641.8816224098206
747.5645484924316
559.2040557127732
515.8652843026554
396.06002916608537
482.0726334253947
323.08611186345416
447.1974921067556
1059.9390716552734
473.23294990062715
416.1852453947067
451.2987958908081
443.90369577407836
567.6581708467924
839.2426512581961
548.2371060988482
339.6899893283844
398.1246902412838
457.18148245130266
473.96258024735886
504.09208664527307
499.8137657543023
369.5139736599392
526.839284658432
318.62494022288223
472.5434559958322
341.7041469573975
354.1153392791748
465.2321395140428
488.8035585230047
511.45674896240234
453.51723776923285
395.7890602520534
396.1979271924054
475.185130182902
433.8955486570086
464.0013839933607
322.69209520022076
457.43894871075946
474.33837154933383
516.4069989885603
487.9699163966709
347.2916171334007
558.5901435017586
376.9358150959015
481.95160469055173
501.49340732281024
600.9591772372906
477.81375269456345
507.8868855696458
447.49019721349083
601.1212302616665
493.8233360742268
460.03971285290186
607.391690662929
272.0744719696045
971.0790891647339
500.35241817474366
408.9816291809082
600.7438543493098
624.7119055986404
370.1994461551789
638.9761328263717
405.0363905131817
536.2968924663685
870.4608262869028
321.9931297302246
429.5465866435658
342.286255917898
441.3643530065363
469.8129883720761
591.2069329654469
629.11736536026
313.82604294731505
349.0668087899685
617.1080284572784
441.38190519809723
327.57466785724347
422.8662903859065
835.3441299438476
445.29588359335196
468.9475038428056
608.5114419301351
529.9263873100281
359.65986313819883
395.088826417923
573.066217581431
359.8586114645004
512.0176823616027
572.372924288114
365.0352690219879
Solved 178/200 episodes
418.18996547216716
Evaluated model in 32.3 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-6
Round 7
Generated trajectories in 72.5 seconds
epoch: 0	train loss: 7662.27392578125	(16.5s)
epoch: 1	train loss: 5023.11279296875	(16.4s)
epoch: 2	train loss: 4910.28564453125	(16.5s)
epoch: 3	train loss: 4663.17578125	(16.6s)
epoch: 4	train loss: 4156.716796875	(16.5s)
epoch: 5	train loss: 4678.3505859375	(16.6s)
epoch: 6	train loss: 4308.97509765625	(16.5s)
epoch: 7	train loss: 3921.378173828125	(16.5s)
epoch: 8	train loss: 3112.82470703125	(16.6s)
epoch: 9	train loss: 3330.50341796875	(16.5s)
epoch: 10	train loss: 3387.35595703125	(16.6s)
epoch: 11	train loss: 3193.454345703125	(16.5s)
epoch: 12	train loss: 2732.496826171875	(16.5s)
epoch: 13	train loss: 5417.89111328125	(16.6s)
epoch: 14	train loss: 2329.160888671875	(16.5s)
epoch: 15	train loss: 2384.018798828125	(16.5s)
epoch: 16	train loss: 4833.21484375	(16.5s)
epoch: 17	train loss: 2165.56689453125	(16.5s)
epoch: 18	train loss: 1898.1016845703125	(16.5s)
epoch: 19	train loss: 2247.77294921875	(16.5s)
epoch: 20	train loss: 3275.465087890625	(16.5s)
epoch: 21	train loss: 2737.882568359375	(16.5s)
epoch: 22	train loss: 2058.895263671875	(16.5s)
epoch: 23	train loss: 1873.4571533203125	(16.5s)
epoch: 24	train loss: 1593.3636474609375	(16.5s)
epoch: 25	train loss: 3879.89013671875	(16.5s)
epoch: 26	train loss: 2858.853271484375	(16.5s)
epoch: 27	train loss: 2123.18310546875	(16.5s)
epoch: 28	train loss: 2355.824462890625	(16.5s)
epoch: 29	train loss: 1509.279296875	(16.5s)
epoch: 30	train loss: 2530.941650390625	(16.5s)
epoch: 31	train loss: 1235.2706298828125	(16.5s)
epoch: 32	train loss: 1285.5946044921875	(16.4s)
epoch: 33	train loss: 2634.146484375	(16.5s)
epoch: 34	train loss: 2728.87548828125	(16.5s)
epoch: 35	train loss: 2089.2001953125	(16.5s)
epoch: 36	train loss: 3108.481689453125	(16.5s)
epoch: 37	train loss: 2454.934814453125	(16.4s)
epoch: 38	train loss: 1749.335205078125	(16.4s)
epoch: 39	train loss: 1981.3988037109375	(16.5s)
epoch: 40	train loss: 1421.79296875	(16.5s)
epoch: 41	train loss: 2310.202880859375	(16.5s)
epoch: 42	train loss: 1528.768798828125	(16.5s)
epoch: 43	train loss: 1629.34130859375	(16.4s)
epoch: 44	train loss: 1963.0975341796875	(16.5s)
epoch: 45	train loss: 2598.265869140625	(16.5s)
epoch: 46	train loss: 2216.987060546875	(16.5s)
epoch: 47	train loss: 1524.07080078125	(16.4s)
epoch: 48	train loss: 1793.2130126953125	(16.5s)
epoch: 49	train loss: 1842.7618408203125	(16.5s)
epoch: 50	train loss: 2083.47119140625	(16.5s)
epoch: 51	train loss: 1898.865234375	(16.5s)
epoch: 52	train loss: 2379.171630859375	(16.5s)
epoch: 53	train loss: 1553.065673828125	(16.4s)
epoch: 54	train loss: 983.850830078125	(16.5s)
epoch: 55	train loss: 1804.2457275390625	(16.5s)
epoch: 56	train loss: 1688.341796875	(16.5s)
epoch: 57	train loss: 1601.1849365234375	(16.5s)
epoch: 58	train loss: 2000.345703125	(16.4s)
epoch: 59	train loss: 1249.5433349609375	(16.5s)
epoch: 60	train loss: 903.5588989257812	(16.5s)
epoch: 61	train loss: 1658.5283203125	(16.5s)
epoch: 62	train loss: 5584.2099609375	(16.5s)
epoch: 63	train loss: 2190.547607421875	(16.4s)
epoch: 64	train loss: 2058.57080078125	(16.5s)
epoch: 65	train loss: 1198.96826171875	(16.5s)
epoch: 66	train loss: 3356.240478515625	(16.4s)
epoch: 67	train loss: 1435.910400390625	(16.5s)
epoch: 68	train loss: 757.0548095703125	(16.4s)
epoch: 69	train loss: 1059.81787109375	(16.3s)
epoch: 70	train loss: 1504.2373046875	(16.3s)
epoch: 71	train loss: 2556.0966796875	(16.3s)
epoch: 72	train loss: 1247.0225830078125	(16.4s)
epoch: 73	train loss: 1438.611572265625	(16.4s)
epoch: 74	train loss: 978.1383666992188	(16.3s)
epoch: 75	train loss: 926.1822509765625	(16.4s)
epoch: 76	train loss: 996.7044067382812	(16.5s)
epoch: 77	train loss: 2141.85009765625	(16.5s)
epoch: 78	train loss: 1704.0269775390625	(16.5s)
epoch: 79	train loss: 794.381591796875	(16.5s)
epoch: 80	train loss: 1367.02392578125	(16.5s)
epoch: 81	train loss: 1403.81982421875	(16.5s)
epoch: 82	train loss: 1156.7491455078125	(16.5s)
epoch: 83	train loss: 1468.7830810546875	(16.5s)
epoch: 84	train loss: 1173.433349609375	(16.4s)
epoch: 85	train loss: 1440.6341552734375	(16.4s)
epoch: 86	train loss: 847.9014892578125	(16.5s)
epoch: 87	train loss: 1216.987060546875	(16.4s)
epoch: 88	train loss: 1482.5157470703125	(16.5s)
epoch: 89	train loss: 1831.96875	(16.4s)
epoch: 90	train loss: 889.8966674804688	(16.4s)
epoch: 91	train loss: 842.9338989257812	(16.5s)
epoch: 92	train loss: 1411.4873046875	(16.4s)
epoch: 93	train loss: 2195.588623046875	(16.5s)
epoch: 94	train loss: 881.3592529296875	(16.4s)
epoch: 95	train loss: 1637.08203125	(16.4s)
epoch: 96	train loss: 716.4937744140625	(16.5s)
epoch: 97	train loss: 2201.67333984375	(16.5s)
epoch: 98	train loss: 1440.070556640625	(16.5s)
epoch: 99	train loss: 1278.071533203125	(16.5s)
Evaluating model on 200 episodes
1224.6870227541242
790.6591755260121
692.9624751055682
875.9638610050596
1974.366718837193
744.9794395848324
1212.6694113810856
2065.64559173584
1209.3557179450988
1043.449497350057
860.6985582552458
1056.7379541397095
694.512017931257
1080.7951463063557
765.588220233009
917.3135499954224
1290.8238775389534
1097.6434853076935
1790.6405250549317
1118.7541563327495
1183.378470494197
930.5373703724629
934.3404828389486
1196.1386229607367
943.6797724077778
679.2381621651027
993.8605077916926
1069.5154631137848
1269.0783226706765
808.9993327331543
1156.29069964091
946.6909817457199
1080.692041516304
1010.6208219528198
752.952376127243
971.8547199249267
784.0185235159738
954.0897580252754
894.2650505804246
886.7274044210261
852.764074124788
1305.0639667510986
1026.1133743286132
1208.1161023094542
777.3230618370903
1229.4965231759209
1009.5188462313483
760.1984098097857
1053.1031218029204
1077.765487730503
955.008224105835
671.3300988333566
966.9755675217201
910.3321346514152
690.7679660264836
853.5788196794914
1080.8074394861858
751.7372955434463
882.8813467283506
1169.8373260696728
920.5402661429512
1342.8562989234924
1130.3109185791016
708.8986872355143
1190.50657081604
783.5213567415873
1305.4526448931013
739.5849161965507
898.2743736135549
912.8638054075695
1027.2830740984748
962.4836345248752
1487.3176264762878
788.7109290707496
853.981014139512
1084.3638668621288
728.2419580078125
862.4911444028219
1441.08141544887
1040.6265949102549
780.4337046305338
834.5697798477976
1363.2486521402996
745.5788762641675
962.4867657423019
880.8932907104493
940.1274113526215
1116.7476102655585
1080.187835931778
821.1963005747114
911.704015661169
748.0487626393636
932.3693579864502
1092.1859482129414
874.8853267942156
685.9922179494586
1090.9853808085124
1325.8889554894488
1129.9853918711344
858.9138041178386
1650.4881510053362
992.0821616833026
726.3387591044108
1185.1524918079376
958.7195401625199
882.6793122291565
736.8175558792917
1108.130667368571
927.6848749433245
1132.840087666231
1333.9538065592449
1280.1529691815376
884.0207791882892
1346.141212463379
1212.2534300762675
831.4248005260121
931.2461488897151
959.2833059559698
928.5603952407837
1324.5174001966204
778.2557819911411
751.8248751681784
879.5905949473381
948.1073501110077
1034.7066730152476
1089.3142800104051
856.8208689371745
1040.198563952195
1259.848056793213
882.5975394248962
1012.2521065421727
1119.3259375435966
888.6261855125427
843.3268771993703
926.447766703944
1060.3596421700936
1064.31136586931
973.3598007626003
1006.6719876802885
720.1858673776899
909.8641609712081
975.695542300189
713.7916352748871
932.5325795049253
821.4124152534887
926.4662421544393
836.4772484753582
1054.8241642543248
846.8399931100698
1113.5706941286724
812.0589537413224
1083.7646697117732
860.9037758020254
942.9737136477515
894.1537802911574
868.8755971847042
1002.8192075532058
1073.8773611704507
1263.811028113732
948.0130870607164
931.5247001086964
1029.2331161499023
932.4251970563616
1225.8222626580132
1463.0177853448051
1500.2591908398797
1041.0301349639892
1183.1740856170654
1054.6616382598877
1226.9525061713325
1199.9717841148376
1208.1626404032988
1045.1845286976206
822.5997540610177
1501.113153219223
992.104572614034
1101.184384952892
837.129776877326
810.4336651393345
1255.213502080817
847.9785652160645
1173.4493060112
1119.424823579334
927.7442562103272
750.5728797045621
1009.1353478022984
2071.6644739423477
1002.7080189740217
Solved 188/200 episodes
955.94488415932
Evaluated model in 32.9 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-7
Round 8
Generated trajectories in 72.6 seconds
epoch: 0	train loss: 4308.107421875	(16.5s)
epoch: 1	train loss: 2894.59423828125	(16.2s)
epoch: 2	train loss: 2547.092529296875	(16.4s)
epoch: 3	train loss: 2124.674560546875	(16.4s)
epoch: 4	train loss: 2966.477294921875	(16.3s)
epoch: 5	train loss: 2608.091796875	(16.4s)
epoch: 6	train loss: 1588.8060302734375	(16.4s)
epoch: 7	train loss: 3206.96044921875	(16.3s)
epoch: 8	train loss: 2128.4580078125	(16.3s)
epoch: 9	train loss: 1652.09521484375	(16.4s)
epoch: 10	train loss: 2156.032470703125	(16.4s)
epoch: 11	train loss: 1415.178955078125	(16.2s)
epoch: 12	train loss: 1562.6776123046875	(16.4s)
epoch: 13	train loss: 1409.3193359375	(16.4s)
epoch: 14	train loss: 2553.430419921875	(16.4s)
epoch: 15	train loss: 1285.67138671875	(16.4s)
epoch: 16	train loss: 3978.57763671875	(16.4s)
epoch: 17	train loss: 2147.375	(16.4s)
epoch: 18	train loss: 1366.2308349609375	(16.3s)
epoch: 19	train loss: 1415.0107421875	(16.4s)
epoch: 20	train loss: 1052.2611083984375	(16.5s)
epoch: 21	train loss: 1277.9962158203125	(16.2s)
epoch: 22	train loss: 2683.86328125	(16.3s)
epoch: 23	train loss: 1140.3544921875	(16.4s)
epoch: 24	train loss: 1397.392578125	(16.4s)
epoch: 25	train loss: 1192.110107421875	(16.4s)
epoch: 26	train loss: 1050.196533203125	(16.2s)
epoch: 27	train loss: 2702.0947265625	(16.4s)
epoch: 28	train loss: 1569.483642578125	(16.3s)
epoch: 29	train loss: 1248.1312255859375	(16.3s)
epoch: 30	train loss: 1032.1297607421875	(16.4s)
epoch: 31	train loss: 746.8577880859375	(16.4s)
epoch: 32	train loss: 614.7337036132812	(16.4s)
epoch: 33	train loss: 876.9614868164062	(16.4s)
epoch: 34	train loss: 1814.379638671875	(16.4s)
epoch: 35	train loss: 2296.394287109375	(16.3s)
epoch: 36	train loss: 2197.63720703125	(16.4s)
epoch: 37	train loss: 1487.287109375	(16.2s)
epoch: 38	train loss: 1024.061767578125	(16.3s)
epoch: 39	train loss: 784.8425903320312	(16.4s)
epoch: 40	train loss: 2973.93896484375	(16.3s)
epoch: 41	train loss: 1001.7257080078125	(16.4s)
epoch: 42	train loss: 821.0015869140625	(16.3s)
epoch: 43	train loss: 1162.7105712890625	(16.3s)
epoch: 44	train loss: 1537.1859130859375	(16.4s)
epoch: 45	train loss: 1169.28662109375	(16.3s)
epoch: 46	train loss: 854.29052734375	(16.5s)
epoch: 47	train loss: 630.122802734375	(16.4s)
epoch: 48	train loss: 922.5573120117188	(16.3s)
epoch: 49	train loss: 2127.562744140625	(16.3s)
epoch: 50	train loss: 1045.6376953125	(16.4s)
epoch: 51	train loss: 1545.3984375	(16.3s)
epoch: 52	train loss: 910.6174926757812	(16.4s)
epoch: 53	train loss: 1210.7161865234375	(16.3s)
epoch: 54	train loss: 3596.33935546875	(16.3s)
epoch: 55	train loss: 1278.2060546875	(16.4s)
epoch: 56	train loss: 513.5596313476562	(16.4s)
epoch: 57	train loss: 845.78173828125	(16.4s)
epoch: 58	train loss: 1271.357421875	(16.3s)
epoch: 59	train loss: 535.1896362304688	(16.3s)
epoch: 60	train loss: 777.6639404296875	(16.3s)
epoch: 61	train loss: 2186.483154296875	(16.3s)
epoch: 62	train loss: 881.8811645507812	(16.5s)
epoch: 63	train loss: 2553.3232421875	(16.4s)
epoch: 64	train loss: 850.8182373046875	(16.3s)
epoch: 65	train loss: 1760.5107421875	(16.4s)
epoch: 66	train loss: 1168.64208984375	(16.3s)
epoch: 67	train loss: 723.8703002929688	(16.4s)
epoch: 68	train loss: 864.2191772460938	(16.2s)
epoch: 69	train loss: 595.2509155273438	(16.2s)
epoch: 70	train loss: 622.6277465820312	(16.2s)
epoch: 71	train loss: 882.5611572265625	(16.2s)
epoch: 72	train loss: 2120.390869140625	(16.3s)
epoch: 73	train loss: 531.3592529296875	(16.2s)
epoch: 74	train loss: 910.854736328125	(16.3s)
epoch: 75	train loss: 809.0590209960938	(16.3s)
epoch: 76	train loss: 1717.0958251953125	(16.4s)
epoch: 77	train loss: 2822.566650390625	(16.3s)
epoch: 78	train loss: 1328.6390380859375	(16.4s)
epoch: 79	train loss: 433.2109375	(16.4s)
epoch: 80	train loss: 1295.0689697265625	(16.4s)
epoch: 81	train loss: 396.92529296875	(16.4s)
epoch: 82	train loss: 711.130859375	(16.3s)
epoch: 83	train loss: 1154.2718505859375	(16.5s)
epoch: 84	train loss: 1291.178955078125	(16.4s)
epoch: 85	train loss: 644.4530029296875	(16.4s)
epoch: 86	train loss: 1243.433349609375	(16.4s)
epoch: 87	train loss: 366.7381896972656	(16.3s)
epoch: 88	train loss: 350.381591796875	(16.4s)
epoch: 89	train loss: 1730.2947998046875	(16.4s)
epoch: 90	train loss: 628.0842895507812	(16.3s)
epoch: 91	train loss: 565.6773681640625	(16.4s)
epoch: 92	train loss: 1626.48828125	(16.4s)
epoch: 93	train loss: 1434.7178955078125	(16.4s)
epoch: 94	train loss: 645.3717041015625	(16.4s)
epoch: 95	train loss: 562.3309326171875	(16.4s)
epoch: 96	train loss: 840.4900512695312	(16.4s)
epoch: 97	train loss: 2934.45166015625	(16.3s)
epoch: 98	train loss: 953.38720703125	(16.3s)
epoch: 99	train loss: 486.2380065917969	(16.4s)
Evaluating model on 200 episodes
820.7833976745605
722.2371747443017
809.1490227092396
599.1091615615353
805.4513134827484
792.1452532155173
855.5017097473144
1299.9855001237656
656.8038739291104
715.3659120375111
836.5761807592291
1515.9150265284948
1065.5339012145996
844.6457946777343
849.0832378246166
718.3532321112497
949.7900594823501
752.0162795384725
676.3444475244593
765.035491104126
695.1744974772135
1030.0333278362568
1011.4234139672641
784.5365427652995
728.0748440424601
941.6880265341865
649.3051484425863
718.9750763086172
969.7250791089289
831.0587807932208
868.759228983233
693.425199508667
998.9851647890531
845.6925671895345
946.4184123277664
1119.9569562276204
569.7438451823066
724.4268716176351
712.5590886371891
614.0511104038784
700.2547937539907
926.7012414371266
881.2451650179349
771.3308962980906
834.3342644827707
928.9056875922463
1110.8547449111938
1140.5817584991455
846.6062663583194
1033.0912536439441
676.3264363606771
755.5169967651367
789.7884891600836
1349.7570788065593
860.590726852417
630.3720615931919
1128.2176750500996
814.9976425170898
761.9190763993697
819.6797609329224
874.0603078206381
734.0849843343099
717.1082786791252
602.3879234313965
883.0594300490159
683.336469196138
577.8475678933634
707.2065408070882
762.0195001874652
882.1459371505246
745.8614202646108
725.3440264109018
811.1507061719894
679.9511866251628
817.0527520430716
906.9447558720907
1074.0124337332588
747.131186167399
686.8463466041967
686.2601288604736
793.6464918631094
652.9256602106867
890.3554284131086
820.1533699035645
878.6474444347879
674.8672168519762
755.1091057459513
831.6021518111229
984.8251759665353
808.4775612063524
648.8291317332875
779.1242553393046
675.6111021908847
948.1759888221478
836.2921108245849
646.8961644326487
1158.8346852111817
862.4927447543424
1063.0124285771296
625.948313837466
1372.486069838206
622.7935141324997
1079.1992205867061
768.6830234527588
665.6363533735275
914.8597166878836
659.9538683683976
781.0759045096005
928.3174819946289
934.690936769758
1005.6793581310071
641.7503367937528
695.4226708984374
806.972342918659
800.4021195442446
920.9875410865335
1242.2403657576617
963.3119549524216
810.0054458618164
795.0775277273996
824.2871134621756
803.2501087188721
1113.4808539222267
565.1730666527382
1283.9750696818035
982.7461272019607
696.5547742341694
873.538444519043
957.6231416619343
1038.897445678711
729.2296485611887
1122.1677805582683
951.5923627614975
831.9773357391357
631.7423515319824
643.5743277176567
923.8869727452596
613.8311494966832
690.0062889099121
624.5206527709961
790.9390382766724
774.8554675692603
868.9494758405184
927.7275216475776
721.9662248948041
896.7813094212459
848.9010860443116
783.1213049752372
746.9411052068075
690.7027408480644
1011.1743315419843
1014.5140099298386
635.8690072573148
619.3485186176915
919.6106281280518
844.8081174577985
776.5636538187663
998.5548930697971
771.2837762832642
797.9976591180872
767.1824794975487
696.7633183577965
731.0010010112416
845.4206690634451
708.9303724652245
1055.0028330196035
1352.8715549815786
1157.425369143486
751.934725994534
720.6005753348855
811.0144727541053
696.4681579768658
791.7662088871002
597.8225037711007
739.6087333239042
660.8167020012351
808.7873894084584
709.9550011754036
1277.8774971008302
1057.6965692043304
1075.3242737906319
820.8577420711517
837.8118225733439
983.152451651437
727.0773544311523
703.0848629633585
927.0468730926514
753.7746205329895
766.7735910415649
935.8373578389486
555.1504305671243
599.7723006293887
719.8955323355539
759.3274087524414
Solved 194/200 episodes
808.8029648980105
Evaluated model in 33.6 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-8
Round 9
Generated trajectories in 72.3 seconds
epoch: 0	train loss: 3652.618896484375	(16.3s)
epoch: 1	train loss: 2303.566162109375	(16.3s)
epoch: 2	train loss: 1556.2066650390625	(16.5s)
epoch: 3	train loss: 1870.26806640625	(16.4s)
epoch: 4	train loss: 2764.049072265625	(16.4s)
epoch: 5	train loss: 1531.812744140625	(16.5s)
epoch: 6	train loss: 1476.679931640625	(16.3s)
epoch: 7	train loss: 1885.1324462890625	(16.4s)
epoch: 8	train loss: 1307.9183349609375	(16.5s)
epoch: 9	train loss: 1420.961669921875	(16.5s)
epoch: 10	train loss: 1593.8629150390625	(16.5s)
epoch: 11	train loss: 1731.005126953125	(16.4s)
epoch: 12	train loss: 2306.9638671875	(16.4s)
epoch: 13	train loss: 1079.0633544921875	(16.5s)
epoch: 14	train loss: 1122.34375	(16.4s)
epoch: 15	train loss: 1052.84912109375	(16.5s)
epoch: 16	train loss: 671.4685668945312	(16.3s)
epoch: 17	train loss: 1360.275634765625	(16.4s)
epoch: 18	train loss: 1667.3104248046875	(16.5s)
epoch: 19	train loss: 871.3933715820312	(16.5s)
epoch: 20	train loss: 2103.1142578125	(16.6s)
epoch: 21	train loss: 1191.3343505859375	(16.5s)
epoch: 22	train loss: 1711.8966064453125	(16.4s)
epoch: 23	train loss: 933.5244750976562	(16.5s)
epoch: 24	train loss: 2106.494384765625	(16.5s)
epoch: 25	train loss: 1024.4508056640625	(16.6s)
epoch: 26	train loss: 915.3818969726562	(16.5s)
epoch: 27	train loss: 689.5066528320312	(16.4s)
epoch: 28	train loss: 696.5549926757812	(16.5s)
epoch: 29	train loss: 1006.0479125976562	(16.5s)
epoch: 30	train loss: 3863.473876953125	(16.5s)
epoch: 31	train loss: 1061.490478515625	(16.3s)
epoch: 32	train loss: 811.2749633789062	(16.5s)
epoch: 33	train loss: 730.0059814453125	(16.4s)
epoch: 34	train loss: 850.29443359375	(16.3s)
epoch: 35	train loss: 1710.705810546875	(16.5s)
epoch: 36	train loss: 880.3038330078125	(16.5s)
epoch: 37	train loss: 687.819091796875	(16.3s)
epoch: 38	train loss: 980.4092407226562	(16.5s)
epoch: 39	train loss: 917.9274291992188	(16.5s)
epoch: 40	train loss: 1150.802001953125	(16.4s)
epoch: 41	train loss: 4052.475341796875	(16.5s)
epoch: 42	train loss: 1521.3419189453125	(16.5s)
epoch: 43	train loss: 1433.9825439453125	(16.5s)
epoch: 44	train loss: 899.23828125	(16.5s)
epoch: 45	train loss: 406.837646484375	(16.5s)
epoch: 46	train loss: 908.94482421875	(16.4s)
epoch: 47	train loss: 339.9923095703125	(16.5s)
epoch: 48	train loss: 678.6492919921875	(16.4s)
epoch: 49	train loss: 2103.974853515625	(16.5s)
epoch: 50	train loss: 928.3301391601562	(16.6s)
epoch: 51	train loss: 472.99066162109375	(16.5s)
epoch: 52	train loss: 1169.6646728515625	(16.5s)
epoch: 53	train loss: 658.6866455078125	(16.4s)
epoch: 54	train loss: 726.4014892578125	(16.5s)
epoch: 55	train loss: 600.0650634765625	(16.4s)
epoch: 56	train loss: 362.2089538574219	(16.6s)
epoch: 57	train loss: 288.6705627441406	(16.3s)
epoch: 58	train loss: 961.4541625976562	(16.4s)
epoch: 59	train loss: 3283.1767578125	(16.5s)
epoch: 60	train loss: 904.7235717773438	(16.3s)
epoch: 61	train loss: 972.8946533203125	(16.4s)
epoch: 62	train loss: 791.2208251953125	(16.3s)
epoch: 63	train loss: 387.5982360839844	(16.3s)
epoch: 64	train loss: 1127.21533203125	(16.5s)
epoch: 65	train loss: 3161.101318359375	(16.3s)
epoch: 66	train loss: 481.23736572265625	(16.3s)
epoch: 67	train loss: 492.8370361328125	(16.4s)
epoch: 68	train loss: 576.0725708007812	(16.3s)
epoch: 69	train loss: 324.09686279296875	(16.3s)
epoch: 70	train loss: 196.11712646484375	(16.4s)
epoch: 71	train loss: 1133.82080078125	(16.4s)
epoch: 72	train loss: 452.6706848144531	(16.3s)
epoch: 73	train loss: 1128.320068359375	(16.4s)
epoch: 74	train loss: 1804.63623046875	(16.3s)
epoch: 75	train loss: 444.4377136230469	(16.5s)
epoch: 76	train loss: 324.6712341308594	(16.5s)
epoch: 77	train loss: 1233.166015625	(16.5s)
epoch: 78	train loss: 356.65069580078125	(16.4s)
epoch: 79	train loss: 500.0822448730469	(16.5s)
epoch: 80	train loss: 1498.3668212890625	(16.4s)
epoch: 81	train loss: 1357.1070556640625	(16.3s)
epoch: 82	train loss: 621.007568359375	(16.6s)
epoch: 83	train loss: 1145.9141845703125	(16.3s)
epoch: 84	train loss: 269.29833984375	(16.5s)
epoch: 85	train loss: 663.3781127929688	(16.5s)
epoch: 86	train loss: 466.87603759765625	(16.5s)
epoch: 87	train loss: 2036.304443359375	(16.5s)
epoch: 88	train loss: 2013.961181640625	(16.4s)
epoch: 89	train loss: 999.861572265625	(16.5s)
epoch: 90	train loss: 763.030517578125	(16.6s)
epoch: 91	train loss: 1111.9642333984375	(16.5s)
epoch: 92	train loss: 1192.317626953125	(16.4s)
epoch: 93	train loss: 584.665771484375	(16.5s)
epoch: 94	train loss: 2741.492919921875	(16.4s)
epoch: 95	train loss: 590.9265747070312	(16.5s)
epoch: 96	train loss: 390.189453125	(16.4s)
epoch: 97	train loss: 379.5222473144531	(16.5s)
epoch: 98	train loss: 500.1850280761719	(16.6s)
epoch: 99	train loss: 586.29296875	(16.4s)
Evaluating model on 200 episodes
1689.537071062171
1216.9085614340645
1285.7024601300557
1418.7155641828265
1226.5541613896687
1797.60169506073
1120.2155362447104
1125.02712579207
956.5283887762772
1463.0250511169434
1677.7664285806509
1014.9068024044946
1069.5980703989665
849.9502482144338
1067.3525444153815
875.641783770393
1264.9982931518555
1194.3950350443522
1014.1405611038208
1386.3244659717266
1567.4378226143974
1503.1929957866669
1540.5225202178956
1113.1429339090982
969.1807904127168
1213.2722651163738
861.7273310706729
2080.1115674972534
2749.4124139149985
1067.487486521403
1032.0531295250203
928.191707611084
1179.4988892499139
1003.213910647801
1127.4592618306478
1822.1135004758835
1149.616873227633
1231.83082372492
1254.1297969818115
1072.895702336964
904.8009017308553
1893.307822091239
1147.919177862314
1381.466973473044
2012.2393215403838
925.8283642855557
1257.385412656344
1046.6005941118513
1343.200643234253
1310.8513135043058
1250.8198155489836
955.8322268093334
1149.421173477173
1188.8572810173034
1427.6402256375268
1025.411113243103
910.3995535108778
1199.281242239064
1386.36228426765
1169.5710713313176
1095.7669246897979
1070.3920442793105
1505.5866971449418
1079.5080250471067
1085.72598550486
1510.809842376709
1264.5804617745537
1256.1602404394814
1147.5935417978387
1157.8288450974685
1141.311744443832
1174.1088523864746
1121.6305190866644
884.2621410511158
1298.1684390918629
1327.6113855300412
1300.2626398631505
861.9993403378655
1083.1909503936768
784.7232040058483
1208.5726585388184
1100.8083010221783
1312.2745608767948
1128.54022248586
1039.792608772836
1152.0669198036194
1154.268765449524
1394.178286075592
930.2502533140637
1264.6697425842285
903.7697434858842
1229.2288428858708
1009.3958666059706
1738.4931190490722
1640.6442346572876
1001.5240645635696
1029.9758540066805
1984.9304666519165
801.5695829649229
1201.336495399475
1340.5150416237968
1721.8022513919407
1162.6993446350098
1030.188864601983
1330.0846760489724
922.9862546466645
941.1509869180877
1576.7794879566538
997.4937166486468
1020.974836696278
918.9767867640445
1366.504269104004
1097.364465264713
1473.1296276901708
1104.7989627838135
1152.2387216356065
1611.8317193424
1391.5310608256948
859.872994258486
1022.3677151997884
1360.4310089874268
965.5944466767488
988.3856431765435
1395.091203212738
1264.4690552711486
1143.112468025901
1156.729095697403
1030.4820173354376
1602.744738006592
1181.236710389455
1253.2135586738586
1242.1260190261037
976.0555034819104
1259.467308892144
823.327370779855
1105.9364829744611
948.4639042821424
1097.2634076085583
1254.7275019425613
919.6385292425389
1063.2949042320251
1559.025250715368
939.9928218126297
900.4883801309686
844.4436628442062
1073.1977509975434
1060.8807512283324
1229.9553711989831
657.0651246584379
1148.8226627550628
988.0376305171421
1156.94822064568
1301.2071975708009
1097.4978078378213
1262.9459303942594
1520.3398068076685
1671.1217997074127
1087.7413185119628
1286.2754780618768
1349.6163660196157
1276.086812626232
1125.6685566535364
2189.8690990448
1023.066724068778
1407.4269332885742
1117.558729848554
1577.918416765001
1072.2593189038728
1247.4350219302708
1132.178589608934
1134.5416552894994
1491.7977358500164
1079.083450453622
1224.0934248299434
1090.9225519353693
754.8426856994629
1476.7155258178711
1855.7969856262207
1229.7175815643802
1167.0239617483955
1271.4554762268067
985.9609502156576
1206.1775273409758
1188.2121883573986
848.583180254156
1174.1496346417596
1286.6795843941825
1513.5703265850361
1094.5510284836228
736.9083267660702
1098.629390321929
1458.9947572435651
1031.1231390049584
1303.4268358866373
1488.4974899291992
Solved 195/200 episodes
1185.088500438007
Evaluated model in 33.7 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-9
Round 10
Generated trajectories in 72.4 seconds
epoch: 0	train loss: 2822.7080078125	(16.2s)
epoch: 1	train loss: 1757.93994140625	(16.2s)
epoch: 2	train loss: 1879.63818359375	(16.3s)
epoch: 3	train loss: 1374.537353515625	(16.4s)
epoch: 4	train loss: 1669.7847900390625	(16.4s)
epoch: 5	train loss: 2019.2169189453125	(16.3s)
epoch: 6	train loss: 1016.8613891601562	(16.4s)
epoch: 7	train loss: 2053.178466796875	(16.4s)
epoch: 8	train loss: 1616.8179931640625	(16.4s)
epoch: 9	train loss: 1354.3779296875	(16.4s)
epoch: 10	train loss: 628.775390625	(16.4s)
epoch: 11	train loss: 609.2862548828125	(16.4s)
epoch: 12	train loss: 1757.6317138671875	(16.4s)
epoch: 13	train loss: 1753.9482421875	(16.3s)
epoch: 14	train loss: 1695.2664794921875	(16.4s)
epoch: 15	train loss: 1262.3204345703125	(16.3s)
epoch: 16	train loss: 981.0084228515625	(16.4s)
epoch: 17	train loss: 1740.0418701171875	(16.4s)
epoch: 18	train loss: 1198.49853515625	(16.3s)
epoch: 19	train loss: 1473.72216796875	(16.4s)
epoch: 20	train loss: 518.0614013671875	(16.3s)
epoch: 21	train loss: 476.4581298828125	(16.3s)
epoch: 22	train loss: 1324.8013916015625	(16.4s)
epoch: 23	train loss: 726.6910400390625	(16.3s)
epoch: 24	train loss: 538.0411376953125	(16.4s)
epoch: 25	train loss: 1089.35400390625	(16.3s)
epoch: 26	train loss: 713.5408325195312	(16.3s)
epoch: 27	train loss: 571.5614013671875	(16.4s)
epoch: 28	train loss: 1031.517333984375	(16.3s)
epoch: 29	train loss: 1453.08154296875	(16.4s)
epoch: 30	train loss: 1577.259765625	(16.3s)
epoch: 31	train loss: 1645.0172119140625	(16.3s)
epoch: 32	train loss: 509.5236511230469	(16.4s)
epoch: 33	train loss: 619.5240478515625	(16.3s)
epoch: 34	train loss: 1345.6214599609375	(16.4s)
epoch: 35	train loss: 1235.0982666015625	(16.3s)
epoch: 36	train loss: 625.15380859375	(16.3s)
epoch: 37	train loss: 1454.1436767578125	(16.4s)
epoch: 38	train loss: 1075.5660400390625	(16.4s)
epoch: 39	train loss: 675.6272583007812	(16.4s)
epoch: 40	train loss: 1545.7301025390625	(16.4s)
epoch: 41	train loss: 454.2454528808594	(16.3s)
epoch: 42	train loss: 2536.9560546875	(16.4s)
epoch: 43	train loss: 710.3236694335938	(16.4s)
epoch: 44	train loss: 795.6640625	(16.4s)
epoch: 45	train loss: 442.795166015625	(16.4s)
epoch: 46	train loss: 534.9556884765625	(16.3s)
epoch: 47	train loss: 1281.3109130859375	(16.3s)
epoch: 48	train loss: 1323.010498046875	(16.4s)
epoch: 49	train loss: 1541.72705078125	(16.4s)
epoch: 50	train loss: 1113.949951171875	(16.4s)
epoch: 51	train loss: 605.5053100585938	(16.3s)
epoch: 52	train loss: 310.1123962402344	(16.4s)
epoch: 53	train loss: 239.67213439941406	(16.4s)
epoch: 54	train loss: 851.885986328125	(16.4s)
epoch: 55	train loss: 550.4974365234375	(16.4s)
epoch: 56	train loss: 516.5980834960938	(16.3s)
epoch: 57	train loss: 4502.95703125	(16.3s)
epoch: 58	train loss: 494.1007385253906	(16.4s)
epoch: 59	train loss: 357.6360168457031	(16.3s)
epoch: 60	train loss: 2011.6971435546875	(16.4s)
epoch: 61	train loss: 595.605712890625	(16.3s)
epoch: 62	train loss: 633.6646728515625	(16.4s)
epoch: 63	train loss: 180.9634552001953	(16.3s)
epoch: 64	train loss: 228.9529571533203	(16.3s)
epoch: 65	train loss: 218.73248291015625	(16.3s)
epoch: 66	train loss: 666.2907104492188	(16.2s)
epoch: 67	train loss: 633.3485717773438	(16.2s)
epoch: 68	train loss: 304.18975830078125	(16.2s)
epoch: 69	train loss: 1420.3134765625	(16.2s)
epoch: 70	train loss: 975.8615112304688	(16.3s)
epoch: 71	train loss: 852.6268920898438	(16.3s)
epoch: 72	train loss: 1238.1810302734375	(16.3s)
epoch: 73	train loss: 1371.054931640625	(16.4s)
epoch: 74	train loss: 779.4107666015625	(16.4s)
epoch: 75	train loss: 596.991943359375	(16.4s)
epoch: 76	train loss: 511.65765380859375	(16.4s)
epoch: 77	train loss: 748.361083984375	(16.3s)
epoch: 78	train loss: 609.8570556640625	(16.3s)
epoch: 79	train loss: 231.60667419433594	(16.4s)
epoch: 80	train loss: 425.6528625488281	(16.4s)
epoch: 81	train loss: 528.2359008789062	(16.4s)
epoch: 82	train loss: 529.131103515625	(16.3s)
epoch: 83	train loss: 780.0288696289062	(16.3s)
epoch: 84	train loss: 1217.9830322265625	(16.4s)
epoch: 85	train loss: 2485.11279296875	(16.3s)
epoch: 86	train loss: 1040.83984375	(16.4s)
epoch: 87	train loss: 896.9418334960938	(16.3s)
epoch: 88	train loss: 457.4422302246094	(16.3s)
epoch: 89	train loss: 269.965576171875	(16.4s)
epoch: 90	train loss: 618.2218017578125	(16.3s)
epoch: 91	train loss: 616.5396728515625	(16.4s)
epoch: 92	train loss: 364.8471374511719	(16.3s)
epoch: 93	train loss: 1414.5965576171875	(16.3s)
epoch: 94	train loss: 1058.56591796875	(16.4s)
epoch: 95	train loss: 1186.4017333984375	(16.3s)
epoch: 96	train loss: 360.1902770996094	(16.4s)
epoch: 97	train loss: 506.0469055175781	(16.3s)
epoch: 98	train loss: 685.8848876953125	(16.3s)
epoch: 99	train loss: 1849.444091796875	(16.3s)
Evaluating model on 200 episodes
911.4515397813585
594.435438973563
751.5290460297556
945.1213108825683
479.08268148248845
961.0222509384155
486.4822402954102
685.6376501230093
710.5941993925306
783.1962990760803
914.5178193788271
618.7805371814304
675.4044721503006
425.61470256132236
562.8532320022583
725.4683924039205
1096.6152198314667
762.1223735151619
619.882435798645
542.3599773553701
814.0904108683268
498.54595268473906
501.3756160175099
721.905884152367
868.6050420337253
736.6447783231736
822.3593258104826
880.7118058204651
785.7426941990852
780.8327496846517
657.9088491439819
648.1477658748627
774.0185521443685
693.606333732605
766.4738523713473
580.0749374158455
412.56777470906576
970.622343381246
745.3089229038784
966.7571595798839
910.0484943543711
812.0014551162719
621.2012283100802
781.124135390572
683.1853499412537
966.8183151245117
750.4237084224306
918.0401445116315
876.6419904232025
585.0706077477871
865.0509948115194
1050.5450652273078
978.9237410575151
895.9202415347099
634.431463914759
623.8536881483518
790.1056512423924
885.326152454723
846.3332691192627
1280.6883543332417
892.4210306693768
526.1681104977926
1045.8539791901906
1094.519843843248
952.7213945822282
792.6278581619263
728.8132684707641
826.8430992762247
552.3726281325022
527.6943412780762
629.3126357793808
840.0922034719716
612.4759514218285
1281.7842247486115
953.3795619328816
1219.895021533966
561.6623298810875
1065.3537096977234
1029.045481999715
818.7391147613525
622.0616277819095
860.2674768235948
759.6427445709705
1068.7694096565247
1250.022123336792
606.7222972196691
771.5255635579427
573.3928991953532
761.2690979003906
862.8145696415621
583.0984797226755
750.2197551312654
758.4486882209778
763.3999072180854
787.4058350457085
921.2644464969635
611.7977270285288
790.8862233884407
773.906016667684
1132.6309949874878
547.0306330786811
796.1061514340914
879.7486414591472
773.2564582824707
647.7415968821599
788.3888540267944
728.7784507149145
1077.7553190503802
666.4919762015343
641.2610366457984
792.5833827018738
756.0221629738808
752.1340774297714
572.4283681123153
897.3787539763885
785.9569780125338
701.7607150222316
622.104355948312
960.3826991893627
702.2286006558326
652.4889836629231
788.7173299789429
637.5098094940186
726.3332489132881
787.3728723526001
514.6643845694406
594.8984404427665
1006.6786414782206
811.5795909464359
677.2084785085735
742.747685289383
643.7938957214355
759.6068294424759
502.06551801241363
654.738332939148
844.3429214336254
696.6897433388908
583.9971680111355
713.9369097421336
739.3243368905166
567.9027396492336
898.956386645635
431.67495453357697
1191.7424011230469
524.2744779586792
745.7945238749186
675.5183744227633
802.7187888046791
663.5852707454136
927.1779050827026
795.2708702087402
580.2083251953125
879.7694331981518
630.9288827822759
837.5186876999704
805.5227065086365
1022.1464622497558
621.7137459464695
760.2313653564453
486.37760025024414
1004.872865041097
1016.9302713730756
611.9825108721852
694.342036655971
604.3843393325806
990.9183713127585
576.3774147431055
586.4116624804104
727.3310750325521
710.6335302168324
782.8460708581484
833.3475867680141
720.6442944662912
696.7861425948865
629.6103569030762
786.8644105820429
670.0539616804856
640.379513539766
565.3336993507717
1145.6914372695119
634.4102534702846
731.0903782564051
644.4890190230476
781.7955457369486
732.8024893540603
898.2720542320839
470.45308734524633
700.2986137866974
827.5356168332307
649.9613970120748
534.5494008931247
612.338814523485
690.4167694515652
815.0997098922729
Solved 194/200 episodes
736.6602561510117
Evaluated model in 32.4 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-10
Round 11
Generated trajectories in 72.0 seconds
epoch: 0	train loss: 2421.82470703125	(16.3s)
epoch: 1	train loss: 1618.8514404296875	(16.3s)
epoch: 2	train loss: 1387.7586669921875	(16.3s)
epoch: 3	train loss: 1244.7530517578125	(16.4s)
epoch: 4	train loss: 1205.62158203125	(16.5s)
epoch: 5	train loss: 919.58154296875	(16.4s)
epoch: 6	train loss: 1955.8447265625	(16.4s)
epoch: 7	train loss: 1310.6607666015625	(16.5s)
epoch: 8	train loss: 1575.1033935546875	(16.4s)
epoch: 9	train loss: 1078.989990234375	(16.4s)
epoch: 10	train loss: 1009.7677612304688	(16.4s)
epoch: 11	train loss: 1312.603759765625	(16.4s)
epoch: 12	train loss: 1264.8822021484375	(16.5s)
epoch: 13	train loss: 1099.7137451171875	(16.4s)
epoch: 14	train loss: 829.2894287109375	(16.4s)
epoch: 15	train loss: 1048.716064453125	(16.4s)
epoch: 16	train loss: 636.372314453125	(16.4s)
epoch: 17	train loss: 1139.4500732421875	(16.5s)
epoch: 18	train loss: 1078.3681640625	(16.4s)
epoch: 19	train loss: 683.9332885742188	(16.4s)
epoch: 20	train loss: 826.1867065429688	(16.4s)
epoch: 21	train loss: 1143.262451171875	(16.4s)
epoch: 22	train loss: 454.7183837890625	(16.5s)
epoch: 23	train loss: 1262.3739013671875	(16.4s)
epoch: 24	train loss: 997.40673828125	(16.4s)
epoch: 25	train loss: 1545.799560546875	(16.4s)
epoch: 26	train loss: 996.3709106445312	(16.4s)
epoch: 27	train loss: 864.3255004882812	(16.4s)
epoch: 28	train loss: 825.9680786132812	(16.4s)
epoch: 29	train loss: 540.8106079101562	(16.4s)
epoch: 30	train loss: 6739.0869140625	(16.4s)
epoch: 31	train loss: 763.8582763671875	(16.4s)
epoch: 32	train loss: 1046.5924072265625	(16.4s)
epoch: 33	train loss: 912.3244018554688	(16.4s)
epoch: 34	train loss: 465.37884521484375	(16.4s)
epoch: 35	train loss: 247.58322143554688	(16.4s)
epoch: 36	train loss: 548.4344482421875	(16.4s)
epoch: 37	train loss: 304.9884948730469	(16.4s)
epoch: 38	train loss: 312.4039611816406	(16.5s)
epoch: 39	train loss: 2567.24658203125	(16.4s)
epoch: 40	train loss: 956.5519409179688	(16.4s)
epoch: 41	train loss: 831.9808959960938	(16.4s)
epoch: 42	train loss: 360.6065979003906	(16.4s)
epoch: 43	train loss: 337.20635986328125	(16.5s)
epoch: 44	train loss: 685.6536254882812	(16.4s)
epoch: 45	train loss: 611.4446411132812	(16.4s)
epoch: 46	train loss: 995.6507568359375	(16.4s)
epoch: 47	train loss: 952.3169555664062	(16.4s)
epoch: 48	train loss: 533.177490234375	(16.5s)
epoch: 49	train loss: 189.71446228027344	(16.4s)
epoch: 50	train loss: 751.3432006835938	(16.4s)
epoch: 51	train loss: 979.35205078125	(16.4s)
epoch: 52	train loss: 376.99395751953125	(16.4s)
epoch: 53	train loss: 1284.239990234375	(16.5s)
epoch: 54	train loss: 933.6444091796875	(16.4s)
epoch: 55	train loss: 435.7017822265625	(16.4s)
epoch: 56	train loss: 2106.770751953125	(16.4s)
epoch: 57	train loss: 501.6163635253906	(16.4s)
epoch: 58	train loss: 1206.436279296875	(16.5s)
epoch: 59	train loss: 1110.6461181640625	(16.4s)
epoch: 60	train loss: 534.4324340820312	(16.4s)
epoch: 61	train loss: 368.5141296386719	(16.4s)
epoch: 62	train loss: 496.22955322265625	(16.4s)
epoch: 63	train loss: 367.9527893066406	(16.4s)
epoch: 64	train loss: 381.37725830078125	(16.3s)
epoch: 65	train loss: 597.5220336914062	(16.3s)
epoch: 66	train loss: 1414.0057373046875	(16.3s)
epoch: 67	train loss: 668.9782104492188	(16.3s)
epoch: 68	train loss: 255.59535217285156	(16.3s)
epoch: 69	train loss: 548.6790771484375	(16.4s)
epoch: 70	train loss: 842.125732421875	(16.3s)
epoch: 71	train loss: 359.4578552246094	(16.3s)
epoch: 72	train loss: 1558.859130859375	(16.4s)
epoch: 73	train loss: 1468.3251953125	(16.5s)
epoch: 74	train loss: 406.3507995605469	(16.5s)
epoch: 75	train loss: 172.4764862060547	(16.4s)
epoch: 76	train loss: 164.35040283203125	(16.4s)
epoch: 77	train loss: 763.896484375	(16.4s)
epoch: 78	train loss: 738.634765625	(16.4s)
epoch: 79	train loss: 630.2327880859375	(16.5s)
epoch: 80	train loss: 238.01177978515625	(16.4s)
epoch: 81	train loss: 417.33624267578125	(16.4s)
epoch: 82	train loss: 597.1802978515625	(16.4s)
epoch: 83	train loss: 671.7605590820312	(16.4s)
epoch: 84	train loss: 338.27880859375	(16.5s)
epoch: 85	train loss: 1397.252197265625	(16.4s)
epoch: 86	train loss: 1026.9500732421875	(16.4s)
epoch: 87	train loss: 480.465576171875	(16.4s)
epoch: 88	train loss: 175.5507049560547	(16.4s)
epoch: 89	train loss: 1423.0975341796875	(16.5s)
epoch: 90	train loss: 227.06365966796875	(16.4s)
epoch: 91	train loss: 628.1309814453125	(16.4s)
epoch: 92	train loss: 1693.4891357421875	(16.4s)
epoch: 93	train loss: 583.7627563476562	(16.4s)
epoch: 94	train loss: 624.938720703125	(16.5s)
epoch: 95	train loss: 1114.2802734375	(16.4s)
epoch: 96	train loss: 260.8863220214844	(16.4s)
epoch: 97	train loss: 318.29376220703125	(16.4s)
epoch: 98	train loss: 392.3397216796875	(16.4s)
epoch: 99	train loss: 718.1729736328125	(16.4s)
Evaluating model on 200 episodes
1441.5168342590332
1540.5379378000896
980.458271408081
2065.459330191979
1357.3456618850296
2321.315698623657
1900.5256523132325
1196.1767143408458
2644.494782584054
1368.8906664621263
1585.2881842526522
1294.1269205266778
1740.228572456925
2321.179364013672
1953.0354909029875
1834.43564726177
2501.6845058600106
1487.320857481523
1591.6616764068604
1655.4436217087966
1245.7600666926458
1184.5431650924684
1352.7619674682617
1859.6667315165203
1736.785491095649
1526.912387251854
1383.2353407541912
1704.9855060577393
1722.1985951832362
1984.612102127075
2082.310231555592
1570.957381508567
1686.5371952056885
1430.888065916119
1408.0340218283914
1121.9247458775837
1615.3046552484686
1398.0219895680746
1950.654984987699
1502.808992266655
1600.3204775101099
1779.1064163843791
1381.6808714866638
1803.4545264067474
1532.2368754184608
2384.3920050534334
1518.7212916782923
1535.3604750382274
1936.5049381256104
1614.2228889031844
1811.1626194545202
1743.074400621302
2029.192355292184
1615.2513619995118
1710.6869069417319
1978.1555567847358
1660.2052015577044
2168.866878691174
2168.051659901937
1508.2345580101014
1821.9593088626862
1156.0109879777237
1821.8307757907444
1504.5459692213271
1446.613552910941
2043.7989552815754
1177.6697273254395
1828.0526154258034
1980.8313988579644
1411.4272420974005
1339.7234802246094
1471.4859511057537
1166.1003217697144
1677.4296067844737
1289.5415622166224
1417.6963760739281
1191.9627135781682
1881.776634462418
1624.8047218322754
1528.2536249160767
1443.1326319376628
1886.5685961106244
1254.780293499982
1757.1900810514178
1722.4259897729626
1462.4677658081055
1678.4416914547191
1575.6102953459088
1140.6127365599286
1985.1240368343535
1969.0153798703793
1361.134696434284
1312.9366862773895
1392.8054994653774
1675.0815185546876
1932.4760789871216
1775.2050635814667
1635.7566791225124
1348.2840367635092
1984.906602859497
1053.5715247562953
1492.2408323287964
2135.132843017578
1254.538706653523
1216.8861130424168
1669.2415924980528
1628.1979894638062
1079.1453040627873
1486.8794555664062
1991.3412728029139
1214.592097195712
1406.046058177948
1410.3044118881226
1106.6133289994864
1471.4298894781816
2106.094919204712
1032.936070175171
1478.674872471736
1129.9296827894268
1284.7269536124336
1829.1610232866728
1677.8399179458618
1484.4135225084092
1500.708792114258
2261.0925886447612
1339.6427641165883
1484.653802871704
1245.9577538626534
1672.2749376296997
1603.3306884765625
1952.5083100168329
1852.3312605222065
1514.5284127628101
1797.613270220549
1911.032262999436
1083.6664001464844
2131.737420425415
1500.713965733846
1499.1577532632011
1856.0274646065452
1517.1918216828376
1529.7793599909003
1527.1983960095574
1648.6224822998047
2187.437213352748
1621.6544922616747
923.4153207632212
1493.5700183868407
1510.3185386657715
1566.5851950872511
1370.5311670610981
1194.2989807128906
1691.4748808315821
1355.9905906004064
1233.2003571987152
1567.6972088019054
1868.126278338225
1650.044069893506
1532.6795702541576
1833.1737704277039
1287.6592082977295
2655.3702293395995
1459.1388259789883
1939.1066579818726
1874.4849063988888
2326.488714090983
1476.9774800176206
2100.042423472685
1694.5381898880005
1415.2882110595704
1793.982616793725
1317.2186370618415
1634.484797414144
1319.0310859680176
1323.5313266631097
2593.828420465643
825.5249303929946
1630.194144376119
1460.570685820146
2277.0962924957275
1697.6060071309407
1488.2960074288505
1682.810883690329
1375.93161378588
1346.4360224405925
1539.740683555603
1526.6083586215973
1831.0502020517986
1466.4085304953835
1603.4076809201922
1856.9340707879317
697.7830177307129
2207.9387917952104
1488.2044035388578
1060.7884022639348
Solved 195/200 episodes
1573.7439628169293
Evaluated model in 34.8 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-11
Round 12
Generated trajectories in 73.5 seconds
epoch: 0	train loss: 2070.56298828125	(16.4s)
epoch: 1	train loss: 1224.2933349609375	(16.3s)
epoch: 2	train loss: 1507.7196044921875	(16.5s)
epoch: 3	train loss: 966.1673583984375	(16.5s)
epoch: 4	train loss: 826.200927734375	(16.5s)
epoch: 5	train loss: 674.9068603515625	(16.6s)
epoch: 6	train loss: 2716.839599609375	(16.5s)
epoch: 7	train loss: 894.1853637695312	(16.5s)
epoch: 8	train loss: 440.5596618652344	(16.6s)
epoch: 9	train loss: 694.8648681640625	(16.5s)
epoch: 10	train loss: 1442.1807861328125	(16.6s)
epoch: 11	train loss: 1073.3292236328125	(16.5s)
epoch: 12	train loss: 712.155029296875	(16.5s)
epoch: 13	train loss: 598.1712036132812	(16.5s)
epoch: 14	train loss: 504.5830078125	(16.5s)
epoch: 15	train loss: 1475.1116943359375	(16.6s)
epoch: 16	train loss: 1524.0589599609375	(16.5s)
epoch: 17	train loss: 689.9335327148438	(16.5s)
epoch: 18	train loss: 383.5915832519531	(16.5s)
epoch: 19	train loss: 901.032958984375	(16.5s)
epoch: 20	train loss: 899.9348754882812	(16.6s)
epoch: 21	train loss: 680.7678833007812	(16.5s)
epoch: 22	train loss: 1012.0247802734375	(16.5s)
epoch: 23	train loss: 2149.55712890625	(16.5s)
epoch: 24	train loss: 469.6421203613281	(16.5s)
epoch: 25	train loss: 593.361083984375	(16.5s)
epoch: 26	train loss: 574.0209350585938	(16.5s)
epoch: 27	train loss: 232.8701934814453	(16.5s)
epoch: 28	train loss: 310.3438720703125	(16.5s)
epoch: 29	train loss: 467.8050231933594	(16.5s)
epoch: 30	train loss: 899.6189575195312	(16.5s)
epoch: 31	train loss: 1578.0870361328125	(16.6s)
epoch: 32	train loss: 780.2233276367188	(16.5s)
epoch: 33	train loss: 1909.3828125	(16.5s)
epoch: 34	train loss: 440.7956848144531	(16.5s)
epoch: 35	train loss: 943.89794921875	(16.5s)
epoch: 36	train loss: 608.323486328125	(16.6s)
epoch: 37	train loss: 508.7809143066406	(16.5s)
epoch: 38	train loss: 421.7119140625	(16.5s)
epoch: 39	train loss: 921.0045776367188	(16.5s)
epoch: 40	train loss: 335.41326904296875	(16.5s)
epoch: 41	train loss: 357.9340515136719	(16.6s)
epoch: 42	train loss: 2666.704833984375	(16.5s)
epoch: 43	train loss: 1169.27392578125	(16.5s)
epoch: 44	train loss: 700.6896362304688	(16.5s)
epoch: 45	train loss: 358.32745361328125	(16.5s)
epoch: 46	train loss: 397.78472900390625	(16.6s)
epoch: 47	train loss: 922.4610595703125	(16.5s)
epoch: 48	train loss: 740.81494140625	(16.5s)
epoch: 49	train loss: 1292.730712890625	(16.5s)
epoch: 50	train loss: 269.52984619140625	(16.6s)
epoch: 51	train loss: 359.5214538574219	(16.5s)
epoch: 52	train loss: 271.476318359375	(16.5s)
epoch: 53	train loss: 439.9788513183594	(16.5s)
epoch: 54	train loss: 202.85923767089844	(16.5s)
epoch: 55	train loss: 248.2625274658203	(16.5s)
epoch: 56	train loss: 603.9091796875	(16.5s)
epoch: 57	train loss: 888.7647705078125	(16.6s)
epoch: 58	train loss: 253.53160095214844	(16.5s)
epoch: 59	train loss: 906.6067504882812	(16.5s)
epoch: 60	train loss: 1137.1649169921875	(16.5s)
epoch: 61	train loss: 699.689697265625	(16.5s)
epoch: 62	train loss: 2062.950439453125	(16.5s)
epoch: 63	train loss: 791.693115234375	(16.4s)
epoch: 64	train loss: 355.3089904785156	(16.4s)
epoch: 65	train loss: 1050.452392578125	(16.4s)
epoch: 66	train loss: 320.143310546875	(16.3s)
epoch: 67	train loss: 414.405517578125	(16.5s)
epoch: 68	train loss: 155.9091033935547	(16.4s)
epoch: 69	train loss: 662.2982788085938	(16.4s)
epoch: 70	train loss: 260.662841796875	(16.4s)
epoch: 71	train loss: 795.900390625	(16.5s)
epoch: 72	train loss: 1022.0671997070312	(16.6s)
epoch: 73	train loss: 401.1457214355469	(16.5s)
epoch: 74	train loss: 129.65707397460938	(16.5s)
epoch: 75	train loss: 131.10946655273438	(16.5s)
epoch: 76	train loss: 1020.2152709960938	(16.5s)
epoch: 77	train loss: 996.0487060546875	(16.5s)
epoch: 78	train loss: 959.7319946289062	(16.6s)
epoch: 79	train loss: 221.94224548339844	(16.5s)
epoch: 80	train loss: 657.6593627929688	(16.5s)
epoch: 81	train loss: 589.4722290039062	(16.5s)
epoch: 82	train loss: 141.27285766601562	(16.5s)
epoch: 83	train loss: 571.2987060546875	(16.6s)
epoch: 84	train loss: 391.8808288574219	(16.5s)
epoch: 85	train loss: 229.43124389648438	(16.5s)
epoch: 86	train loss: 2024.47314453125	(16.5s)
epoch: 87	train loss: 879.9915161132812	(16.5s)
epoch: 88	train loss: 406.43829345703125	(16.6s)
epoch: 89	train loss: 310.4395751953125	(16.5s)
epoch: 90	train loss: 526.9674682617188	(16.5s)
epoch: 91	train loss: 543.015869140625	(16.5s)
epoch: 92	train loss: 208.3325653076172	(16.5s)
epoch: 93	train loss: 761.720703125	(16.6s)
epoch: 94	train loss: 841.42822265625	(16.5s)
epoch: 95	train loss: 1764.5264892578125	(16.5s)
epoch: 96	train loss: 1195.300537109375	(16.5s)
epoch: 97	train loss: 350.2414245605469	(16.5s)
epoch: 98	train loss: 195.01332092285156	(16.5s)
epoch: 99	train loss: 356.3233337402344	(16.5s)
Evaluating model on 200 episodes
3275.028349789706
1834.6853014264789
2629.23936340332
2039.3927327692509
2992.4920124477812
2804.4484622839727
2388.098650061566
2012.4377973647345
2009.7367737293243
1258.0526469548543
3023.9090960025787
2405.857331412179
3429.329073376126
2374.9598224524298
2223.2362872229683
2551.819292280409
2276.817697252546
2192.758555730184
2294.2319585014793
2275.118283730966
1846.5671354594983
2182.6845121383667
2231.3001908801853
2252.8044424057007
2470.5015621185303
5680.3366775512695
2103.3927518671208
2818.133567244918
2482.3916066884995
2666.4378814697266
2749.550357525165
2024.838564130995
2478.033072440855
3074.2613472938538
2603.095239215427
2408.2166420509075
2723.219146141639
1574.1816936674572
2042.957689111883
2560.9006609916687
2455.334004674639
2540.878612365723
2363.229804780748
2761.0239601135254
1897.2651427057053
2596.5417817433677
2203.5157835218643
3264.2394549629903
2666.116741725377
2722.2411946835723
2198.730890819005
3036.554588518645
2151.353993733724
1848.7742701939173
1500.5909957885742
2505.6288639068603
2797.452851295471
2247.3810693064042
2091.491496743827
2157.6486168938713
2485.8635619367874
3125.4240940093996
2619.634933948517
1749.2340471903483
2111.6337566375732
2353.7253456115723
2541.0537289706144
2509.734865427017
2679.096573022696
1650.0670280456543
1976.27630666097
2620.1827987829843
1705.382997194926
2892.6044962565106
2456.5869952931125
2268.338891809637
3279.2995933532716
3612.7999452289782
2021.5229717053865
1597.2589117685955
2769.7239151000977
1808.0821627239848
2838.372095017206
1559.1915469603105
2572.7294863891602
3754.3972034454346
1789.414960861206
2390.7118381261826
2532.2265073988174
3923.4428777240573
2771.4725942611694
2371.339324668602
2568.7109580204406
2784.9450801502576
1710.8670847357773
2627.510027058919
2461.075209094632
2533.4230596923826
1928.8470820765342
5483.926047643025
2624.7169101021505
2236.8136196136475
2203.285224125303
2908.7798991733125
1927.2636444694117
3132.266033411026
2830.1491821513455
2750.3274478912354
1652.5420656476701
2957.7047348889437
2138.564529800415
1706.119981765747
2297.2058404684067
2548.020286814372
1916.2509921904534
3484.768904509368
2707.545408194406
3937.407383918762
2589.9311180114746
2675.5315389279967
1617.4016468593052
2952.417361831665
2464.810579607564
2002.637274631234
3235.544092314584
2230.0187720192803
1889.6096438239601
3487.859534672328
2198.294355392456
2547.0253457289477
2685.0112470429517
2106.1655462053086
4458.148274739583
2067.67442333337
3293.832947625054
2481.3399158824573
1340.941165630634
2169.7514679431915
2601.6867546081544
3278.450247192383
1340.249091204475
2035.8007604735237
3566.0981791360036
2566.4386785888673
2813.570319890976
3091.050954182943
2326.6483896891277
3355.5903615315756
1986.9762442906697
3791.533341217041
3897.7683495741626
2653.2043592279606
1609.9264159483068
2539.759671158261
2642.6146553039553
2648.9589798538773
2532.8008134776146
2113.0673226443205
2101.745840072632
2575.9014235602485
2699.6133308410645
2552.5055064539756
2073.189877537319
2819.207606267929
2205.9753671792837
2439.2530905560748
3021.9471246628536
1828.1127014160156
2688.8625340712697
2924.184411843618
3090.7362347841263
1866.8758103743844
2159.7305442094803
2486.3835604050582
3116.7599862416587
2784.2241612309995
3757.4114741683006
1725.5904846191406
2947.3044277372815
2233.17479948564
2269.170021057129
2409.0685034857856
1802.789294719696
3107.5341233753024
2599.9868371486664
2346.8013879335845
2242.212239074707
2514.462214502795
1893.020009770113
3207.417660522461
1483.820337395919
2053.4166259765625
2668.2288911002024
2103.4335239047095
2500.082788567794
2391.6740067425894
3118.2834339141846
1727.3380650111608
2160.8233132081873
Solved 199/200 episodes
2496.2585252837525
Evaluated model in 33.1 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-12
Round 13
Generated trajectories in 72.4 seconds
epoch: 0	train loss: 2152.76025390625	(16.3s)
epoch: 1	train loss: 1195.4090576171875	(16.3s)
epoch: 2	train loss: 1028.4296875	(16.5s)
epoch: 3	train loss: 871.3917236328125	(16.4s)
epoch: 4	train loss: 1152.7215576171875	(16.5s)
epoch: 5	train loss: 1470.958251953125	(16.4s)
epoch: 6	train loss: 661.4862670898438	(16.4s)
epoch: 7	train loss: 868.7094116210938	(16.5s)
epoch: 8	train loss: 810.1572265625	(16.5s)
epoch: 9	train loss: 1014.0408325195312	(16.5s)
epoch: 10	train loss: 724.5108642578125	(16.4s)
epoch: 11	train loss: 851.248046875	(16.4s)
epoch: 12	train loss: 644.326171875	(16.5s)
epoch: 13	train loss: 939.0347900390625	(16.5s)
epoch: 14	train loss: 1545.462158203125	(16.5s)
epoch: 15	train loss: 682.9906616210938	(16.5s)
epoch: 16	train loss: 1299.151611328125	(16.4s)
epoch: 17	train loss: 958.5617065429688	(16.5s)
epoch: 18	train loss: 789.807373046875	(16.5s)
epoch: 19	train loss: 607.8510131835938	(16.5s)
epoch: 20	train loss: 686.2750854492188	(16.5s)
epoch: 21	train loss: 860.9542846679688	(16.4s)
epoch: 22	train loss: 909.0385131835938	(16.5s)
epoch: 23	train loss: 357.486083984375	(16.5s)
epoch: 24	train loss: 624.6968383789062	(16.4s)
epoch: 25	train loss: 2477.68896484375	(16.5s)
epoch: 26	train loss: 429.3885498046875	(16.4s)
epoch: 27	train loss: 344.2084655761719	(16.4s)
epoch: 28	train loss: 678.5128784179688	(16.5s)
epoch: 29	train loss: 782.6031494140625	(16.4s)
epoch: 30	train loss: 327.5439453125	(16.5s)
epoch: 31	train loss: 373.6109313964844	(16.4s)
epoch: 32	train loss: 590.6392822265625	(16.4s)
epoch: 33	train loss: 695.3057861328125	(16.5s)
epoch: 34	train loss: 376.7823791503906	(16.5s)
epoch: 35	train loss: 421.42425537109375	(16.5s)
epoch: 36	train loss: 275.7720947265625	(16.4s)
epoch: 37	train loss: 497.805908203125	(16.4s)
epoch: 38	train loss: 1718.8016357421875	(16.5s)
epoch: 39	train loss: 1394.479248046875	(16.5s)
epoch: 40	train loss: 1062.2138671875	(16.5s)
epoch: 41	train loss: 1570.66943359375	(16.5s)
epoch: 42	train loss: 571.4305419921875	(16.4s)
epoch: 43	train loss: 579.4747314453125	(16.5s)
epoch: 44	train loss: 986.7418823242188	(16.5s)
epoch: 45	train loss: 199.9360809326172	(16.4s)
epoch: 46	train loss: 932.2122802734375	(16.6s)
epoch: 47	train loss: 507.299072265625	(16.4s)
epoch: 48	train loss: 119.66143035888672	(16.4s)
epoch: 49	train loss: 222.5143585205078	(16.5s)
epoch: 50	train loss: 562.1067504882812	(16.4s)
epoch: 51	train loss: 160.58535766601562	(16.5s)
epoch: 52	train loss: 114.75703430175781	(16.4s)
epoch: 53	train loss: 476.2088928222656	(16.4s)
epoch: 54	train loss: 539.6426391601562	(16.5s)
epoch: 55	train loss: 254.3283233642578	(16.4s)
epoch: 56	train loss: 781.5209350585938	(16.5s)
epoch: 57	train loss: 365.6427307128906	(16.4s)
epoch: 58	train loss: 1185.44580078125	(16.4s)
epoch: 59	train loss: 906.6627807617188	(16.5s)
epoch: 60	train loss: 546.1103515625	(16.4s)
epoch: 61	train loss: 391.84075927734375	(16.4s)
epoch: 62	train loss: 170.65611267089844	(16.4s)
epoch: 63	train loss: 95.82080078125	(16.4s)
epoch: 64	train loss: 134.01487731933594	(16.3s)
epoch: 65	train loss: 1231.4306640625	(16.3s)
epoch: 66	train loss: 613.6843872070312	(16.3s)
epoch: 67	train loss: 1609.1011962890625	(16.4s)
epoch: 68	train loss: 1542.203125	(16.4s)
epoch: 69	train loss: 1216.146728515625	(16.4s)
epoch: 70	train loss: 536.8652954101562	(16.5s)
epoch: 71	train loss: 1014.8784790039062	(16.4s)
epoch: 72	train loss: 339.2621765136719	(16.5s)
epoch: 73	train loss: 149.63082885742188	(16.4s)
epoch: 74	train loss: 103.65313720703125	(16.4s)
epoch: 75	train loss: 446.89129638671875	(16.5s)
epoch: 76	train loss: 1412.1583251953125	(16.5s)
epoch: 77	train loss: 580.2508544921875	(16.5s)
epoch: 78	train loss: 229.54171752929688	(16.4s)
epoch: 79	train loss: 170.66412353515625	(16.5s)
epoch: 80	train loss: 300.6998596191406	(16.5s)
epoch: 81	train loss: 394.0086364746094	(16.4s)
epoch: 82	train loss: 329.2532043457031	(16.5s)
epoch: 83	train loss: 376.8040771484375	(16.5s)
epoch: 84	train loss: 300.59002685546875	(16.4s)
epoch: 85	train loss: 513.758056640625	(16.5s)
epoch: 86	train loss: 2025.173828125	(16.5s)
epoch: 87	train loss: 454.6305236816406	(16.5s)
epoch: 88	train loss: 429.54486083984375	(16.5s)
epoch: 89	train loss: 755.078857421875	(16.4s)
epoch: 90	train loss: 562.7991333007812	(16.5s)
epoch: 91	train loss: 1321.6724853515625	(16.5s)
epoch: 92	train loss: 392.3927001953125	(16.5s)
epoch: 93	train loss: 409.3470764160156	(16.5s)
epoch: 94	train loss: 240.4652862548828	(16.4s)
epoch: 95	train loss: 659.590087890625	(16.4s)
epoch: 96	train loss: 133.84829711914062	(16.5s)
epoch: 97	train loss: 190.52626037597656	(16.5s)
epoch: 98	train loss: 44.69236373901367	(16.5s)
epoch: 99	train loss: 27.861804962158203	(16.5s)
Evaluating model on 200 episodes
2894.325082397461
2346.389332857999
4472.2701393127445
2822.829811442982
3126.99387049675
2445.306316693624
3578.245821800232
3441.6692534226636
2929.400926589966
2510.023088659559
2665.4509863419967
2389.7253660202027
2769.9609513963974
3974.1651351928713
1724.113357782364
3115.3004392426587
4031.596096801758
2846.1123986079774
2760.927903827868
1731.152395248413
2602.0050533435965
2525.145043509347
2867.7605025551534
2136.4902189345585
2750.6178941272556
2308.2437673950194
2622.698218451606
3758.2056739113546
2342.9746627807617
2644.246229345148
3238.9359357198077
3320.5790528962107
2872.5222659833503
2361.9893182754518
3143.939953016198
2113.1282811846054
2234.161716534541
3061.886925138276
2528.599274105496
2445.4147162898894
3566.675761580467
3640.293769836426
2645.6773479779563
2601.812388556344
2466.288698533002
1678.1380217075348
2510.437424182892
2136.6946379343667
2216.1774658203126
3375.315884192785
2588.7090576434957
2634.306067696933
2871.361804679588
3316.3088467696616
2347.4212988339937
2481.1281688087865
3066.4428231920515
2346.218179321289
3164.6655864715576
3101.5251171805644
2695.2403365425444
2919.094348621368
2521.580433502197
1869.4068895067487
2559.0949000482974
3164.230039889996
2657.973157442533
2656.4524786689067
3430.8596700032554
3467.6789577484133
2098.2581061814963
2558.0816179911294
2298.1143647707427
2150.9366718928018
2156.576922953129
2665.6164051402698
2515.541763850621
2508.5151037918895
3277.5155505130165
2647.163655434885
3541.1422756740026
3093.288282394409
2614.636844114824
2668.322900136312
2787.6947838995193
3718.776771454584
2526.498383379997
2080.7225319544473
2729.264614359538
2172.2708478655136
2352.4183444976807
2869.5091206094494
3001.054157658627
3769.1183096000127
2800.6019053988985
2641.482140152543
2698.4940976109997
1959.0096245933983
2373.614161529541
3080.7165187835694
2408.211188634237
3066.1988974119486
2530.5428365071616
2732.646895980835
1699.8303778512138
3342.2202241516115
2545.520193227132
2053.3431927607608
2672.2843602498374
2731.6934613766875
2278.4467910357885
2439.708223256198
2651.6951217651367
4770.261411190033
3044.641120783488
1741.3705467810998
2282.9515163020083
3467.285493373871
3316.7355850682115
2882.5955917835236
3221.7266418055483
2474.8538777119406
2433.8631970882416
3402.3297080993652
2334.3040717340286
2964.4802141189575
3105.492754123829
3208.8097319352
3666.9561154595735
1698.4515846653987
3148.102812520919
2198.160171508789
4531.15855884552
2633.472403690733
3080.2672713308625
2524.127469062805
2331.588452265813
3372.853549003601
2686.9068484836152
2712.1613713777983
4234.236303329468
3377.98686356978
1960.8922073190863
2641.6078449249267
2777.035487856184
3498.5544384850396
3025.8798385902687
2584.5443840026855
2890.575034558773
2503.2468338775634
3599.7506614923477
2005.8168557484944
3849.646912314675
2539.6500864664713
2966.666380330136
2633.4881553649902
2127.7522292137146
2031.190411376953
2975.0446944236755
3337.2005729675293
2084.935505474315
3062.3827529234045
2518.6212363243103
2644.556137084961
3367.0418559265136
2345.05831246627
2852.3919110017664
2695.3750474112376
2467.9799480438232
3328.0256116655137
2892.9305247138527
3950.62492897397
3320.6627603258407
2509.486058643886
2539.089769077301
4210.17501449585
3090.4784282502674
3858.58570022583
2485.2354155949183
3141.3179676055906
3233.568501325754
3098.5946945777305
2720.8822318109974
2230.3001552581786
2778.571587456597
3071.0640806310316
2047.1604177856445
2893.768457852877
2078.9531005512586
2492.810749848684
2695.0711341271035
3352.974389856512
3175.313488824027
2895.8465604782104
2371.9904542402787
3359.4247945785523
3422.418706258138
2936.541093826294
Solved 198/200 episodes
2778.384476173849
Evaluated model in 32.9 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-13
Round 14
Generated trajectories in 73.7 seconds
epoch: 0	train loss: 1955.58544921875	(16.4s)
epoch: 1	train loss: 1413.037109375	(16.4s)
epoch: 2	train loss: 962.83740234375	(16.5s)
epoch: 3	train loss: 690.9654541015625	(16.5s)
epoch: 4	train loss: 512.4270629882812	(16.6s)
epoch: 5	train loss: 500.0812072753906	(16.5s)
epoch: 6	train loss: 1062.7640380859375	(16.5s)
epoch: 7	train loss: 2065.94091796875	(16.6s)
epoch: 8	train loss: 984.41943359375	(16.6s)
epoch: 9	train loss: 579.721435546875	(16.6s)
epoch: 10	train loss: 743.0057983398438	(16.5s)
epoch: 11	train loss: 1225.6341552734375	(16.5s)
epoch: 12	train loss: 519.1056518554688	(16.6s)
epoch: 13	train loss: 521.147216796875	(16.6s)
epoch: 14	train loss: 1679.6236572265625	(16.6s)
epoch: 15	train loss: 790.7990112304688	(16.5s)
epoch: 16	train loss: 638.31005859375	(16.5s)
epoch: 17	train loss: 760.0142822265625	(16.6s)
epoch: 18	train loss: 464.2736511230469	(16.5s)
epoch: 19	train loss: 299.07421875	(16.6s)
epoch: 20	train loss: 529.9961547851562	(16.5s)
epoch: 21	train loss: 1298.483154296875	(16.5s)
epoch: 22	train loss: 781.9976806640625	(16.6s)
epoch: 23	train loss: 2539.521484375	(16.6s)
epoch: 24	train loss: 884.4871826171875	(16.6s)
epoch: 25	train loss: 553.809326171875	(16.5s)
epoch: 26	train loss: 342.0709533691406	(16.5s)
epoch: 27	train loss: 385.4333190917969	(16.5s)
epoch: 28	train loss: 572.4527587890625	(16.6s)
epoch: 29	train loss: 1741.050537109375	(16.6s)
epoch: 30	train loss: 552.9467163085938	(16.6s)
epoch: 31	train loss: 261.5180969238281	(16.5s)
epoch: 32	train loss: 939.1195068359375	(16.6s)
epoch: 33	train loss: 318.20062255859375	(16.6s)
epoch: 34	train loss: 184.1407928466797	(16.5s)
epoch: 35	train loss: 115.11422729492188	(16.6s)
epoch: 36	train loss: 654.791015625	(16.5s)
epoch: 37	train loss: 1351.4029541015625	(16.5s)
epoch: 38	train loss: 1617.44287109375	(16.6s)
epoch: 39	train loss: 835.1912231445312	(16.6s)
epoch: 40	train loss: 692.6784057617188	(16.6s)
epoch: 41	train loss: 366.54852294921875	(16.5s)
epoch: 42	train loss: 721.5569458007812	(16.5s)
epoch: 43	train loss: 810.4925537109375	(16.6s)
epoch: 44	train loss: 291.46905517578125	(16.5s)
epoch: 45	train loss: 196.55633544921875	(16.6s)
epoch: 46	train loss: 347.2369689941406	(16.5s)
epoch: 47	train loss: 1190.6710205078125	(16.5s)
epoch: 48	train loss: 551.6508178710938	(16.6s)
epoch: 49	train loss: 1075.2322998046875	(16.5s)
epoch: 50	train loss: 355.9539794921875	(16.6s)
epoch: 51	train loss: 375.66314697265625	(16.5s)
epoch: 52	train loss: 399.550537109375	(16.5s)
epoch: 53	train loss: 623.2532348632812	(16.5s)
epoch: 54	train loss: 1648.0377197265625	(16.5s)
epoch: 55	train loss: 547.4560546875	(16.5s)
epoch: 56	train loss: 290.051025390625	(16.5s)
epoch: 57	train loss: 224.4686737060547	(16.5s)
epoch: 58	train loss: 250.2036590576172	(16.5s)
epoch: 59	train loss: 356.82476806640625	(16.5s)
epoch: 60	train loss: 675.4326782226562	(16.5s)
epoch: 61	train loss: 149.08670043945312	(16.5s)
epoch: 62	train loss: 121.43817901611328	(16.4s)
epoch: 63	train loss: 1333.822021484375	(16.3s)
epoch: 64	train loss: 761.1256713867188	(16.4s)
epoch: 65	train loss: 264.7646484375	(16.4s)
epoch: 66	train loss: 224.4755401611328	(16.5s)
epoch: 67	train loss: 891.5243530273438	(16.4s)
epoch: 68	train loss: 1430.7091064453125	(16.4s)
epoch: 69	train loss: 389.2785339355469	(16.5s)
epoch: 70	train loss: 245.4735565185547	(16.5s)
epoch: 71	train loss: 252.6192169189453	(16.6s)
epoch: 72	train loss: 332.2947998046875	(16.5s)
epoch: 73	train loss: 753.6187133789062	(16.5s)
epoch: 74	train loss: 1351.50048828125	(16.5s)
epoch: 75	train loss: 652.9649047851562	(16.5s)
epoch: 76	train loss: 182.06057739257812	(16.5s)
epoch: 77	train loss: 106.89811706542969	(16.5s)
epoch: 78	train loss: 626.5889282226562	(16.5s)
epoch: 79	train loss: 538.1557006835938	(16.5s)
epoch: 80	train loss: 794.7551879882812	(16.5s)
epoch: 81	train loss: 476.5889587402344	(16.5s)
epoch: 82	train loss: 838.4954833984375	(16.5s)
epoch: 83	train loss: 270.6573791503906	(16.5s)
epoch: 84	train loss: 422.34222412109375	(16.5s)
epoch: 85	train loss: 1950.78466796875	(16.5s)
epoch: 86	train loss: 676.5449829101562	(16.5s)
epoch: 87	train loss: 583.833251953125	(16.6s)
epoch: 88	train loss: 200.6451873779297	(16.5s)
epoch: 89	train loss: 998.9754638671875	(16.5s)
epoch: 90	train loss: 182.7198486328125	(16.6s)
epoch: 91	train loss: 73.5845718383789	(16.6s)
epoch: 92	train loss: 26.50524139404297	(16.6s)
epoch: 93	train loss: 18.570411682128906	(16.5s)
epoch: 94	train loss: 15.273006439208984	(16.5s)
epoch: 95	train loss: 20.86656951904297	(16.6s)
epoch: 96	train loss: 818.4219360351562	(16.5s)
epoch: 97	train loss: 224.98228454589844	(16.6s)
epoch: 98	train loss: 1276.5682373046875	(16.5s)
epoch: 99	train loss: 2274.406005859375	(16.6s)
Evaluating model on 200 episodes
3145.615748910343
3729.1300025708747
2285.9999166870116
4171.9313823155
3109.6080374717712
2269.039885697541
3242.657709227668
2882.543937810262
3519.652255296707
4079.115763092041
3265.155675676134
2069.878378354586
3846.7756357425596
3545.1948352995373
4046.085081777265
3702.2297677993774
3789.0648523218492
2588.1595502580917
4189.075549316406
5225.08818605211
4723.487612406413
3201.5902608983656
3266.771282332284
2764.553496274081
4158.736139678955
3556.584322058636
4059.571100870768
5844.289573962872
2958.956008911133
2843.634635231712
2625.352652809837
3736.7435513664695
5395.412526130676
3144.692848856856
4141.127048775002
2480.6261031811055
3743.9974910191127
4025.522411528088
3365.9189068046776
2956.645393848419
4173.990333903919
2738.3458338419596
2716.780186722918
2341.4144294375465
3487.0744058709397
4742.139276504517
3875.309348424276
3653.7737382458104
3942.4763002066775
4082.5967249189102
3793.061283420872
1954.8576053437732
3281.2981106302013
3409.044978085686
2577.454253283414
3113.227708209645
3060.7101340915847
3659.4826164245605
3362.2792977905274
3583.2247772216797
3580.9389814649307
2763.779331907934
3924.828217547873
3608.7462425231934
3567.879709097055
2325.809090508355
4676.537827650706
3976.4457800728933
3213.3781414031982
2906.898667075417
2946.2170073721145
2593.662128660414
3046.2305397987366
3200.1656354757456
3743.2827326456704
2729.810588684082
3894.5019639968873
3914.2094790140786
3686.9215698242188
2968.566601092999
2862.6837549209595
3729.254971590909
3958.130824455848
3003.455757756387
2746.096669028787
3267.6099070140294
2695.0097156524657
3697.578674731047
3701.020957336426
3396.079778319911
2640.1152481079102
2792.1368600168535
4322.92830657959
4604.661535474989
2805.2678799099394
2401.7471105135405
3561.4598537113357
4485.49814453125
3006.686191705557
2914.9445921182632
4931.860935211182
2878.1337883289043
2819.7674778408473
4159.80791414701
3871.276901626587
3407.930410279168
2603.8842911353477
4008.2969188690186
3025.758283996582
2421.4532422152433
3940.852684469784
2592.8066590253043
2978.8458162035263
1934.095471572876
2132.0824169921875
3462.708445140294
2304.7707427342734
2973.1854824739344
2451.280361973962
4249.685905948762
2171.4191733466255
2043.4430749592957
3077.8691430342824
2305.6274416996885
2844.3611627197265
4797.689717610677
5384.4748178209575
4197.2935426773565
2884.773561859131
2062.130485534668
2650.4603907677433
3661.251915401883
2929.3240660208244
5432.751846313477
5767.296076965332
3060.6378359587297
3974.732799275716
2640.185400685956
4518.808413872352
3657.9246856249297
3073.358993736473
3960.300003051758
3323.650605837504
3239.614547729492
5152.517552115701
5861.860812794079
3236.464923010932
3700.1059312820435
3956.645783197312
3224.3600447972617
2582.4961470942344
3922.7854382324217
2525.6612975176645
3900.109042663574
3307.5479412078857
4233.170153184371
2742.2281579971313
3351.7455702561597
4171.459336853028
4795.267518997192
3308.859322767991
3682.8199738722583
3780.5333580603965
1817.4505679222846
3358.3809135700094
3914.639628657588
4507.557256106673
4526.647929191589
3639.8576984405518
3011.0797398885093
3980.1931952017326
2352.543388366699
3884.1297656059264
3615.7899079756303
3070.682271684919
3238.535846233368
3693.827765724876
3128.0184326171875
2728.6375714408027
3565.8243437267483
3232.9049173990884
3656.964985438756
3784.130307091607
3543.3044691085815
2630.941264152527
4688.623088291713
3094.174941778183
3568.839559020996
4358.696702436967
3569.964444732666
3416.156001697887
3439.8909192452065
3323.511199951172
2955.1507261276247
4219.095310974121
2844.8112923406784
3284.787544965744
2840.71504080923
Solved 198/200 episodes
3409.1844746350175
Evaluated model in 34.9 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-14
Round 15
Generated trajectories in 72.1 seconds
epoch: 0	train loss: 1798.47119140625	(16.4s)
epoch: 1	train loss: 952.0975341796875	(16.3s)
epoch: 2	train loss: 862.474609375	(16.5s)
epoch: 3	train loss: 677.728759765625	(16.4s)
epoch: 4	train loss: 1674.8087158203125	(16.4s)
epoch: 5	train loss: 663.8101196289062	(16.4s)
epoch: 6	train loss: 346.82391357421875	(16.4s)
epoch: 7	train loss: 1428.0465087890625	(16.5s)
epoch: 8	train loss: 696.1063842773438	(16.4s)
epoch: 9	train loss: 645.5679931640625	(16.4s)
epoch: 10	train loss: 1253.106201171875	(16.5s)
epoch: 11	train loss: 678.3858642578125	(16.4s)
epoch: 12	train loss: 497.3414306640625	(16.4s)
epoch: 13	train loss: 1138.2591552734375	(16.5s)
epoch: 14	train loss: 724.2423095703125	(16.4s)
epoch: 15	train loss: 804.364501953125	(16.4s)
epoch: 16	train loss: 656.0714111328125	(16.4s)
epoch: 17	train loss: 234.00027465820312	(16.4s)
epoch: 18	train loss: 194.6043701171875	(16.5s)
epoch: 19	train loss: 440.76068115234375	(16.4s)
epoch: 20	train loss: 371.13330078125	(16.4s)
epoch: 21	train loss: 216.5753936767578	(16.5s)
epoch: 22	train loss: 225.8347625732422	(16.4s)
epoch: 23	train loss: 1987.8890380859375	(16.5s)
epoch: 24	train loss: 803.1190795898438	(16.4s)
epoch: 25	train loss: 449.9224853515625	(16.4s)
epoch: 26	train loss: 514.87451171875	(16.4s)
epoch: 27	train loss: 693.5640258789062	(16.4s)
epoch: 28	train loss: 631.6923828125	(16.4s)
epoch: 29	train loss: 252.69021606445312	(16.4s)
epoch: 30	train loss: 530.7703247070312	(16.4s)
epoch: 31	train loss: 1192.168212890625	(16.4s)
epoch: 32	train loss: 269.3884582519531	(16.5s)
epoch: 33	train loss: 275.11639404296875	(16.5s)
epoch: 34	train loss: 978.9212646484375	(16.5s)
epoch: 35	train loss: 207.0762939453125	(16.4s)
epoch: 36	train loss: 282.7433776855469	(16.4s)
epoch: 37	train loss: 86.72561645507812	(16.5s)
epoch: 38	train loss: 55.301490783691406	(16.4s)
epoch: 39	train loss: 36.842952728271484	(16.5s)
epoch: 40	train loss: 52.072349548339844	(16.4s)
epoch: 41	train loss: 1200.558349609375	(16.4s)
epoch: 42	train loss: 604.6590576171875	(16.5s)
epoch: 43	train loss: 1122.1298828125	(16.4s)
epoch: 44	train loss: 1429.9478759765625	(16.5s)
epoch: 45	train loss: 1373.66796875	(16.4s)
epoch: 46	train loss: 626.7408447265625	(16.4s)
epoch: 47	train loss: 750.648681640625	(16.5s)
epoch: 48	train loss: 377.3464050292969	(16.4s)
epoch: 49	train loss: 438.22991943359375	(16.5s)
epoch: 50	train loss: 661.204833984375	(16.4s)
epoch: 51	train loss: 674.9629516601562	(16.4s)
epoch: 52	train loss: 147.34176635742188	(16.5s)
epoch: 53	train loss: 145.5872039794922	(16.4s)
epoch: 54	train loss: 1297.5709228515625	(16.5s)
epoch: 55	train loss: 1361.4989013671875	(16.4s)
epoch: 56	train loss: 855.0703735351562	(16.4s)
epoch: 57	train loss: 666.471435546875	(16.4s)
epoch: 58	train loss: 198.71658325195312	(16.5s)
epoch: 59	train loss: 315.58868408203125	(16.4s)
epoch: 60	train loss: 976.546875	(16.4s)
epoch: 61	train loss: 224.82809448242188	(16.4s)
epoch: 62	train loss: 398.9605712890625	(16.3s)
epoch: 63	train loss: 247.4702911376953	(16.4s)
epoch: 64	train loss: 228.98231506347656	(16.3s)
epoch: 65	train loss: 213.78695678710938	(16.4s)
epoch: 66	train loss: 146.68592834472656	(16.3s)
epoch: 67	train loss: 86.63113403320312	(16.3s)
epoch: 68	train loss: 2728.759765625	(16.5s)
epoch: 69	train loss: 548.2102661132812	(16.4s)
epoch: 70	train loss: 722.1975708007812	(16.5s)
epoch: 71	train loss: 355.8143310546875	(16.4s)
epoch: 72	train loss: 234.67730712890625	(16.4s)
epoch: 73	train loss: 91.21025848388672	(16.5s)
epoch: 74	train loss: 400.39422607421875	(16.4s)
epoch: 75	train loss: 133.26560974121094	(16.4s)
epoch: 76	train loss: 198.1821746826172	(16.4s)
epoch: 77	train loss: 710.3692626953125	(16.4s)
epoch: 78	train loss: 125.95565795898438	(16.4s)
epoch: 79	train loss: 75.9511489868164	(16.4s)
epoch: 80	train loss: 144.58937072753906	(16.4s)
epoch: 81	train loss: 756.6862182617188	(16.4s)
epoch: 82	train loss: 1527.595458984375	(16.3s)
epoch: 83	train loss: 972.1567993164062	(16.4s)
epoch: 84	train loss: 1089.9088134765625	(16.4s)
epoch: 85	train loss: 498.54986572265625	(16.4s)
epoch: 86	train loss: 249.7028350830078	(16.4s)
epoch: 87	train loss: 216.6578826904297	(16.3s)
epoch: 88	train loss: 848.8074951171875	(16.4s)
epoch: 89	train loss: 372.7629089355469	(16.4s)
epoch: 90	train loss: 118.59892272949219	(16.4s)
epoch: 91	train loss: 228.2293701171875	(16.4s)
epoch: 92	train loss: 823.4759521484375	(16.3s)
epoch: 93	train loss: 216.1111297607422	(16.3s)
epoch: 94	train loss: 528.9971313476562	(16.4s)
epoch: 95	train loss: 213.68896484375	(16.4s)
epoch: 96	train loss: 51.89767074584961	(16.4s)
epoch: 97	train loss: 235.4119110107422	(16.4s)
epoch: 98	train loss: 755.5750122070312	(16.4s)
epoch: 99	train loss: 453.08282470703125	(16.4s)
Evaluating model on 200 episodes
3586.5418224334717
2266.9173223322086
3285.988199234009
2676.0130168690403
1926.8161712646483
4618.61513227575
4117.532982903558
5456.284688711166
2526.246708741059
3828.772485136986
4176.337532424926
4551.446732112339
4142.951480143779
3834.957907485962
3571.8497443199158
4162.331925328573
4337.659360100241
4155.567430284288
4739.381189982097
3963.0841960906982
3367.9064653836763
2818.771570841471
3846.1450445001774
4669.630695819855
3537.5057859665308
3754.705897198783
3663.4191393038122
3132.9687777432528
5833.752355748957
4528.658436527959
4463.770049124053
3663.7975539717563
4560.806871032715
4604.098445892334
5999.892663192749
2802.779140023624
2800.2352057332578
3258.6852000432136
5055.099302994578
5296.1134605407715
2590.2999172210693
4913.75265045166
4278.829942779541
4792.673362445831
4194.23073223659
2714.9257246653237
2887.9736489795505
4316.567179761614
3015.950704721304
3754.880835639106
3298.101708901895
3082.875208013198
2917.0688851096415
4044.422961425781
4484.306815407493
3725.4383126461144
4399.38098526001
3538.754943575178
3069.5671069496557
3872.5932589310864
3115.112095060803
3182.090608723958
2730.772171020508
4919.983459091187
3895.62709611462
4850.500497034618
4130.993720327105
3211.064885756549
3703.3149556699004
5564.498697509765
3731.872664451599
3013.4122285842896
3634.2140928569593
2280.305768220321
4968.679665785569
3273.630410194397
3383.4084371089934
3525.133253479004
2820.5885320203056
2954.976222144233
4009.0835986890293
4019.5668018341066
4273.556491470337
5480.442406373865
3846.0557446633616
3738.9196814695993
4094.8435442071213
2517.877296447754
3895.928331017494
3595.966552204556
3305.13889412258
3372.0656078338625
4975.5377588272095
4648.304654786081
3737.845220184326
4014.0056369615636
3609.8667368334395
3625.320552494215
3634.1491985321045
4620.132439028832
6378.930220709906
3685.5994561331613
4369.544781185332
3841.341217313494
3309.462693416711
4183.853283601648
5305.374741973877
4600.770919106223
2900.519794282459
2687.515498161316
3313.138991722694
6435.234090042114
4586.575743336832
3666.430658411097
2574.9020023345947
3658.1099389394126
3315.1556561377743
4089.9229100346565
3738.334083557129
4045.924136543274
3926.090928591215
2947.410385131836
4680.087828874588
3905.842048962911
5093.186376783583
4595.893025398254
3535.48330078125
3146.5398980502423
3970.860439954485
4289.270092072025
3266.1664222268496
3974.310333251953
3649.2543403362406
4914.372478908963
5597.741916474842
2117.6530326843263
3759.88260419441
4692.736886850993
3394.1940950666153
3692.424454825265
3430.66496270895
3439.124160033006
3162.789761029757
4777.917871558148
3929.853215853373
4381.138709174262
4821.558441825535
5765.819955825806
4456.520903396607
4340.488673132819
3990.3206928253176
3945.261636213823
3987.988285827637
3222.5903569062552
2956.9491039773693
3578.0771989822388
3689.210639260032
2424.2576475143433
3133.864255444757
3544.5984933035716
3852.46304545683
3380.5699247572156
5434.491005168242
3970.7323978130635
3657.77452917099
4617.447760445731
4263.699526468913
3734.5796625237717
3403.2790087531594
2884.252758372914
3727.889189402262
3759.3532951528377
3406.2985294048603
3717.2705562425695
5020.835906982422
3017.042745798826
3780.571360661433
3687.882587178548
2424.909300910102
3115.377688407898
4230.556891577585
4103.853564474318
4226.030657132466
3626.2079231037815
4436.810449727376
3373.63774394989
4785.144584655762
4572.707991513339
2184.986734662737
3933.0986041114443
3823.2024440058954
4856.212127276829
3992.0696420411805
4573.513277326311
4891.11912244161
2330.511027744838
3291.5353535832587
4177.743430472709
3132.597389984131
3436.273375701904
Solved 200/200 episodes
3865.1549468486714
Evaluated model in 34.5 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-15
Round 16
Generated trajectories in 72.7 seconds
epoch: 0	train loss: 1856.340576171875	(16.4s)
epoch: 1	train loss: 773.1807250976562	(16.4s)
epoch: 2	train loss: 566.33349609375	(16.4s)
epoch: 3	train loss: 701.9754638671875	(16.5s)
epoch: 4	train loss: 647.0682373046875	(16.5s)
epoch: 5	train loss: 1836.5047607421875	(16.5s)
epoch: 6	train loss: 684.4426879882812	(16.4s)
epoch: 7	train loss: 600.098388671875	(16.5s)
epoch: 8	train loss: 674.489990234375	(16.5s)
epoch: 9	train loss: 1681.125	(16.5s)
epoch: 10	train loss: 610.1275634765625	(16.6s)
epoch: 11	train loss: 420.227783203125	(16.5s)
epoch: 12	train loss: 1353.4578857421875	(16.5s)
epoch: 13	train loss: 929.3544311523438	(16.5s)
epoch: 14	train loss: 855.3309326171875	(16.5s)
epoch: 15	train loss: 435.8330993652344	(16.5s)
epoch: 16	train loss: 582.0325317382812	(16.5s)
epoch: 17	train loss: 272.0735778808594	(16.6s)
epoch: 18	train loss: 406.22802734375	(16.5s)
epoch: 19	train loss: 398.08367919921875	(16.5s)
epoch: 20	train loss: 847.6915893554688	(16.5s)
epoch: 21	train loss: 317.7155456542969	(16.5s)
epoch: 22	train loss: 147.1040496826172	(16.5s)
epoch: 23	train loss: 368.8664245605469	(16.5s)
epoch: 24	train loss: 134.81466674804688	(16.5s)
epoch: 25	train loss: 1383.9326171875	(16.5s)
epoch: 26	train loss: 1598.95556640625	(16.5s)
epoch: 27	train loss: 528.45947265625	(16.5s)
epoch: 28	train loss: 473.9579772949219	(16.5s)
epoch: 29	train loss: 377.3463439941406	(16.5s)
epoch: 30	train loss: 963.4310302734375	(16.4s)
epoch: 31	train loss: 449.3206481933594	(16.5s)
epoch: 32	train loss: 1732.040283203125	(16.5s)
epoch: 33	train loss: 752.947998046875	(16.6s)
epoch: 34	train loss: 307.1258239746094	(16.3s)
epoch: 35	train loss: 931.1876220703125	(16.5s)
epoch: 36	train loss: 322.4772033691406	(16.5s)
epoch: 37	train loss: 201.89662170410156	(16.5s)
epoch: 38	train loss: 440.0260925292969	(16.6s)
epoch: 39	train loss: 1091.6480712890625	(16.5s)
epoch: 40	train loss: 781.4656372070312	(16.5s)
epoch: 41	train loss: 331.7879638671875	(16.5s)
epoch: 42	train loss: 486.62548828125	(16.5s)
epoch: 43	train loss: 220.80044555664062	(16.6s)
epoch: 44	train loss: 780.7523193359375	(16.5s)
epoch: 45	train loss: 914.6781005859375	(16.5s)
epoch: 46	train loss: 338.32049560546875	(16.4s)
epoch: 47	train loss: 164.35997009277344	(16.5s)
epoch: 48	train loss: 63.16499710083008	(16.6s)
epoch: 49	train loss: 47.69489288330078	(16.5s)
epoch: 50	train loss: 153.73435974121094	(16.4s)
epoch: 51	train loss: 945.447509765625	(16.5s)
epoch: 52	train loss: 858.96728515625	(16.4s)
epoch: 53	train loss: 921.4951171875	(16.5s)
epoch: 54	train loss: 455.4942626953125	(16.4s)
epoch: 55	train loss: 314.3093566894531	(16.4s)
epoch: 56	train loss: 446.8495178222656	(16.4s)
epoch: 57	train loss: 180.66213989257812	(16.5s)
epoch: 58	train loss: 132.70465087890625	(16.5s)
epoch: 59	train loss: 151.86546325683594	(16.4s)
epoch: 60	train loss: 1613.8482666015625	(16.3s)
epoch: 61	train loss: 1419.246826171875	(16.4s)
epoch: 62	train loss: 1036.8570556640625	(16.3s)
epoch: 63	train loss: 459.1090087890625	(16.3s)
epoch: 64	train loss: 285.51190185546875	(16.3s)
epoch: 65	train loss: 316.4919128417969	(16.3s)
epoch: 66	train loss: 513.498046875	(16.3s)
epoch: 67	train loss: 488.6763000488281	(16.4s)
epoch: 68	train loss: 146.01699829101562	(16.4s)
epoch: 69	train loss: 135.84275817871094	(16.4s)
epoch: 70	train loss: 212.2709197998047	(16.4s)
epoch: 71	train loss: 766.3543701171875	(16.4s)
epoch: 72	train loss: 282.32293701171875	(16.5s)
epoch: 73	train loss: 229.30589294433594	(16.4s)
epoch: 74	train loss: 55.319217681884766	(16.5s)
epoch: 75	train loss: 777.6878051757812	(16.4s)
epoch: 76	train loss: 200.3486785888672	(16.4s)
epoch: 77	train loss: 89.84110260009766	(16.5s)
epoch: 78	train loss: 594.5803833007812	(16.5s)
epoch: 79	train loss: 274.5164794921875	(16.5s)
epoch: 80	train loss: 2538.990966796875	(16.5s)
epoch: 81	train loss: 1318.984375	(16.4s)
epoch: 82	train loss: 914.4258422851562	(16.4s)
epoch: 83	train loss: 659.2318115234375	(16.5s)
epoch: 84	train loss: 403.31060791015625	(16.5s)
epoch: 85	train loss: 1136.4334716796875	(16.5s)
epoch: 86	train loss: 352.06085205078125	(16.4s)
epoch: 87	train loss: 783.4591064453125	(16.5s)
epoch: 88	train loss: 121.7009506225586	(16.5s)
epoch: 89	train loss: 74.65852355957031	(16.3s)
epoch: 90	train loss: 90.89881896972656	(16.5s)
epoch: 91	train loss: 464.7887268066406	(16.4s)
epoch: 92	train loss: 894.5323486328125	(16.3s)
epoch: 93	train loss: 209.03533935546875	(16.5s)
epoch: 94	train loss: 170.81431579589844	(16.4s)
epoch: 95	train loss: 140.05796813964844	(16.5s)
epoch: 96	train loss: 69.23481750488281	(16.3s)
epoch: 97	train loss: 28.262834548950195	(16.3s)
epoch: 98	train loss: 129.7599334716797	(16.3s)
epoch: 99	train loss: 1232.4354248046875	(16.3s)
Evaluating model on 200 episodes
6481.0747163672195
5160.342068142361
6439.90290760994
5308.899225725447
5971.554005214146
3971.8687603290264
6796.245348059613
5218.646408807664
5370.597817140467
7315.1874643053325
5964.244382629395
4503.381251335144
8719.478541056314
6887.739906164316
4584.464577122739
4714.611575879549
7381.717864990234
7346.071551154641
8383.42889489068
6492.083355671651
5090.273545638374
6704.019706726074
8687.04402756691
5060.513237380981
4975.228299021721
4990.127657318115
4660.074007415771
5555.102230787277
3443.4259562772863
4284.35087122236
6270.829741923014
9248.059699376425
3912.578967285156
4910.138689377729
5904.8678639729815
4939.9096515828915
4544.499923629761
5518.359328621312
6298.024914264679
10051.300939941406
3805.7883823939733
5059.026622517904
4247.372755197378
6229.272135925293
4713.891624114092
5246.661659240723
4373.278352050781
5775.195165405274
4381.4892541133995
4987.12907409668
3774.7145998554847
4329.704031967535
5459.27230108352
4509.645399607145
5491.395884922573
4793.945785522461
7451.3813529968265
5278.242128448486
7841.988852364676
4943.027871068319
4224.67418762207
5914.0709214644
5528.5330784771895
5493.297996852709
4808.542691118577
5136.930367689866
4915.07729887351
5356.210884857178
5625.828469340006
5537.621224916898
6259.300195905897
5736.827877680461
9698.308623886109
5385.432477678572
5248.182013878455
7567.721406119211
6308.095821857452
5418.487794240315
6262.49484278361
7005.870338439941
6410.875096860139
4470.155041781339
8139.76431189643
4314.081797106513
6190.87047543733
3627.1922539605034
5718.117417619035
6665.23282342208
4504.732826457304
4706.892076449924
4711.701713908802
6317.253650806569
5587.430172511509
4541.808042086088
8319.690338134766
6692.431900501251
5536.012693517348
2906.3591274261476
6511.416084552633
5061.515731811523
4890.298294632523
5390.948061393969
7624.613069974459
6818.452301357103
7082.868000030518
4597.463488838889
4815.7226287841795
3954.451192855835
5082.852359056473
5936.4074009190435
7239.404859161377
7197.56201008388
5903.566828641025
5346.305705769857
5228.733440198396
4917.833369814116
5211.228246789229
15780.3994140625
6204.602608175839
4863.054713294619
5536.857063912056
4257.22612878048
4655.398377932035
5374.00399249533
4717.933023361933
5366.671258334456
5083.972140290017
3868.1107834180198
6176.082945505778
3875.606383155374
4153.85877641042
5113.977312655063
7400.0282771898355
6075.60799584022
6239.8056035723
5413.168245892192
5049.852621793747
5423.500272846222
4491.931745147705
5656.256093502045
4548.625196456909
4084.050367909093
3839.1944397374205
5511.440790812175
5844.83771870931
5800.173019857968
5679.367657131619
3914.088970043041
4884.580473247327
5537.148935029001
5286.183601074219
4047.779352611966
5999.99180330549
4010.5934078818873
4757.43782043457
8119.548088618687
4305.855661964417
6604.933204650879
5594.389832206394
6073.488542036576
5243.497188047929
5525.413330078125
4574.582587814331
6116.587564256456
7094.488149581417
7557.997794087728
6019.663008949973
6761.216377258301
5390.288096675166
4208.636714776357
5305.975842884609
5086.8346157073975
4985.918590727307
5844.510678863526
3701.9094200134277
5075.003736972809
5575.921153533153
4709.752840677897
4592.847075195313
6483.179904076361
6738.3640247213425
4939.072173527309
4985.590087108123
5294.438920762804
4977.771251902861
6822.662750244141
4304.619635068453
4372.971567461567
5016.443071036503
4273.655594398236
5561.859287427819
7427.040647697449
5674.218835292719
6160.939594480727
5124.424654560705
5047.219410419464
7105.242627868653
5429.998125076294
5168.0487543741865
Solved 199/200 episodes
5579.284334039715
Evaluated model in 35.0 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-16
Round 17
Generated trajectories in 72.1 seconds
epoch: 0	train loss: 1810.788818359375	(16.3s)
epoch: 1	train loss: 959.9868774414062	(16.3s)
epoch: 2	train loss: 1436.17578125	(16.3s)
epoch: 3	train loss: 1958.58984375	(16.3s)
epoch: 4	train loss: 1128.328125	(16.3s)
epoch: 5	train loss: 846.6885986328125	(16.3s)
epoch: 6	train loss: 630.323974609375	(16.4s)
epoch: 7	train loss: 476.3774108886719	(16.3s)
epoch: 8	train loss: 538.4422607421875	(16.3s)
epoch: 9	train loss: 668.2891235351562	(16.5s)
epoch: 10	train loss: 916.2769775390625	(16.4s)
epoch: 11	train loss: 1684.810302734375	(16.5s)
epoch: 12	train loss: 1050.7950439453125	(16.4s)
epoch: 13	train loss: 884.9282836914062	(16.3s)
epoch: 14	train loss: 510.14874267578125	(16.4s)
epoch: 15	train loss: 349.4166564941406	(16.4s)
epoch: 16	train loss: 276.41375732421875	(16.5s)
epoch: 17	train loss: 343.6118469238281	(16.3s)
epoch: 18	train loss: 437.7347106933594	(16.3s)
epoch: 19	train loss: 906.27783203125	(16.4s)
epoch: 20	train loss: 497.10009765625	(16.3s)
epoch: 21	train loss: 210.6040496826172	(16.4s)
epoch: 22	train loss: 579.46728515625	(16.3s)
epoch: 23	train loss: 322.08740234375	(16.4s)
epoch: 24	train loss: 270.8433532714844	(16.4s)
epoch: 25	train loss: 635.0617065429688	(16.4s)
epoch: 26	train loss: 421.0323486328125	(16.4s)
epoch: 27	train loss: 1178.8511962890625	(16.4s)
epoch: 28	train loss: 2393.4248046875	(16.4s)
epoch: 29	train loss: 1135.2337646484375	(16.4s)
epoch: 30	train loss: 1228.72802734375	(16.5s)
epoch: 31	train loss: 1027.2689208984375	(16.4s)
epoch: 32	train loss: 645.100830078125	(16.4s)
epoch: 33	train loss: 662.9179077148438	(16.3s)
epoch: 34	train loss: 293.0336608886719	(16.4s)
epoch: 35	train loss: 462.19378662109375	(16.4s)
epoch: 36	train loss: 364.5914611816406	(16.3s)
epoch: 37	train loss: 185.6749267578125	(16.4s)
epoch: 38	train loss: 246.82403564453125	(16.4s)
epoch: 39	train loss: 1582.39013671875	(16.3s)
epoch: 40	train loss: 278.7441101074219	(16.4s)
epoch: 41	train loss: 81.04105377197266	(16.3s)
epoch: 42	train loss: 73.71504211425781	(16.4s)
epoch: 43	train loss: 44.76219177246094	(16.3s)
epoch: 44	train loss: 35.86683654785156	(16.3s)
epoch: 45	train loss: 42.26761245727539	(16.4s)
epoch: 46	train loss: 988.821533203125	(16.3s)
epoch: 47	train loss: 940.7308349609375	(16.4s)
epoch: 48	train loss: 713.5291748046875	(16.3s)
epoch: 49	train loss: 677.3515625	(16.4s)
epoch: 50	train loss: 475.8697509765625	(16.5s)
epoch: 51	train loss: 307.5681457519531	(16.4s)
epoch: 52	train loss: 406.2958068847656	(16.4s)
epoch: 53	train loss: 551.8076171875	(16.4s)
epoch: 54	train loss: 1364.019775390625	(16.4s)
epoch: 55	train loss: 429.1214294433594	(16.4s)
epoch: 56	train loss: 758.577880859375	(16.4s)
epoch: 57	train loss: 222.3091583251953	(16.3s)
epoch: 58	train loss: 168.01699829101562	(16.3s)
epoch: 59	train loss: 189.39715576171875	(16.4s)
epoch: 60	train loss: 2215.898193359375	(16.3s)
epoch: 61	train loss: 287.7955017089844	(16.3s)
epoch: 62	train loss: 195.6755828857422	(16.2s)
epoch: 63	train loss: 410.5743713378906	(16.3s)
epoch: 64	train loss: 746.5275268554688	(16.3s)
epoch: 65	train loss: 208.42071533203125	(16.3s)
epoch: 66	train loss: 97.20003509521484	(16.4s)
epoch: 67	train loss: 353.3102722167969	(16.4s)
epoch: 68	train loss: 141.97052001953125	(16.5s)
epoch: 69	train loss: 653.9412841796875	(16.4s)
epoch: 70	train loss: 1318.997314453125	(16.4s)
epoch: 71	train loss: 2055.469970703125	(16.4s)
epoch: 72	train loss: 922.6658325195312	(16.4s)
epoch: 73	train loss: 763.3192749023438	(16.5s)
epoch: 74	train loss: 608.1448974609375	(16.4s)
epoch: 75	train loss: 403.1020812988281	(16.4s)
epoch: 76	train loss: 192.26463317871094	(16.4s)
epoch: 77	train loss: 77.04449462890625	(16.4s)
epoch: 78	train loss: 111.54529571533203	(16.5s)
epoch: 79	train loss: 55.087249755859375	(16.4s)
epoch: 80	train loss: 868.9419555664062	(16.4s)
epoch: 81	train loss: 221.6245574951172	(16.4s)
epoch: 82	train loss: 169.9141845703125	(16.4s)
epoch: 83	train loss: 78.3650131225586	(16.4s)
epoch: 84	train loss: 306.0736999511719	(16.4s)
epoch: 85	train loss: 483.4340515136719	(16.4s)
epoch: 86	train loss: 309.9547119140625	(16.4s)
epoch: 87	train loss: 196.49395751953125	(16.4s)
epoch: 88	train loss: 1310.8648681640625	(16.4s)
epoch: 89	train loss: 1388.3519287109375	(16.4s)
epoch: 90	train loss: 260.8741760253906	(16.4s)
epoch: 91	train loss: 517.9931030273438	(16.4s)
epoch: 92	train loss: 406.0177001953125	(16.4s)
epoch: 93	train loss: 257.7509460449219	(16.4s)
epoch: 94	train loss: 64.63395690917969	(16.5s)
epoch: 95	train loss: 67.30738830566406	(16.4s)
epoch: 96	train loss: 24.610200881958008	(16.4s)
epoch: 97	train loss: 153.4109344482422	(16.4s)
epoch: 98	train loss: 277.45697021484375	(16.4s)
epoch: 99	train loss: 1892.434326171875	(16.5s)
Evaluating model on 200 episodes
4858.504411969866
8384.88628829153
3997.194687080383
4350.612937545777
4918.734358549118
2981.054533481598
2887.8920928955076
4352.166409562274
4549.845135582818
4251.097030639648
3786.3658130168915
3994.5811026723763
3375.6381896972657
4503.420366484544
4762.083100574772
5418.1708246866865
2502.7839471435545
3270.7701366258702
4664.350457509358
4580.78941013502
6251.6414303249785
4650.296383401622
4280.075052028749
5619.680852104636
5229.970675819798
4795.6477291870115
6260.768747965495
5034.241016948924
5394.125501844618
3065.254982948303
3598.7074909210205
3316.10516166687
3767.661968231201
3847.4713525772095
4021.5204000762014
5259.247498194377
3583.078758098461
4006.1050822918232
5074.8400836181645
4355.586221483019
4176.602099524604
5205.875333786011
4312.355563036601
3504.553434583876
4848.1462617761945
6303.126296323889
3399.1823554039
5542.34588792589
5035.36224609375
4803.629662513733
4852.820935273782
3907.5107138497488
3356.4955326203376
5104.659200032552
4363.105429562655
3605.8489599682034
5226.558390436946
5039.393433570862
5037.951555023194
2929.7612384996914
4062.7126239908152
4424.2027104695635
5111.515076319377
3995.5121954055057
13021.679141998291
4331.870574951172
4617.615449269612
4825.298996362931
3397.0987208230154
6349.42700805664
6348.850908279419
7232.239325417413
3717.121169567108
4400.437242456384
3888.5857514412173
4282.842535909017
5761.983396530151
4224.514501783583
5986.571933521944
4933.552572704497
3913.2144527435303
4779.909469763438
4379.947862809704
6243.556294123332
4611.346548783152
5468.403853280203
4590.576945326064
6487.126373291016
5390.137064308956
4630.634512583415
2509.7327835083006
5166.616398435651
5281.24812672933
5452.435064043318
3678.4663883077687
4132.3537673950195
4695.524775981903
4335.509422425301
4235.102021669087
4994.091859340668
5378.187015656502
5630.651945659092
3952.0953538894655
7018.367613898383
4996.015813927901
3990.6057122548423
3652.2403748456168
2606.9573919122868
4357.876627793184
3094.4473856608074
6518.235887674185
4218.530987010283
3973.9256504603795
4748.65922315193
4477.552048619588
2855.3286465864917
3435.8837951660157
4212.173264639719
4106.831952546772
4985.581457773845
5263.51852106165
3614.3096635001048
7298.4368571528685
5011.836087181455
5552.200556154604
7153.801863352458
4289.600320314106
4564.043675472862
3739.6097316741943
4374.368305206299
4652.8763263408955
4848.286329789596
6215.516355614913
3463.9626911708287
4620.305022239685
4205.704024806168
5378.239417817857
3305.9998092651367
5751.655055236816
2461.3750005868765
4826.572659624948
4540.826196908951
4294.507941526525
3918.143159346147
3918.476274617513
4362.036659876506
4780.127822310837
4302.505899047852
5737.287173730356
5239.261817238547
3207.737085342407
3976.0982333592005
4571.159750576677
3264.1703092787
4005.7817358289444
3409.030075391134
5858.621545040246
2991.438569502397
2876.7211196899416
5020.974359784807
6469.85949283176
4537.133843612671
5667.867330763075
4176.810668120513
4808.576778553151
3963.678765296936
3717.13165918986
5456.587881596884
6478.995444615682
3264.255470275879
4364.202696663992
3629.2537071228026
7538.101077079773
3774.355590629578
4514.312643241882
3854.6125306629
2926.321805705195
3361.5320991516114
3053.2341360009236
5752.063814499799
7044.799523671468
2325.722017833165
4971.910614694868
6160.82291692098
5273.267040252686
4328.196374511719
5597.31922458958
4949.289027494543
4731.698410644532
5899.502361522002
5070.6757058536305
5793.139207503375
4659.51896144322
4730.137630820274
5655.19397846345
5481.81431795756
Solved 196/200 episodes
4556.534924373358
Evaluated model in 33.9 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-17
Round 18
Generated trajectories in 71.5 seconds
epoch: 0	train loss: 1850.7197265625	(16.4s)
epoch: 1	train loss: 851.8084716796875	(16.3s)
epoch: 2	train loss: 1000.84716796875	(16.4s)
epoch: 3	train loss: 715.2496948242188	(16.3s)
epoch: 4	train loss: 879.8159790039062	(16.5s)
epoch: 5	train loss: 1163.8759765625	(16.5s)
epoch: 6	train loss: 567.7305908203125	(16.4s)
epoch: 7	train loss: 501.6778869628906	(16.5s)
epoch: 8	train loss: 502.88970947265625	(16.5s)
epoch: 9	train loss: 495.73748779296875	(16.5s)
epoch: 10	train loss: 899.331298828125	(16.4s)
epoch: 11	train loss: 415.473388671875	(16.4s)
epoch: 12	train loss: 339.8048095703125	(16.4s)
epoch: 13	train loss: 404.7367248535156	(16.5s)
epoch: 14	train loss: 1125.2196044921875	(16.5s)
epoch: 15	train loss: 665.9560546875	(16.5s)
epoch: 16	train loss: 1016.1184692382812	(16.4s)
epoch: 17	train loss: 744.5513305664062	(16.3s)
epoch: 18	train loss: 332.84515380859375	(16.5s)
epoch: 19	train loss: 477.5409851074219	(16.5s)
epoch: 20	train loss: 1266.198974609375	(16.5s)
epoch: 21	train loss: 534.11572265625	(16.2s)
epoch: 22	train loss: 351.4101257324219	(16.5s)
epoch: 23	train loss: 1385.79052734375	(16.5s)
epoch: 24	train loss: 206.8402099609375	(16.4s)
epoch: 25	train loss: 106.15739440917969	(16.5s)
epoch: 26	train loss: 64.20658111572266	(16.4s)
epoch: 27	train loss: 66.26754760742188	(16.4s)
epoch: 28	train loss: 386.9278564453125	(16.3s)
epoch: 29	train loss: 2119.641357421875	(16.5s)
epoch: 30	train loss: 1691.2841796875	(16.5s)
epoch: 31	train loss: 538.0133056640625	(16.5s)
epoch: 32	train loss: 266.34466552734375	(16.3s)
epoch: 33	train loss: 203.39215087890625	(16.4s)
epoch: 34	train loss: 143.96986389160156	(16.5s)
epoch: 35	train loss: 583.6962280273438	(16.5s)
epoch: 36	train loss: 463.3111572265625	(16.5s)
epoch: 37	train loss: 228.93194580078125	(16.4s)
epoch: 38	train loss: 369.6473388671875	(16.4s)
epoch: 39	train loss: 2168.703125	(16.4s)
epoch: 40	train loss: 986.4837646484375	(16.3s)
epoch: 41	train loss: 728.9030151367188	(16.5s)
epoch: 42	train loss: 292.1661376953125	(16.4s)
epoch: 43	train loss: 268.9680480957031	(16.4s)
epoch: 44	train loss: 101.85978698730469	(16.3s)
epoch: 45	train loss: 45.661373138427734	(16.4s)
epoch: 46	train loss: 24.596036911010742	(16.5s)
epoch: 47	train loss: 555.5890502929688	(16.4s)
epoch: 48	train loss: 327.45648193359375	(16.4s)
epoch: 49	train loss: 1268.9581298828125	(16.5s)
epoch: 50	train loss: 947.3407592773438	(16.4s)
epoch: 51	train loss: 505.9140319824219	(16.5s)
epoch: 52	train loss: 368.1069641113281	(16.4s)
epoch: 53	train loss: 741.1900024414062	(16.4s)
epoch: 54	train loss: 171.802490234375	(16.4s)
epoch: 55	train loss: 181.71542358398438	(16.5s)
epoch: 56	train loss: 388.5937194824219	(16.5s)
epoch: 57	train loss: 408.514892578125	(16.4s)
epoch: 58	train loss: 1538.99658203125	(16.4s)
epoch: 59	train loss: 683.2479248046875	(16.3s)
epoch: 60	train loss: 692.7551879882812	(16.3s)
epoch: 61	train loss: 510.7505187988281	(16.2s)
epoch: 62	train loss: 184.96002197265625	(16.3s)
epoch: 63	train loss: 177.36221313476562	(16.3s)
epoch: 64	train loss: 148.70516967773438	(16.3s)
epoch: 65	train loss: 457.9681396484375	(16.4s)
epoch: 66	train loss: 591.3964233398438	(16.4s)
epoch: 67	train loss: 611.5211181640625	(16.5s)
epoch: 68	train loss: 126.05087280273438	(16.3s)
epoch: 69	train loss: 50.75992965698242	(16.4s)
epoch: 70	train loss: 21.04469871520996	(16.5s)
epoch: 71	train loss: 13.585832595825195	(16.5s)
epoch: 72	train loss: 633.5603637695312	(16.5s)
epoch: 73	train loss: 494.25494384765625	(16.5s)
epoch: 74	train loss: 1826.31103515625	(16.5s)
epoch: 75	train loss: 436.2376708984375	(16.4s)
epoch: 76	train loss: 182.7624969482422	(16.5s)
epoch: 77	train loss: 113.29923248291016	(16.6s)
epoch: 78	train loss: 131.89515686035156	(16.5s)
epoch: 79	train loss: 783.07177734375	(16.3s)
epoch: 80	train loss: 979.5807495117188	(16.5s)
epoch: 81	train loss: 196.5919647216797	(16.5s)
epoch: 82	train loss: 126.0618896484375	(16.5s)
epoch: 83	train loss: 50.44847106933594	(16.5s)
epoch: 84	train loss: 327.1766357421875	(16.5s)
epoch: 85	train loss: 552.135009765625	(16.4s)
epoch: 86	train loss: 1483.806640625	(16.5s)
epoch: 87	train loss: 516.8313598632812	(16.5s)
epoch: 88	train loss: 442.3126220703125	(16.5s)
epoch: 89	train loss: 97.59882354736328	(16.5s)
epoch: 90	train loss: 476.67840576171875	(16.5s)
epoch: 91	train loss: 135.27056884765625	(16.5s)
epoch: 92	train loss: 367.43902587890625	(16.4s)
epoch: 93	train loss: 1516.5692138671875	(16.5s)
epoch: 94	train loss: 736.8191528320312	(16.3s)
epoch: 95	train loss: 461.0788269042969	(16.4s)
epoch: 96	train loss: 489.0179443359375	(16.5s)
epoch: 97	train loss: 167.28684997558594	(16.4s)
epoch: 98	train loss: 183.03244018554688	(16.5s)
epoch: 99	train loss: 213.3508758544922	(16.3s)
Evaluating model on 200 episodes
8561.535226709702
8163.660409780649
8000.908770424979
6831.117469787598
9913.018274943033
6435.183604472392
8649.338177059008
7397.561213175456
10360.512136550176
9990.545642505993
11628.298210144043
10293.40213839213
9056.330093948929
6652.729557846532
5863.657301389254
8591.6243300665
9197.003122541639
6925.899645291842
9363.884688822429
6036.4569753011065
8350.478238550822
6773.942503087661
5439.887821015857
6359.403291262113
9009.964008532072
9990.188141716852
11049.631193462172
7000.795762379964
6719.72649473893
10086.000229263305
6557.11941746303
8469.690764465333
10717.627763271332
7369.9966548039365
9411.563828745197
6447.033922697368
6787.875176822438
7809.640201344209
10564.10445175171
6221.61390346951
8298.569910849294
7521.156852722168
9164.007497999402
7667.296178817749
5924.085342407227
8104.918899536133
7278.330345399918
7001.458071708679
8389.464301639133
8988.92796603116
6805.704774220784
4229.482924938202
7016.589600996537
8563.01342010498
9108.024753952026
6638.379781386431
9906.015590122768
8171.5258607535525
6562.349721069336
9839.274695502387
7994.162109851837
7714.416255121646
8939.159282684326
9419.982707977295
8969.575231933593
5273.3650433676585
8012.97852391782
10329.546465555826
7764.824481790716
6434.115845304547
8650.478902016917
8224.934725806826
6144.309270788122
7249.171936932732
8392.829557719984
6795.437947379218
7949.45827460289
10379.557666015626
8232.706691196987
9197.376145935059
10316.702308654785
13517.103026580811
9387.006019592285
5911.123720169067
9310.497387625954
7371.345515463087
8413.037011282784
5671.111257765028
8967.971720819887
7873.7672026952105
9591.04659885168
7694.030548587923
7940.885613277041
9950.019216537476
6172.155054800851
8103.222441020765
8055.336273193359
7075.252163056404
6982.619208702674
8048.136489868164
7042.095248413086
10097.23942820231
7387.627977259019
4858.182824028863
9859.08643798828
5634.4894535358135
7574.497356096904
7650.770823160808
11202.887073810283
8380.717381564054
7236.793772183932
8400.154072655572
7987.907603672573
7939.487740889839
9254.590773315429
17352.364055633545
8845.493203524886
11835.205269622802
8271.659636306762
6391.849103450775
6505.1156977900755
10459.582058376736
7096.46767340766
5624.0088855198455
9030.308954366048
13205.258995749733
7904.730563693576
7659.855106898716
8847.24348786298
8481.96408589681
9390.054433059693
9450.330598449707
9600.695061238606
9175.587493896484
7544.889883858817
10145.685797177828
7703.454187746401
7491.494754497821
4829.396730957032
7038.749966588513
10332.418361497961
5789.230189583518
7816.367246900286
6716.196244812012
9185.168593406677
5170.076805538602
6406.631546747117
8861.366547851563
6462.617052482836
6237.966831752232
8035.275893240264
13675.556626213922
6783.919203324752
7375.169056611902
5407.604313243519
8621.9364157284
8744.63474559784
7062.55912475586
7505.581082893141
9295.876708397498
8810.684589930943
8790.757416534423
9662.187593688965
11404.873517990112
7794.227917380955
6756.66774559021
8896.323976135254
9459.485474806566
6242.39928483963
7239.134390024038
6148.89499648412
14456.796502113342
7097.878415970576
6051.844557642937
6701.857708474864
6152.933427211216
7581.090070724487
6921.2654434204105
5375.388563156128
6686.465267944336
8236.432345686288
9278.18007590554
7329.818337402344
8449.35960798997
8075.12340717931
9450.177743727161
7310.507784329928
6098.6819402521305
10036.46169104943
7622.166295369466
8405.018913269043
4926.645176743016
6303.744047376845
11331.6279296875
7198.77784576416
7474.478627613613
8518.459809875489
6747.2163652692525
7800.527170817058
Solved 199/200 episodes
8086.734856554493
Evaluated model in 34.0 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-18
Round 19
Generated trajectories in 72.2 seconds
epoch: 0	train loss: 1625.5819091796875	(16.4s)
epoch: 1	train loss: 907.078857421875	(16.4s)
epoch: 2	train loss: 943.0488891601562	(16.6s)
epoch: 3	train loss: 1037.1358642578125	(16.5s)
epoch: 4	train loss: 419.4296569824219	(16.6s)
epoch: 5	train loss: 264.4250183105469	(16.5s)
epoch: 6	train loss: 252.77638244628906	(16.5s)
epoch: 7	train loss: 738.7723388671875	(16.6s)
epoch: 8	train loss: 283.9941101074219	(16.5s)
epoch: 9	train loss: 702.7556762695312	(16.6s)
epoch: 10	train loss: 823.3091430664062	(16.6s)
epoch: 11	train loss: 491.6650085449219	(16.5s)
epoch: 12	train loss: 648.918701171875	(16.6s)
epoch: 13	train loss: 379.9228515625	(16.5s)
epoch: 14	train loss: 477.97955322265625	(16.6s)
epoch: 15	train loss: 1715.4027099609375	(16.5s)
epoch: 16	train loss: 1328.398681640625	(16.5s)
epoch: 17	train loss: 532.5005493164062	(16.6s)
epoch: 18	train loss: 252.57571411132812	(16.4s)
epoch: 19	train loss: 312.5214538574219	(16.6s)
epoch: 20	train loss: 478.2032775878906	(16.5s)
epoch: 21	train loss: 431.0798645019531	(16.4s)
epoch: 22	train loss: 619.1111450195312	(16.6s)
epoch: 23	train loss: 1063.8372802734375	(16.5s)
epoch: 24	train loss: 918.2949829101562	(16.6s)
epoch: 25	train loss: 818.0902099609375	(16.5s)
epoch: 26	train loss: 667.5961303710938	(16.5s)
epoch: 27	train loss: 636.5587768554688	(16.6s)
epoch: 28	train loss: 544.9003295898438	(16.5s)
epoch: 29	train loss: 337.8984680175781	(16.6s)
epoch: 30	train loss: 388.8901672363281	(16.5s)
epoch: 31	train loss: 198.479248046875	(16.5s)
epoch: 32	train loss: 978.5177001953125	(16.5s)
epoch: 33	train loss: 560.7224731445312	(16.6s)
epoch: 34	train loss: 178.514404296875	(16.5s)
epoch: 35	train loss: 346.5590515136719	(16.6s)
epoch: 36	train loss: 238.51535034179688	(16.5s)
epoch: 37	train loss: 201.6219940185547	(16.5s)
epoch: 38	train loss: 902.7239990234375	(16.6s)
epoch: 39	train loss: 389.8778991699219	(16.5s)
epoch: 40	train loss: 515.6044921875	(16.6s)
epoch: 41	train loss: 142.83633422851562	(16.5s)
epoch: 42	train loss: 86.4839096069336	(16.5s)
epoch: 43	train loss: 315.80657958984375	(16.6s)
epoch: 44	train loss: 632.697021484375	(16.5s)
epoch: 45	train loss: 530.3027954101562	(16.6s)
epoch: 46	train loss: 1051.2518310546875	(16.5s)
epoch: 47	train loss: 1112.317138671875	(16.5s)
epoch: 48	train loss: 329.0643615722656	(16.6s)
epoch: 49	train loss: 2575.868408203125	(16.5s)
epoch: 50	train loss: 238.88510131835938	(16.6s)
epoch: 51	train loss: 107.3454818725586	(16.5s)
epoch: 52	train loss: 44.8333625793457	(16.5s)
epoch: 53	train loss: 44.641632080078125	(16.5s)
epoch: 54	train loss: 32.27455139160156	(16.5s)
epoch: 55	train loss: 144.3143310546875	(16.6s)
epoch: 56	train loss: 680.3433837890625	(16.5s)
epoch: 57	train loss: 324.30706787109375	(16.5s)
epoch: 58	train loss: 67.77623748779297	(16.5s)
epoch: 59	train loss: 37.14494705200195	(16.4s)
epoch: 60	train loss: 21.173187255859375	(16.4s)
epoch: 61	train loss: 22.967426300048828	(16.3s)
epoch: 62	train loss: 8.62796401977539	(16.3s)
epoch: 63	train loss: 5.409183025360107	(16.4s)
epoch: 64	train loss: 4.360156536102295	(16.5s)
epoch: 65	train loss: 3.389615058898926	(16.5s)
epoch: 66	train loss: 2.217341423034668	(16.4s)
epoch: 67	train loss: 1.535006046295166	(16.5s)
epoch: 68	train loss: 1.2190122604370117	(16.5s)
epoch: 69	train loss: 1.016570806503296	(16.6s)
epoch: 70	train loss: 0.8313653469085693	(16.5s)
epoch: 71	train loss: 0.6849141716957092	(16.6s)
epoch: 72	train loss: 0.5831573009490967	(16.5s)
epoch: 73	train loss: 0.5110741853713989	(16.5s)
epoch: 74	train loss: 0.44898706674575806	(16.6s)
epoch: 75	train loss: 0.39819803833961487	(16.5s)
epoch: 76	train loss: 0.35475867986679077	(16.6s)
epoch: 77	train loss: 0.32032036781311035	(16.5s)
epoch: 78	train loss: 0.288958877325058	(16.5s)
epoch: 79	train loss: 0.2589980959892273	(16.6s)
epoch: 80	train loss: 0.23602762818336487	(16.5s)
epoch: 81	train loss: 0.21422864496707916	(16.6s)
epoch: 82	train loss: 0.1959984004497528	(16.5s)
epoch: 83	train loss: 0.17974142730236053	(16.5s)
epoch: 84	train loss: 0.1636151522397995	(16.6s)
epoch: 85	train loss: 0.15049001574516296	(16.5s)
epoch: 86	train loss: 0.13832637667655945	(16.6s)
epoch: 87	train loss: 0.12664948403835297	(16.5s)
epoch: 88	train loss: 0.11684127897024155	(16.5s)
epoch: 89	train loss: 0.10718937963247299	(16.5s)
epoch: 90	train loss: 0.09838210046291351	(16.5s)
epoch: 91	train loss: 0.09100377559661865	(16.6s)
epoch: 92	train loss: 0.08391810208559036	(16.5s)
epoch: 93	train loss: 0.07717955112457275	(16.5s)
epoch: 94	train loss: 0.0710577592253685	(16.5s)
epoch: 95	train loss: 0.06557431071996689	(16.6s)
epoch: 96	train loss: 0.060680683702230453	(16.5s)
epoch: 97	train loss: 0.05614381283521652	(16.5s)
epoch: 98	train loss: 0.052234210073947906	(16.5s)
epoch: 99	train loss: 0.04840855672955513	(16.5s)
Evaluating model on 200 episodes
18112.952195278434
17043.861699697132
22606.264951705933
20039.24471588135
16108.434613396139
17821.723890516492
13720.335240981158
11443.327194213867
14400.064457820012
18532.674497331893
19020.64437866211
14977.94558454241
14786.952752834413
17446.731891534266
9573.765591575986
13989.68643951416
12090.499680539395
13552.323832850303
15425.204193115234
20988.027666364396
19214.90736799974
12852.438484191895
17309.655465932992
28366.78247833252
16545.21152750651
16790.037103271483
16810.325608062743
18265.869782511392
16654.013427734375
15440.09706233098
18058.938313802082
16757.201958104182
15540.578635475853
17831.723720296224
13290.806226228413
17364.553561939912
24324.65789794922
14223.663005155675
13791.385523946661
20377.44750613258
13968.942823752379
14897.370681762695
16900.98050396259
15979.287802206503
15550.138075395063
17462.242057398747
18016.73079970787
13814.26874346052
14790.36612877479
22318.123825073242
20542.42785699027
12520.343125661215
18046.932216521232
9822.442313561072
14707.193837483725
16468.79206169976
16258.135986328125
16257.547480174473
13611.728387451172
13701.444569307216
16113.68085746765
14540.742992532665
15595.587865492877
16543.533571326214
19362.081085205078
17273.597873687744
14037.639747619629
14008.598561006434
16858.960367838543
15902.12618473598
15845.47161342076
14325.442380038176
20671.947907840506
13459.599184097782
17898.58200465611
20474.842008863176
19428.421313694544
15028.280112304688
14478.45625591278
20462.48902784075
20421.59221104213
19351.865718494762
13959.719645261765
14966.13963623047
13056.940356914814
17485.474743071056
18397.36054715243
19972.97743733724
20822.26550858109
14489.314187186104
15278.865070211476
15372.04882897271
17110.131909942625
17165.72698635525
18673.479892730713
17581.225569725037
17556.09080696106
17951.0548992157
15205.167739513308
15449.476857503256
16251.197796333126
22549.006192294033
16824.882903000405
18697.564453125
10766.185531616211
19358.685491485594
15569.498452113225
17496.60053888957
20637.36283416748
17745.85687794405
19936.5701915196
16045.029609093299
13968.346277410334
19399.325926694004
19292.841006630344
15614.084884643555
14662.455433824327
19477.803247305063
17941.21922302246
14667.882059733072
15453.853705512152
16647.610202789307
14442.149744937295
15042.537093098957
13241.35660963792
16296.420694102411
22370.972595214844
21436.114510672433
12317.095604578653
15063.682373894586
15882.406683731078
14148.048171099494
13669.397019606371
17005.890105376373
13643.65266066331
21312.861016524465
15393.495520340768
11395.259178899949
18087.780911897356
17934.477840968542
13172.575745391845
17231.421981811523
16677.26718678194
17235.687297011867
17423.52638244629
15213.449180385045
16204.324762015507
17021.320492214625
13811.627705624229
14045.800777141865
13998.816601092998
18571.26342555455
17123.31249445135
17991.955661010743
18509.256145477295
15661.788620919891
17575.480861957258
15616.194836846713
17092.096634714228
13605.408917236327
17239.192590886898
22080.105611947867
15461.983995939556
17321.90072562478
10221.430033874512
15031.073814060377
13361.128479003906
21803.212982904344
13326.74352231233
13848.226554022895
16911.16029624939
15444.682792262027
14125.170166015625
14510.755832837976
15278.546228027344
16931.99771012931
12269.66694859096
12820.245737711588
17329.361386899596
17526.533254262562
21763.1464954723
16646.973117404516
23143.841583251953
20183.529839651925
16518.385424931843
18544.343796502977
20155.4458694458
21418.18451538086
15541.680783658414
18131.287283325197
11450.510918112363
16238.168884277344
15249.064870198568
22621.09356689453
16252.172124650744
15354.607543496524
17821.641067504883
Solved 197/200 episodes
16355.253160510594
Evaluated model in 35.0 seconds
Saved model for run
e1bc0e086e2f4b18a8b742bc79bdc02e with name round-19
Completed training in 35188.5 seconds
