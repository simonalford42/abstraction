wandb: Currently logged in as: simonalford42. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.12.20
wandb: Run data is saved locally in /home/sca63/abstraction/wandb/run-20220629_224114-1b8w04q8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-snow-26
wandb: â­ï¸ View project at https://wandb.ai/simonalford42/abstraction
wandb: ğŸš€ View run at https://wandb.ai/simonalford42/abstraction/runs/1b8w04q8
Starting run:
29c057e845064a86a883fb49f69a39c3
params: Namespace(n=20000, b=10, abstract_pen=1.0, model='hmm-homo', seed=1, lr=0.0008, abstract_dim=32, tau_noise_std=0.0, freeze=False, load=False, ellis=False, no_log=False, num_test=200, fine_tune=False, tau_precompute=False, replace_trans_net=False, batch_norm=False, no_tau_norm=False, relational_micro=False, toy_test=False, separate_option_nets=False, gumbel=False, g_start_temp=1, g_stop_temp=1, num_categories=8, shrink_micro_net=False, shrink_loss_scale=1, length=(1, 2, 3, 4), muzero=False, test_every=90, save_every=180, batch_size=32, traj_updates=10000000.0, model_load_path='models/e14b78d01cc548239ffd57286e59e819.pt', gumbel_sched=False, device='NVIDIA GeForce RTX 2080 Ti', id='29c057e845064a86a883fb49f69a39c3')
Net has 41898 parameters
Saved model at models/29c057e845064a86a883fb49f69a39c3-epoch-152.pt
Saved model at models/29c057e845064a86a883fb49f69a39c3-epoch-284.pt
Saved model at models/29c057e845064a86a883fb49f69a39c3-epoch-426.pt
Saved model at models/29c057e845064a86a883fb49f69a39c3.pt
Completed training in 38556.2 seconds
wandb: Waiting for W&B process to finish... (success).
wandb: - 1.102 MB of 1.102 MB uploaded (0.000 MB deduped)wandb: \ 1.102 MB of 1.102 MB uploaded (0.000 MB deduped)wandb: | 1.102 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: / 1.102 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: - 1.118 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: \ 1.118 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: | 1.118 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: / 1.118 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: - 1.118 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: \ 1.118 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: | 1.118 MB of 1.118 MB uploaded (0.000 MB deduped)wandb: / 1.118 MB of 1.118 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     loss â–ˆâ–…â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: test/acc â–â–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:    epoch 499
wandb:     loss 21894.70275
wandb: test/acc 0.99
wandb: 
wandb: Synced olive-snow-26: https://wandb.ai/simonalford42/abstraction/runs/1b8w04q8
wandb: Synced 6 W&B file(s), 164 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220629_224114-1b8w04q8/logs
